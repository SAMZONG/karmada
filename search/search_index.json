{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Karmada Overview Quick Start Installation Refer to Installing Karmada . Concepts User Guide Cluster Registration Resource Propagating Cluster Failover Aggregated Kubernetes API Endpoint Customizing Resource Interpreter Configuring Controllers Best Practices Adoptions User cases in production. Karmada at VIPKID English \u4e2d\u6587 Developer Guide Contributors GitHub workflow Cherry Pick Overview Reference Troubleshooting Refer to Troubleshooting Frequently Asked Questions Refer to FAQ .","title":"Karmada"},{"location":"#karmada","text":"","title":"Karmada"},{"location":"#overview","text":"","title":"Overview"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installation","text":"Refer to Installing Karmada .","title":"Installation"},{"location":"#concepts","text":"","title":"Concepts"},{"location":"#user-guide","text":"Cluster Registration Resource Propagating Cluster Failover Aggregated Kubernetes API Endpoint Customizing Resource Interpreter Configuring Controllers","title":"User Guide"},{"location":"#best-practices","text":"","title":"Best Practices"},{"location":"#adoptions","text":"User cases in production. Karmada at VIPKID English \u4e2d\u6587","title":"Adoptions"},{"location":"#developer-guide","text":"","title":"Developer Guide"},{"location":"#contributors","text":"GitHub workflow Cherry Pick Overview","title":"Contributors"},{"location":"#reference","text":"","title":"Reference"},{"location":"#troubleshooting","text":"Refer to Troubleshooting","title":"Troubleshooting"},{"location":"#frequently-asked-questions","text":"Refer to FAQ .","title":"Frequently Asked Questions"},{"location":"bash-auto-completion-on-linux/","text":"bash auto-completion on Linux Introduction The karmadactl completion script for Bash can be generated with the command karmadactl completion bash. Sourcing the completion script in your shell enables karmadactl autocompletion. However, the completion script depends on bash-completion , which means that you have to install this software first (you can test if you have bash-completion already installed by running type _init_completion ). Install bash-completion bash-completion is provided by many package managers (see here ). You can install it with apt-get install bash-completion or yum install bash-completion , etc. The above commands create /usr/share/bash-completion/bash_completion , which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your ~/.bashrc file. source /usr/share/bash-completion/bash_completion Reload your shell and verify that bash-completion is correctly installed by typing type _init_completion . Enable karmadactl autocompletion You now need to ensure that the karmadactl completion script gets sourced in all your shell sessions. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(karmadactl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: karmadactl completion bash >/etc/bash_completion.d/karmadactl If you have an alias for karmadactl, you can extend shell completion to work with that alias: echo 'alias km=karmadactl' >>~/.bashrc echo 'complete -F __start_karmadactl km' >>~/.bashrc Note: bash-completion sources all completion scripts in /etc/bash_completion.d. Both approaches are equivalent. After reloading your shell, karmadactl autocompletion should be working. Enable kubectl-karmada autocompletion Currently, kubectl plugins do not support autocomplete, but it is already planned in Command line completion for kubectl plugins . We will update the documentation as soon as it does.","title":"bash auto-completion on Linux"},{"location":"bash-auto-completion-on-linux/#bash-auto-completion-on-linux","text":"","title":"bash auto-completion on Linux"},{"location":"bash-auto-completion-on-linux/#introduction","text":"The karmadactl completion script for Bash can be generated with the command karmadactl completion bash. Sourcing the completion script in your shell enables karmadactl autocompletion. However, the completion script depends on bash-completion , which means that you have to install this software first (you can test if you have bash-completion already installed by running type _init_completion ).","title":"Introduction"},{"location":"bash-auto-completion-on-linux/#install-bash-completion","text":"bash-completion is provided by many package managers (see here ). You can install it with apt-get install bash-completion or yum install bash-completion , etc. The above commands create /usr/share/bash-completion/bash_completion , which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your ~/.bashrc file. source /usr/share/bash-completion/bash_completion Reload your shell and verify that bash-completion is correctly installed by typing type _init_completion .","title":"Install bash-completion"},{"location":"bash-auto-completion-on-linux/#enable-karmadactl-autocompletion","text":"You now need to ensure that the karmadactl completion script gets sourced in all your shell sessions. There are two ways in which you can do this: Source the completion script in your ~/.bashrc file: echo 'source <(karmadactl completion bash)' >>~/.bashrc Add the completion script to the /etc/bash_completion.d directory: karmadactl completion bash >/etc/bash_completion.d/karmadactl If you have an alias for karmadactl, you can extend shell completion to work with that alias: echo 'alias km=karmadactl' >>~/.bashrc echo 'complete -F __start_karmadactl km' >>~/.bashrc Note: bash-completion sources all completion scripts in /etc/bash_completion.d. Both approaches are equivalent. After reloading your shell, karmadactl autocompletion should be working.","title":"Enable karmadactl autocompletion"},{"location":"bash-auto-completion-on-linux/#enable-kubectl-karmada-autocompletion","text":"Currently, kubectl plugins do not support autocomplete, but it is already planned in Command line completion for kubectl plugins . We will update the documentation as soon as it does.","title":"Enable kubectl-karmada autocompletion"},{"location":"descheduler/","text":"Descheduler Users could divide their replicas of a workload into different clusters in terms of available resources of member clusters. However, the scheduler's decisions are influenced by its view of Karmada at that point of time when a new ResourceBinding appears for scheduling. As Karmada multi-clusters are very dynamic and their state changes over time, there may be desire to move already running replicas to some other clusters due to lack of resources for the cluster. This may happen when some nodes of a cluster failed and the cluster does not have enough resource to accommodate their pods or the estimators have some estimation deviation, which is inevitable. The karmada-descheduler will detect all deployments once in a while, every 2 minutes by default. In every period, it will find out how many unschedulable replicas a deployment has in target scheduled clusters by calling karmada-scheduler-estimator. Then it will evict them from decreasing spec.clusters and trigger karmada-scheduler to do a 'Scale Schedule' based on the current situation. Note that it will take effect only when the replica scheduling strategy is dynamic division. Prerequisites Karmada has been installed We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases. Member cluster component is ready Ensure that all member clusters have joined Karmada and their corresponding karmada-scheduler-estimator is installed into karmada-host. Check member clusters using the following command: # Check whether member clusters have joined $ kubectl get cluster NAME VERSION MODE READY AGE member1 v1.19.1 Push True 11m member2 v1.19.1 Push True 11m member3 v1.19.1 Pull True 5m12s # check whether the karmada-scheduler-estimator of a member cluster has been working well $ kubectl --context karmada-host get pod -n karmada-system | grep estimator karmada-scheduler-estimator-member1-696b54fd56-xt789 1/1 Running 0 77s karmada-scheduler-estimator-member2-774fb84c5d-md4wt 1/1 Running 0 75s karmada-scheduler-estimator-member3-5c7d87f4b4-76gv9 1/1 Running 0 72s If a cluster has not joined, use hack/deploy-agent-and-estimator.sh to deploy both karmada-agent and karmada-scheduler-estimator. If the clusters have joined, use hack/deploy-scheduler-estimator.sh to only deploy karmada-scheduler-estimator. Scheduler option '--enable-scheduler-estimator' After all member clusters have joined and estimators are all ready, specify the option --enable-scheduler-estimator=true to enable scheduler estimator. # edit the deployment of karmada-scheduler $ kubectl --context karmada-host edit -n karmada-system deployments.apps karmada-scheduler Add the option --enable-scheduler-estimator=true into the command of container karmada-scheduler . Descheduler has been installed Ensure that the karmada-descheduler has been installed onto karmada-host. $ kubectl --context karmada-host get pod -n karmada-system | grep karmada-descheduler karmada-descheduler-658648d5b-c22qf 1/1 Running 0 80s Example Let's simulate a replica scheduling failure in a member cluster due to lack of resources. First we create a deployment with 3 replicas and divide them into 3 member clusters. apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: dynamicWeight: AvailableReplicas --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx resources: requests: cpu: \"2\" It is possible for these 3 replicas to be evenly divided into 3 member clusters, that is, one replica in each cluster. Now we taint all nodes in member1 and evict the replica. $ kubectl --context member1 cordon member1-control-plane $ kubectl --context member1 delete pod nginx-68b895fcbd-jgwz6 A new pod will be created and cannot be scheduled by kube-scheduler due to lack of resources. $ kubectl --context member1 get pod NAME READY STATUS RESTARTS AGE nginx-68b895fcbd-fccg4 1/1 Pending 0 80s After about 5 to 7 minutes, the pod in member1 will be evicted and scheduled to other available clusters. $ kubectl --context member1 get pod No resources found in default namespace. # kubectl --context member2 get pod NAME READY STATUS RESTARTS AGE nginx-68b895fcbd-dgd4x 1/1 Running 0 6m3s nginx-68b895fcbd-nwgjn 1/1 Running 0 4s","title":"Descheduler"},{"location":"descheduler/#descheduler","text":"Users could divide their replicas of a workload into different clusters in terms of available resources of member clusters. However, the scheduler's decisions are influenced by its view of Karmada at that point of time when a new ResourceBinding appears for scheduling. As Karmada multi-clusters are very dynamic and their state changes over time, there may be desire to move already running replicas to some other clusters due to lack of resources for the cluster. This may happen when some nodes of a cluster failed and the cluster does not have enough resource to accommodate their pods or the estimators have some estimation deviation, which is inevitable. The karmada-descheduler will detect all deployments once in a while, every 2 minutes by default. In every period, it will find out how many unschedulable replicas a deployment has in target scheduled clusters by calling karmada-scheduler-estimator. Then it will evict them from decreasing spec.clusters and trigger karmada-scheduler to do a 'Scale Schedule' based on the current situation. Note that it will take effect only when the replica scheduling strategy is dynamic division.","title":"Descheduler"},{"location":"descheduler/#prerequisites","text":"","title":"Prerequisites"},{"location":"descheduler/#karmada-has-been-installed","text":"We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases.","title":"Karmada has been installed"},{"location":"descheduler/#member-cluster-component-is-ready","text":"Ensure that all member clusters have joined Karmada and their corresponding karmada-scheduler-estimator is installed into karmada-host. Check member clusters using the following command: # Check whether member clusters have joined $ kubectl get cluster NAME VERSION MODE READY AGE member1 v1.19.1 Push True 11m member2 v1.19.1 Push True 11m member3 v1.19.1 Pull True 5m12s # check whether the karmada-scheduler-estimator of a member cluster has been working well $ kubectl --context karmada-host get pod -n karmada-system | grep estimator karmada-scheduler-estimator-member1-696b54fd56-xt789 1/1 Running 0 77s karmada-scheduler-estimator-member2-774fb84c5d-md4wt 1/1 Running 0 75s karmada-scheduler-estimator-member3-5c7d87f4b4-76gv9 1/1 Running 0 72s If a cluster has not joined, use hack/deploy-agent-and-estimator.sh to deploy both karmada-agent and karmada-scheduler-estimator. If the clusters have joined, use hack/deploy-scheduler-estimator.sh to only deploy karmada-scheduler-estimator.","title":"Member cluster component is ready"},{"location":"descheduler/#scheduler-option-enable-scheduler-estimator","text":"After all member clusters have joined and estimators are all ready, specify the option --enable-scheduler-estimator=true to enable scheduler estimator. # edit the deployment of karmada-scheduler $ kubectl --context karmada-host edit -n karmada-system deployments.apps karmada-scheduler Add the option --enable-scheduler-estimator=true into the command of container karmada-scheduler .","title":"Scheduler option '--enable-scheduler-estimator'"},{"location":"descheduler/#descheduler-has-been-installed","text":"Ensure that the karmada-descheduler has been installed onto karmada-host. $ kubectl --context karmada-host get pod -n karmada-system | grep karmada-descheduler karmada-descheduler-658648d5b-c22qf 1/1 Running 0 80s","title":"Descheduler has been installed"},{"location":"descheduler/#example","text":"Let's simulate a replica scheduling failure in a member cluster due to lack of resources. First we create a deployment with 3 replicas and divide them into 3 member clusters. apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: dynamicWeight: AvailableReplicas --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx resources: requests: cpu: \"2\" It is possible for these 3 replicas to be evenly divided into 3 member clusters, that is, one replica in each cluster. Now we taint all nodes in member1 and evict the replica. $ kubectl --context member1 cordon member1-control-plane $ kubectl --context member1 delete pod nginx-68b895fcbd-jgwz6 A new pod will be created and cannot be scheduled by kube-scheduler due to lack of resources. $ kubectl --context member1 get pod NAME READY STATUS RESTARTS AGE nginx-68b895fcbd-fccg4 1/1 Pending 0 80s After about 5 to 7 minutes, the pod in member1 will be evicted and scheduled to other available clusters. $ kubectl --context member1 get pod No resources found in default namespace. # kubectl --context member2 get pod NAME READY STATUS RESTARTS AGE nginx-68b895fcbd-dgd4x 1/1 Running 0 6m3s nginx-68b895fcbd-nwgjn 1/1 Running 0 4s","title":"Example"},{"location":"frequently-asked-questions/","text":"FAQ(Frequently Asked Questions) What is the difference between PropagationPolicy and ClusterPropagationPolicy? The PropagationPolicy is a namespace-scoped resource type which means the objects with this type must reside in a namespace. And the ClusterPropagationPolicy is the cluster-scoped resource type which means the objects with this type don't have a namespace. Both of them are used to hold the propagation declaration, but they have different capacities: - PropagationPolicy: can only represent the propagation policy for the resources in the same namespace. - ClusterPropagationPolicy: can represent the propagation policy for all resources including namespace-scoped and cluster-scoped resources. What is the difference between 'Push' and 'Pull' mode of a cluster? Please refer to Overview of Push and Pull . Why Karmada requires kube-controller-manager ? kube-controller-manager is composed of a bunch of controllers, Karmada inherits some controllers from it to keep a consistent user experience and behavior. It's worth noting that not all controllers are needed by Karmada, for the recommended controllers please refer to Recommended Controllers . Can I install Karmada in a Kubernetes cluster and reuse the kube-apiserver as Karmada apiserver? The quick answer is yes . In that case, you can save the effort to deploy karmada-apiserver and just share the APIServer between Kubernetes and Karmada. In addition, the high availability capabilities in the origin clusters can be inherited seamlessly. We do have some users using Karmada in this way. There are some things you should consider before doing so: This approach hasn't been fully tested by the Karmada community and no plan for it yet. This approach will increase computation costs for the Karmada system. E.g. After you apply a resource template , take Deployment as an example, the kube-controller will create Pods for the Deployment and update the status persistently, Karmada system will reconcile these changes too, so there might be conflicts. TODO: Link to adoption use case once it gets on board.","title":"FAQ(Frequently Asked Questions)"},{"location":"frequently-asked-questions/#faqfrequently-asked-questions","text":"","title":"FAQ(Frequently Asked Questions)"},{"location":"frequently-asked-questions/#what-is-the-difference-between-propagationpolicy-and-clusterpropagationpolicy","text":"The PropagationPolicy is a namespace-scoped resource type which means the objects with this type must reside in a namespace. And the ClusterPropagationPolicy is the cluster-scoped resource type which means the objects with this type don't have a namespace. Both of them are used to hold the propagation declaration, but they have different capacities: - PropagationPolicy: can only represent the propagation policy for the resources in the same namespace. - ClusterPropagationPolicy: can represent the propagation policy for all resources including namespace-scoped and cluster-scoped resources.","title":"What is the difference between PropagationPolicy and ClusterPropagationPolicy?"},{"location":"frequently-asked-questions/#what-is-the-difference-between-push-and-pull-mode-of-a-cluster","text":"Please refer to Overview of Push and Pull .","title":"What is the difference between 'Push' and 'Pull' mode of a cluster?"},{"location":"frequently-asked-questions/#why-karmada-requires-kube-controller-manager","text":"kube-controller-manager is composed of a bunch of controllers, Karmada inherits some controllers from it to keep a consistent user experience and behavior. It's worth noting that not all controllers are needed by Karmada, for the recommended controllers please refer to Recommended Controllers .","title":"Why Karmada requires kube-controller-manager?"},{"location":"frequently-asked-questions/#can-i-install-karmada-in-a-kubernetes-cluster-and-reuse-the-kube-apiserver-as-karmada-apiserver","text":"The quick answer is yes . In that case, you can save the effort to deploy karmada-apiserver and just share the APIServer between Kubernetes and Karmada. In addition, the high availability capabilities in the origin clusters can be inherited seamlessly. We do have some users using Karmada in this way. There are some things you should consider before doing so: This approach hasn't been fully tested by the Karmada community and no plan for it yet. This approach will increase computation costs for the Karmada system. E.g. After you apply a resource template , take Deployment as an example, the kube-controller will create Pods for the Deployment and update the status persistently, Karmada system will reconcile these changes too, so there might be conflicts. TODO: Link to adoption use case once it gets on board.","title":"Can I install Karmada in a Kubernetes cluster and reuse the kube-apiserver as Karmada apiserver?"},{"location":"istio-on-karmada/","text":"Use Istio on Karmada This document uses an example to demonstrate how to use Istio on Karmada. Follow this guide to install the Istio control plane on karmada-host (the primary cluster) and configure member1 and member2 (the remote cluster) to use the control plane in karmada-host . All clusters reside on the network1 network, meaning there is direct connectivity between the pods in both clusters. Install Karmada Install karmada control plane Following the steps Install karmada control plane in Quick Start, you can get a Karmada. Deploy Istio If you are testing multicluster setup on kind you can use MetalLB to make use of EXTERNAL-IP for LoadBalancer services. Install istioctl Please refer to the istioctl Installation. Prepare CA certificates Following the steps plug-in-certificates-and-key-into-the-cluster to configure Istio CA. Replace the cluster name cluster1 with primary , the output will looks like as follwing: root@karmada-demo istio-on-karmada# tree certs certs \u251c\u2500\u2500 primary \u2502 \u251c\u2500\u2500 ca-cert.pem \u2502 \u251c\u2500\u2500 ca-key.pem \u2502 \u251c\u2500\u2500 cert-chain.pem \u2502 \u2514\u2500\u2500 root-cert.pem \u251c\u2500\u2500 root-ca.conf \u251c\u2500\u2500 root-cert.csr \u251c\u2500\u2500 root-cert.pem \u251c\u2500\u2500 root-cert.srl \u2514\u2500\u2500 root-key.pem Install Istio on karmada-apiserver Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Create a secret cacerts in istio-system namespace: kubectl create namespace istio-system kubectl create secret generic cacerts -n istio-system \\ --from-file=certs/primary/ca-cert.pem \\ --from-file=certs/primary/ca-key.pem \\ --from-file=certs/primary/root-cert.pem \\ --from-file=certs/primary/cert-chain.pem Create a propagation policy for cacert secret: cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: cacerts-propagation namespace: istio-system spec: resourceSelectors: - apiVersion: v1 kind: Secret name: cacerts placement: clusterAffinity: clusterNames: - member1 - member2 EOF Run the following command to install istio CRDs on karmada apiserver: cat <<EOF | istioctl install -y --set profile=minimal -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout values: global: meshID: mesh1 multiCluster: clusterName: primary network: network1 EOF Karmada apiserver will not deploy a real istiod pod, you should press ctrl+c to exit installation when Processing resources for Istiod . \u2714 Istio core installed - Processing resources for Istiod. Install Istio on karmada host Create secret on karmada-host Karmada host is not a member cluster, we need create the cacerts secret for istiod . Export KUBECONFIG and switch to karmada host : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-host Create a secret cacerts in istio-system namespace: kubectl create namespace istio-system kubectl create secret generic cacerts -n istio-system \\ --from-file=certs/primary/ca-cert.pem \\ --from-file=certs/primary/ca-key.pem \\ --from-file=certs/primary/root-cert.pem \\ --from-file=certs/primary/cert-chain.pem Create istio-kubeconfig on karmada-host kubectl get secret -nkarmada-system kubeconfig --template={{.data.kubeconfig}} | base64 -d > kind-karmada.yaml kubectl create secret generic istio-kubeconfig --from-file=config=kind-karmada.yaml -nistio-system Install istio control plane cat <<EOF | istioctl install -y --set profile=minimal -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout values: global: meshID: mesh1 multiCluster: clusterName: primary network: network1 EOF Expose istiod service Run the following command to create a service for the istiod service: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: istiod-elb namespace: istio-system spec: ports: - name: https-dns port: 15012 protocol: TCP targetPort: 15012 selector: app: istiod istio: pilot sessionAffinity: None type: LoadBalancer EOF Export DISCOVERY_ADDRESS: export DISCOVERY_ADDRESS=$(kubectl get svc istiod-elb -nistio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}') # verify echo $DISCOVERY_ADDRESS Prepare member1 cluster secret Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 Create istio remote secret for member1: istioctl x create-remote-secret --name=member1 > istio-remote-secret-member1.yaml Prepare member2 cluster secret Export KUBECONFIG and switch to karmada member2 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member2 Create istio remote secret for member1: istioctl x create-remote-secret --name=member2 > istio-remote-secret-member2.yaml Apply istio remote secret Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Apply istio remote secret: kubectl apply -f istio-remote-secret-member1.yaml kubectl apply -f istio-remote-secret-member2.yaml Install istio remote Install istio remote member1 Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 cat <<EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: meshID: mesh1 multiCluster: clusterName: member1 network: network1 remotePilotAddress: ${DISCOVERY_ADDRESS} EOF Install istio remote member2 Export KUBECONFIG and switch to karmada member2 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member2 cat <<EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: meshID: mesh1 multiCluster: clusterName: member2 network: network1 remotePilotAddress: ${DISCOVERY_ADDRESS} EOF Deploy bookinfo application Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Create an istio-demo namespace: kubectl create namespace istio-demo Label the namespace that will host the application with istio-injection=enabled : kubectl label namespace istio-demo istio-injection=enabled Deploy your application using the kubectl command: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml Run the following command to create default destination rules for the Bookinfo services: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/networking/destination-rule-all.yaml Run the following command to create virtual service for the Bookinfo services: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/networking/virtual-service-all-v1.yaml Run the following command to create propagation policy for the Bookinfo services: cat <<EOF | kubectl apply -nistio-demo -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: service-propagation spec: resourceSelectors: - apiVersion: v1 kind: Service name: productpage - apiVersion: v1 kind: Service name: details - apiVersion: v1 kind: Service name: reviews - apiVersion: v1 kind: Service name: ratings placement: clusterAffinity: clusterNames: - member1 - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: produtpage-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: productpage-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-productpage placement: clusterAffinity: clusterNames: - member1 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: details-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: details-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-details placement: clusterAffinity: clusterNames: - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: reviews-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: reviews-v1 - apiVersion: apps/v1 kind: Deployment name: reviews-v2 - apiVersion: apps/v1 kind: Deployment name: reviews-v3 - apiVersion: v1 kind: ServiceAccount name: bookinfo-reviews placement: clusterAffinity: clusterNames: - member1 - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: ratings-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: ratings-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-ratings placement: clusterAffinity: clusterNames: - member2 EOF Deploy fortio application using the kubectl command: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/httpbin/sample-client/fortio-deploy.yaml Run the following command to create propagation policy for the fortio services: cat <<EOF | kubectl apply -nistio-demo -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: fortio-propagation spec: resourceSelectors: - apiVersion: v1 kind: Service name: fortio - apiVersion: apps/v1 kind: Deployment name: fortio-deploy placement: clusterAffinity: clusterNames: - member1 - member2 EOF Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 Run the following command to verify productpage application installation: export FORTIO_POD=`kubectl get po -nistio-demo | grep fortio | awk '{print $1}'` kubectl exec -it ${FORTIO_POD} -nistio-demo -- fortio load -t 3s productpage:9080/productpage What's next Folling the guide to confirm the app is accessible from outside the cluster.","title":"Use Istio on Karmada"},{"location":"istio-on-karmada/#use-istio-on-karmada","text":"This document uses an example to demonstrate how to use Istio on Karmada. Follow this guide to install the Istio control plane on karmada-host (the primary cluster) and configure member1 and member2 (the remote cluster) to use the control plane in karmada-host . All clusters reside on the network1 network, meaning there is direct connectivity between the pods in both clusters.","title":"Use Istio on Karmada"},{"location":"istio-on-karmada/#install-karmada","text":"","title":"Install Karmada"},{"location":"istio-on-karmada/#install-karmada-control-plane","text":"Following the steps Install karmada control plane in Quick Start, you can get a Karmada.","title":"Install karmada control plane"},{"location":"istio-on-karmada/#deploy-istio","text":"If you are testing multicluster setup on kind you can use MetalLB to make use of EXTERNAL-IP for LoadBalancer services.","title":"Deploy Istio"},{"location":"istio-on-karmada/#install-istioctl","text":"Please refer to the istioctl Installation.","title":"Install istioctl"},{"location":"istio-on-karmada/#prepare-ca-certificates","text":"Following the steps plug-in-certificates-and-key-into-the-cluster to configure Istio CA. Replace the cluster name cluster1 with primary , the output will looks like as follwing: root@karmada-demo istio-on-karmada# tree certs certs \u251c\u2500\u2500 primary \u2502 \u251c\u2500\u2500 ca-cert.pem \u2502 \u251c\u2500\u2500 ca-key.pem \u2502 \u251c\u2500\u2500 cert-chain.pem \u2502 \u2514\u2500\u2500 root-cert.pem \u251c\u2500\u2500 root-ca.conf \u251c\u2500\u2500 root-cert.csr \u251c\u2500\u2500 root-cert.pem \u251c\u2500\u2500 root-cert.srl \u2514\u2500\u2500 root-key.pem","title":"Prepare CA certificates"},{"location":"istio-on-karmada/#install-istio-on-karmada-apiserver","text":"Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Create a secret cacerts in istio-system namespace: kubectl create namespace istio-system kubectl create secret generic cacerts -n istio-system \\ --from-file=certs/primary/ca-cert.pem \\ --from-file=certs/primary/ca-key.pem \\ --from-file=certs/primary/root-cert.pem \\ --from-file=certs/primary/cert-chain.pem Create a propagation policy for cacert secret: cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: cacerts-propagation namespace: istio-system spec: resourceSelectors: - apiVersion: v1 kind: Secret name: cacerts placement: clusterAffinity: clusterNames: - member1 - member2 EOF Run the following command to install istio CRDs on karmada apiserver: cat <<EOF | istioctl install -y --set profile=minimal -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout values: global: meshID: mesh1 multiCluster: clusterName: primary network: network1 EOF Karmada apiserver will not deploy a real istiod pod, you should press ctrl+c to exit installation when Processing resources for Istiod . \u2714 Istio core installed - Processing resources for Istiod.","title":"Install Istio on karmada-apiserver"},{"location":"istio-on-karmada/#install-istio-on-karmada-host","text":"Create secret on karmada-host Karmada host is not a member cluster, we need create the cacerts secret for istiod . Export KUBECONFIG and switch to karmada host : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-host Create a secret cacerts in istio-system namespace: kubectl create namespace istio-system kubectl create secret generic cacerts -n istio-system \\ --from-file=certs/primary/ca-cert.pem \\ --from-file=certs/primary/ca-key.pem \\ --from-file=certs/primary/root-cert.pem \\ --from-file=certs/primary/cert-chain.pem Create istio-kubeconfig on karmada-host kubectl get secret -nkarmada-system kubeconfig --template={{.data.kubeconfig}} | base64 -d > kind-karmada.yaml kubectl create secret generic istio-kubeconfig --from-file=config=kind-karmada.yaml -nistio-system Install istio control plane cat <<EOF | istioctl install -y --set profile=minimal -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: meshConfig: accessLogFile: /dev/stdout values: global: meshID: mesh1 multiCluster: clusterName: primary network: network1 EOF Expose istiod service Run the following command to create a service for the istiod service: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: istiod-elb namespace: istio-system spec: ports: - name: https-dns port: 15012 protocol: TCP targetPort: 15012 selector: app: istiod istio: pilot sessionAffinity: None type: LoadBalancer EOF Export DISCOVERY_ADDRESS: export DISCOVERY_ADDRESS=$(kubectl get svc istiod-elb -nistio-system -o jsonpath='{.status.loadBalancer.ingress[0].ip}') # verify echo $DISCOVERY_ADDRESS","title":"Install Istio on karmada host"},{"location":"istio-on-karmada/#prepare-member1-cluster-secret","text":"Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 Create istio remote secret for member1: istioctl x create-remote-secret --name=member1 > istio-remote-secret-member1.yaml","title":"Prepare member1 cluster secret"},{"location":"istio-on-karmada/#prepare-member2-cluster-secret","text":"Export KUBECONFIG and switch to karmada member2 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member2 Create istio remote secret for member1: istioctl x create-remote-secret --name=member2 > istio-remote-secret-member2.yaml","title":"Prepare member2 cluster secret"},{"location":"istio-on-karmada/#apply-istio-remote-secret","text":"Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Apply istio remote secret: kubectl apply -f istio-remote-secret-member1.yaml kubectl apply -f istio-remote-secret-member2.yaml","title":"Apply istio remote secret"},{"location":"istio-on-karmada/#install-istio-remote","text":"Install istio remote member1 Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 cat <<EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: meshID: mesh1 multiCluster: clusterName: member1 network: network1 remotePilotAddress: ${DISCOVERY_ADDRESS} EOF Install istio remote member2 Export KUBECONFIG and switch to karmada member2 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member2 cat <<EOF | istioctl install -y -f - apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: meshID: mesh1 multiCluster: clusterName: member2 network: network1 remotePilotAddress: ${DISCOVERY_ADDRESS} EOF","title":"Install istio remote"},{"location":"istio-on-karmada/#deploy-bookinfo-application","text":"Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Create an istio-demo namespace: kubectl create namespace istio-demo Label the namespace that will host the application with istio-injection=enabled : kubectl label namespace istio-demo istio-injection=enabled Deploy your application using the kubectl command: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml Run the following command to create default destination rules for the Bookinfo services: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/networking/destination-rule-all.yaml Run the following command to create virtual service for the Bookinfo services: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/networking/virtual-service-all-v1.yaml Run the following command to create propagation policy for the Bookinfo services: cat <<EOF | kubectl apply -nistio-demo -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: service-propagation spec: resourceSelectors: - apiVersion: v1 kind: Service name: productpage - apiVersion: v1 kind: Service name: details - apiVersion: v1 kind: Service name: reviews - apiVersion: v1 kind: Service name: ratings placement: clusterAffinity: clusterNames: - member1 - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: produtpage-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: productpage-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-productpage placement: clusterAffinity: clusterNames: - member1 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: details-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: details-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-details placement: clusterAffinity: clusterNames: - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: reviews-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: reviews-v1 - apiVersion: apps/v1 kind: Deployment name: reviews-v2 - apiVersion: apps/v1 kind: Deployment name: reviews-v3 - apiVersion: v1 kind: ServiceAccount name: bookinfo-reviews placement: clusterAffinity: clusterNames: - member1 - member2 --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: ratings-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: ratings-v1 - apiVersion: v1 kind: ServiceAccount name: bookinfo-ratings placement: clusterAffinity: clusterNames: - member2 EOF Deploy fortio application using the kubectl command: kubectl apply -nistio-demo -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/httpbin/sample-client/fortio-deploy.yaml Run the following command to create propagation policy for the fortio services: cat <<EOF | kubectl apply -nistio-demo -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: fortio-propagation spec: resourceSelectors: - apiVersion: v1 kind: Service name: fortio - apiVersion: apps/v1 kind: Deployment name: fortio-deploy placement: clusterAffinity: clusterNames: - member1 - member2 EOF Export KUBECONFIG and switch to karmada member1 : export KUBECONFIG=\"$HOME/.kube/members.config\" kubectl config use-context member1 Run the following command to verify productpage application installation: export FORTIO_POD=`kubectl get po -nistio-demo | grep fortio | awk '{print $1}'` kubectl exec -it ${FORTIO_POD} -nistio-demo -- fortio load -t 3s productpage:9080/productpage","title":"Deploy bookinfo application"},{"location":"istio-on-karmada/#whats-next","text":"Folling the guide to confirm the app is accessible from outside the cluster.","title":"What's next"},{"location":"multi-cluster-ingress/","text":"Multi-cluster Ingress Users can use MultiClusterIngress API provided in Karmada to import external traffic to services in the member clusters. Prerequisites Karmada has been installed We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases. Cluster Network Currently, we need to use the MCS feature to import external traffic. So we need to ensure that the container networks between the host cluster and member clusters are connected. The host cluster indicates the cluster where the Karmada control plane is deployed. If you use the hack/local-up-karmada.sh script to deploy Karmada, Karmada will have three member clusters, and the container networks between the host cluster , member1 and member2 are connected. You can use Submariner or other related open source projects to connected networks between clusters. Example Step 1: Deploy ingress-nginx on the host cluster We use multi-cluster-ingress-nginx as the demo for demonstration. We've made some changes based on the latest version(controller-v1.1.1) of ingress-nginx . Download code // for HTTPS git clone https://github.com/karmada-io/multi-cluster-ingress-nginx.git // for SSH git clone git@github.com:karmada-io/multi-cluster-ingress-nginx.git Build and deploy ingress-nginx Using the existing karmada-host kind cluster to build and deploy the ingress controller. export KIND_CLUSTER_NAME=karmada-host kubectl config use-context karmada-host make dev-env Apply kubeconfig secret Create a secret that contains the karmada-apiserver authentication credential in the following format: # karmada-kubeconfig-secret.yaml apiVersion: v1 data: kubeconfig: {data} # Base64-encoded kind: Secret metadata: name: kubeconfig namespace: ingress-nginx type: Opaque You can get the authentication credential from the /root/.kube/karmada.config file, or use command: kubectl -n karmada-system get secret kubeconfig -oyaml | grep kubeconfig | sed -n '1p' | awk '{print $2}' Then apply yaml: kubectl apply -f karmada-kubeconfig-secret.yaml Edit ingress-nginx-controller deployment We want nginx-ingress-controller to access karmada-apiserver to listen to changes in resources(such as multiclusteringress, endpointslices, and service). Therefore, we need to mount the authentication credential of karmada-apiserver to the nginx-ingress-controller . kubectl -n ingress-nginx edit deployment ingress-nginx-controller Edit as follows: apiVersion: apps/v1 kind: Deployment metadata: ... spec: ... template: spec: containers: - args: - /nginx-ingress-controller - --karmada-kubeconfig=/etc/kubeconfig # new line ... volumeMounts: ... - mountPath: /etc/kubeconfig # new line name: kubeconfig # new line subPath: kubeconfig # new line volumes: ... - name: kubeconfig # new line secret: # new line secretName: kubeconfig # new line Step 2: Use the MCS feature to discovery service Install ServiceExport and ServiceImport CRDs Refer to here . Deploy web on member1 cluster deploy.yaml: unfold me to see the yaml apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 1 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: hello-app image: gcr.io/google-samples/hello-app:1.0 ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web spec: ports: - port: 81 targetPort: 8080 selector: app: web --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: mcs-workload spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: web - apiVersion: v1 kind: Service name: web placement: clusterAffinity: clusterNames: - member1 kubectl --context karmada-apiserver apply -f deploy.yaml Export web service from member1 cluster service_export.yaml: unfold me to see the yaml apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: web --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: web-export-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport name: web placement: clusterAffinity: clusterNames: - member1 kubectl --context karmada-apiserver apply -f service_export.yaml Import web service to member2 cluster service_import.yaml: unfold me to see the yaml apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport metadata: name: web spec: type: ClusterSetIP ports: - port: 81 protocol: TCP --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: web-import-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport name: web placement: clusterAffinity: clusterNames: - member2 kubectl --context karmada-apiserver apply -f service_import.yaml Step 3: Deploy multiclusteringress on karmada-controlplane mci-web.yaml: unfold me to see the yaml apiVersion: networking.karmada.io/v1alpha1 kind: MultiClusterIngress metadata: name: demo-localhost namespace: default spec: ingressClassName: nginx rules: - host: demo.localdev.me http: paths: - backend: service: name: web port: number: 81 path: /web pathType: Prefix kubectl --context karmada-apiserver apply -f Step 4: Local testing Let's forward a local port to the ingress controller: kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80 At this point, if you access http://demo.localdev.me:8080/web/, you should see an HTML page telling you: Hello, world! Version: 1.0.0 Hostname: web-xxx-xxx","title":"Multi-cluster Ingress"},{"location":"multi-cluster-ingress/#multi-cluster-ingress","text":"Users can use MultiClusterIngress API provided in Karmada to import external traffic to services in the member clusters.","title":"Multi-cluster Ingress"},{"location":"multi-cluster-ingress/#prerequisites","text":"","title":"Prerequisites"},{"location":"multi-cluster-ingress/#karmada-has-been-installed","text":"We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases.","title":"Karmada has been installed"},{"location":"multi-cluster-ingress/#cluster-network","text":"Currently, we need to use the MCS feature to import external traffic. So we need to ensure that the container networks between the host cluster and member clusters are connected. The host cluster indicates the cluster where the Karmada control plane is deployed. If you use the hack/local-up-karmada.sh script to deploy Karmada, Karmada will have three member clusters, and the container networks between the host cluster , member1 and member2 are connected. You can use Submariner or other related open source projects to connected networks between clusters.","title":"Cluster Network"},{"location":"multi-cluster-ingress/#example","text":"","title":"Example"},{"location":"multi-cluster-ingress/#step-1-deploy-ingress-nginx-on-the-host-cluster","text":"We use multi-cluster-ingress-nginx as the demo for demonstration. We've made some changes based on the latest version(controller-v1.1.1) of ingress-nginx .","title":"Step 1: Deploy ingress-nginx on the host cluster"},{"location":"multi-cluster-ingress/#download-code","text":"// for HTTPS git clone https://github.com/karmada-io/multi-cluster-ingress-nginx.git // for SSH git clone git@github.com:karmada-io/multi-cluster-ingress-nginx.git","title":"Download code"},{"location":"multi-cluster-ingress/#build-and-deploy-ingress-nginx","text":"Using the existing karmada-host kind cluster to build and deploy the ingress controller. export KIND_CLUSTER_NAME=karmada-host kubectl config use-context karmada-host make dev-env","title":"Build and deploy ingress-nginx"},{"location":"multi-cluster-ingress/#apply-kubeconfig-secret","text":"Create a secret that contains the karmada-apiserver authentication credential in the following format: # karmada-kubeconfig-secret.yaml apiVersion: v1 data: kubeconfig: {data} # Base64-encoded kind: Secret metadata: name: kubeconfig namespace: ingress-nginx type: Opaque You can get the authentication credential from the /root/.kube/karmada.config file, or use command: kubectl -n karmada-system get secret kubeconfig -oyaml | grep kubeconfig | sed -n '1p' | awk '{print $2}' Then apply yaml: kubectl apply -f karmada-kubeconfig-secret.yaml","title":"Apply kubeconfig secret"},{"location":"multi-cluster-ingress/#edit-ingress-nginx-controller-deployment","text":"We want nginx-ingress-controller to access karmada-apiserver to listen to changes in resources(such as multiclusteringress, endpointslices, and service). Therefore, we need to mount the authentication credential of karmada-apiserver to the nginx-ingress-controller . kubectl -n ingress-nginx edit deployment ingress-nginx-controller Edit as follows: apiVersion: apps/v1 kind: Deployment metadata: ... spec: ... template: spec: containers: - args: - /nginx-ingress-controller - --karmada-kubeconfig=/etc/kubeconfig # new line ... volumeMounts: ... - mountPath: /etc/kubeconfig # new line name: kubeconfig # new line subPath: kubeconfig # new line volumes: ... - name: kubeconfig # new line secret: # new line secretName: kubeconfig # new line","title":"Edit ingress-nginx-controller deployment"},{"location":"multi-cluster-ingress/#step-2-use-the-mcs-feature-to-discovery-service","text":"","title":"Step 2: Use the MCS feature to discovery service"},{"location":"multi-cluster-ingress/#install-serviceexport-and-serviceimport-crds","text":"Refer to here .","title":"Install ServiceExport and ServiceImport CRDs"},{"location":"multi-cluster-ingress/#deploy-web-on-member1-cluster","text":"deploy.yaml: unfold me to see the yaml apiVersion: apps/v1 kind: Deployment metadata: name: web spec: replicas: 1 selector: matchLabels: app: web template: metadata: labels: app: web spec: containers: - name: hello-app image: gcr.io/google-samples/hello-app:1.0 ports: - containerPort: 8080 protocol: TCP --- apiVersion: v1 kind: Service metadata: name: web spec: ports: - port: 81 targetPort: 8080 selector: app: web --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: mcs-workload spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: web - apiVersion: v1 kind: Service name: web placement: clusterAffinity: clusterNames: - member1 kubectl --context karmada-apiserver apply -f deploy.yaml","title":"Deploy web on member1 cluster"},{"location":"multi-cluster-ingress/#export-web-service-from-member1-cluster","text":"service_export.yaml: unfold me to see the yaml apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: web --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: web-export-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport name: web placement: clusterAffinity: clusterNames: - member1 kubectl --context karmada-apiserver apply -f service_export.yaml","title":"Export web service from member1 cluster"},{"location":"multi-cluster-ingress/#import-web-service-to-member2-cluster","text":"service_import.yaml: unfold me to see the yaml apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport metadata: name: web spec: type: ClusterSetIP ports: - port: 81 protocol: TCP --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: web-import-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport name: web placement: clusterAffinity: clusterNames: - member2 kubectl --context karmada-apiserver apply -f service_import.yaml","title":"Import web service to member2 cluster"},{"location":"multi-cluster-ingress/#step-3-deploy-multiclusteringress-on-karmada-controlplane","text":"mci-web.yaml: unfold me to see the yaml apiVersion: networking.karmada.io/v1alpha1 kind: MultiClusterIngress metadata: name: demo-localhost namespace: default spec: ingressClassName: nginx rules: - host: demo.localdev.me http: paths: - backend: service: name: web port: number: 81 path: /web pathType: Prefix kubectl --context karmada-apiserver apply -f","title":"Step 3: Deploy multiclusteringress on karmada-controlplane"},{"location":"multi-cluster-ingress/#step-4-local-testing","text":"Let's forward a local port to the ingress controller: kubectl port-forward --namespace=ingress-nginx service/ingress-nginx-controller 8080:80 At this point, if you access http://demo.localdev.me:8080/web/, you should see an HTML page telling you: Hello, world! Version: 1.0.0 Hostname: web-xxx-xxx","title":"Step 4: Local testing"},{"location":"multi-cluster-service/","text":"Multi-cluster Service Discovery Users are able to export and import services between clusters with Multi-cluster Service APIs . Prerequisites Karmada has been installed We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases. Member Cluster Network Ensure that at least two clusters have been added to Karmada, and the container networks between member clusters are connected. If you use the hack/local-up-karmada.sh script to deploy Karmada, Karmada will have three member clusters, and the container networks of the member1 and member2 will be connected. You can use Submariner or other related open source projects to connected networks between member clusters. The ServiceExport and ServiceImport CRDs have been installed We need to install ServiceExport and ServiceImport in the member clusters. After ServiceExport and ServiceImport have been installed on the karmada control-plane , we can create ClusterPropagationPolicy to propagate those two CRDs to the member clusters. # propagate ServiceExport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceexport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceexports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 --- # propagate ServiceImport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceimport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceimports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 Example Step 1: Deploy service on the member1 cluster We need to deploy service on the member1 cluster for discovery. apiVersion: apps/v1 kind: Deployment metadata: name: serve spec: replicas: 1 selector: matchLabels: app: serve template: metadata: labels: app: serve spec: containers: - name: serve image: jeremyot/serve:0a40de8 args: - \"--message='hello from cluster 1 (Node: {{env \\\"NODE_NAME\\\"}} Pod: {{env \\\"POD_NAME\\\"}} Address: {{addr}})'\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name --- apiVersion: v1 kind: Service metadata: name: serve spec: ports: - port: 80 targetPort: 8080 selector: app: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: mcs-workload spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: serve - apiVersion: v1 kind: Service name: serve placement: clusterAffinity: clusterNames: - member1 Step 2: Export service to the member2 cluster Create a ServiceExport object on karmada control-plane , and then create a PropagationPolicy to propagate the ServiceExport object to the member1 cluster. apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-export-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport name: serve placement: clusterAffinity: clusterNames: - member1 Create a ServiceImport object on karmada control-plane , and then create a PropagationPlicy to propagate the ServiceImport object to the member2 cluster. apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport metadata: name: serve spec: type: ClusterSetIP ports: - port: 80 protocol: TCP --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-import-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport name: serve placement: clusterAffinity: clusterNames: - member2 Step 3: Consume service from member2 cluster After the above steps, we can find the derived service which has the prefix derived- on the member2 cluster. Then, we can access the derived service to access the service on the member1 cluster. Start a Pod request on the member2 cluster to access the ClusterIP of derived service : kubectl run -i --rm --restart=Never --image=jeremyot/request:0a40de8 request -- --duration={duration-time} --address={ClusterIP of derived service}","title":"Multi-cluster Service Discovery"},{"location":"multi-cluster-service/#multi-cluster-service-discovery","text":"Users are able to export and import services between clusters with Multi-cluster Service APIs .","title":"Multi-cluster Service Discovery"},{"location":"multi-cluster-service/#prerequisites","text":"","title":"Prerequisites"},{"location":"multi-cluster-service/#karmada-has-been-installed","text":"We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases.","title":"Karmada has been installed"},{"location":"multi-cluster-service/#member-cluster-network","text":"Ensure that at least two clusters have been added to Karmada, and the container networks between member clusters are connected. If you use the hack/local-up-karmada.sh script to deploy Karmada, Karmada will have three member clusters, and the container networks of the member1 and member2 will be connected. You can use Submariner or other related open source projects to connected networks between member clusters.","title":"Member Cluster Network"},{"location":"multi-cluster-service/#the-serviceexport-and-serviceimport-crds-have-been-installed","text":"We need to install ServiceExport and ServiceImport in the member clusters. After ServiceExport and ServiceImport have been installed on the karmada control-plane , we can create ClusterPropagationPolicy to propagate those two CRDs to the member clusters. # propagate ServiceExport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceexport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceexports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 --- # propagate ServiceImport CRD apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceimport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceimports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2","title":"The ServiceExport and ServiceImport CRDs have been installed"},{"location":"multi-cluster-service/#example","text":"","title":"Example"},{"location":"multi-cluster-service/#step-1-deploy-service-on-the-member1-cluster","text":"We need to deploy service on the member1 cluster for discovery. apiVersion: apps/v1 kind: Deployment metadata: name: serve spec: replicas: 1 selector: matchLabels: app: serve template: metadata: labels: app: serve spec: containers: - name: serve image: jeremyot/serve:0a40de8 args: - \"--message='hello from cluster 1 (Node: {{env \\\"NODE_NAME\\\"}} Pod: {{env \\\"POD_NAME\\\"}} Address: {{addr}})'\" env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name --- apiVersion: v1 kind: Service metadata: name: serve spec: ports: - port: 80 targetPort: 8080 selector: app: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: mcs-workload spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: serve - apiVersion: v1 kind: Service name: serve placement: clusterAffinity: clusterNames: - member1","title":"Step 1: Deploy service on the member1 cluster"},{"location":"multi-cluster-service/#step-2-export-service-to-the-member2-cluster","text":"Create a ServiceExport object on karmada control-plane , and then create a PropagationPolicy to propagate the ServiceExport object to the member1 cluster. apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: serve --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-export-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport name: serve placement: clusterAffinity: clusterNames: - member1 Create a ServiceImport object on karmada control-plane , and then create a PropagationPlicy to propagate the ServiceImport object to the member2 cluster. apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport metadata: name: serve spec: type: ClusterSetIP ports: - port: 80 protocol: TCP --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: serve-import-policy spec: resourceSelectors: - apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceImport name: serve placement: clusterAffinity: clusterNames: - member2","title":"Step 2: Export service to the member2 cluster"},{"location":"multi-cluster-service/#step-3-consume-service-from-member2-cluster","text":"After the above steps, we can find the derived service which has the prefix derived- on the member2 cluster. Then, we can access the derived service to access the service on the member1 cluster. Start a Pod request on the member2 cluster to access the ClusterIP of derived service : kubectl run -i --rm --restart=Never --image=jeremyot/request:0a40de8 request -- --duration={duration-time} --address={ClusterIP of derived service}","title":"Step 3: Consume service from member2 cluster"},{"location":"object-association-map/","text":"Karmada Object association mapping Review This picture is made by draw.io. If you need to update the Review , you can use the file object-association-map.drawio . Label/Annotation information table Note: These labels and annotations are managed by the Karmada. Please do not modify them. Object Tag KeyName Usage ResourceTemplate label propagationpolicy.karmada.io/namespace propagationpolicy.karmada.io/name The labels can be used to determine whether the current resource template is claimed by PropagationPolicy. label clusterpropagationpolicy.karmada.io/name The label can be used to determine whether the current resource template is claimed by ClusterPropagationPolicy. ResourceBinding label propagationpolicy.karmada.io/namespace propagationpolicy.karmada.io/name Through those two labels, logic can find the associated ResourceBinding from the PropagationPolicy or trace it back from the ResourceBinding to the corresponding PropagationPolicy. label clusterpropagationpolicy.karmada.io/name Through the label, logic can find the associated ResourceBinding from the ClusterPropagationPolicy or trace it back from the ResourceBinding to the corresponding ClusterPropagationPolicy. annotation policy.karmada.io/applied-placement Record applied placement declaration. The placement could be either PropagationPolicy's or ClusterPropagationPolicy's. ClusterResourceBinding label clusterpropagationpolicy.karmada.io/name Through the label, logic can find the associated ClusterResourceBinding from the ClusterPropagationPolicy or trace it back from the ClusterResourceBinding to the corresponding ClusterPropagationPolicy. annotation policy.karmada.io/applied-placement Record applied placement declaration. The placement could be either PropagationPolicy's or ClusterPropagationPolicy's. Work label resourcebinding.karmada.io/namespace resourcebinding.karmada.io/name Through those two labels, logic can find the associated WorkList from the ResourceBinding or trace it back from the Work to the corresponding ResourceBinding. label clusterresourcebinding.karmada.io/name Through the label, logic can find the associated WorkList from the ClusterResourceBinding or trace it back from the Work to the corresponding ClusterResourceBinding. label propagation.karmada.io/instruction Valid values includes: - suppressed: indicates that the resource should not be propagated. label endpointslice.karmada.io/namespace endpointslice.karmada.io/name Those labels are added to work object, which is report by member cluster, to specify service associated with EndpointSlice. annotation policy.karmada.io/applied-overrides Record override items, the overrides items should be sorted alphabetically in ascending order by OverridePolicy's name. annotation policy.karmada.io/applied-cluster-overrides Record override items, the overrides items should be sorted alphabetically in ascending order by ClusterOverridePolicy's name. Workload label work.karmada.io/namespace work.karmada.io/name The labels can be used to determine whether the current workload is managed by karmada. Through those labels, logic can find the associated Work or trace it back from the Work to the corresponding Workload.","title":"Karmada Object association mapping"},{"location":"object-association-map/#karmada-object-association-mapping","text":"","title":"Karmada Object association mapping"},{"location":"object-association-map/#review","text":"This picture is made by draw.io. If you need to update the Review , you can use the file object-association-map.drawio .","title":"Review"},{"location":"object-association-map/#labelannotation-information-table","text":"Note: These labels and annotations are managed by the Karmada. Please do not modify them. Object Tag KeyName Usage ResourceTemplate label propagationpolicy.karmada.io/namespace propagationpolicy.karmada.io/name The labels can be used to determine whether the current resource template is claimed by PropagationPolicy. label clusterpropagationpolicy.karmada.io/name The label can be used to determine whether the current resource template is claimed by ClusterPropagationPolicy. ResourceBinding label propagationpolicy.karmada.io/namespace propagationpolicy.karmada.io/name Through those two labels, logic can find the associated ResourceBinding from the PropagationPolicy or trace it back from the ResourceBinding to the corresponding PropagationPolicy. label clusterpropagationpolicy.karmada.io/name Through the label, logic can find the associated ResourceBinding from the ClusterPropagationPolicy or trace it back from the ResourceBinding to the corresponding ClusterPropagationPolicy. annotation policy.karmada.io/applied-placement Record applied placement declaration. The placement could be either PropagationPolicy's or ClusterPropagationPolicy's. ClusterResourceBinding label clusterpropagationpolicy.karmada.io/name Through the label, logic can find the associated ClusterResourceBinding from the ClusterPropagationPolicy or trace it back from the ClusterResourceBinding to the corresponding ClusterPropagationPolicy. annotation policy.karmada.io/applied-placement Record applied placement declaration. The placement could be either PropagationPolicy's or ClusterPropagationPolicy's. Work label resourcebinding.karmada.io/namespace resourcebinding.karmada.io/name Through those two labels, logic can find the associated WorkList from the ResourceBinding or trace it back from the Work to the corresponding ResourceBinding. label clusterresourcebinding.karmada.io/name Through the label, logic can find the associated WorkList from the ClusterResourceBinding or trace it back from the Work to the corresponding ClusterResourceBinding. label propagation.karmada.io/instruction Valid values includes: - suppressed: indicates that the resource should not be propagated. label endpointslice.karmada.io/namespace endpointslice.karmada.io/name Those labels are added to work object, which is report by member cluster, to specify service associated with EndpointSlice. annotation policy.karmada.io/applied-overrides Record override items, the overrides items should be sorted alphabetically in ascending order by OverridePolicy's name. annotation policy.karmada.io/applied-cluster-overrides Record override items, the overrides items should be sorted alphabetically in ascending order by ClusterOverridePolicy's name. Workload label work.karmada.io/namespace work.karmada.io/name The labels can be used to determine whether the current workload is managed by karmada. Through those labels, logic can find the associated Work or trace it back from the Work to the corresponding Workload.","title":"Label/Annotation information table"},{"location":"reserved-namespaces/","text":"Reserved Namespaces Note: Avoid creating namespaces with the prefix kube- and karmada- , since they are reserved for Kubernetes and Karmada system namespaces. For now, resources under the following namespaces will not be propagated: namespaces prefix kube- (including but not limited to kube-system , kube-public , kube-node-lease ) karmada-system karmada-cluster karmada-es-*","title":"Reserved Namespaces"},{"location":"reserved-namespaces/#reserved-namespaces","text":"Note: Avoid creating namespaces with the prefix kube- and karmada- , since they are reserved for Kubernetes and Karmada system namespaces. For now, resources under the following namespaces will not be propagated: namespaces prefix kube- (including but not limited to kube-system , kube-public , kube-node-lease ) karmada-system karmada-cluster karmada-es-*","title":"Reserved Namespaces"},{"location":"scheduler-estimator/","text":"Cluster Accurate Scheduler Estimator Users could divide their replicas of a workload into different clusters in terms of available resources of member clusters. When some clusters are lack of resources, scheduler would not assign excessive replicas into these clusters by calling karmada-scheduler-estimator. Prerequisites Karmada has been installed We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases. Member cluster component is ready Ensure that all member clusters has been joined and their corresponding karmada-scheduler-estimator is installed into karmada-host. You could check by using the following command: # check whether the member cluster has been joined $ kubectl get cluster NAME VERSION MODE READY AGE member1 v1.19.1 Push True 11m member2 v1.19.1 Push True 11m member3 v1.19.1 Pull True 5m12s # check whether the karmada-scheduler-estimator of a member cluster has been working well $ kubectl --context karmada-host get pod -n karmada-system | grep estimator karmada-scheduler-estimator-member1-696b54fd56-xt789 1/1 Running 0 77s karmada-scheduler-estimator-member2-774fb84c5d-md4wt 1/1 Running 0 75s karmada-scheduler-estimator-member3-5c7d87f4b4-76gv9 1/1 Running 0 72s If the cluster has not been joined, you could use hack/deploy-agent-and-estimator.sh to deploy both karmada-agent and karmada-scheduler-estimator. If the cluster has been joined already, you could use hack/deploy-scheduler-estimator.sh to only deploy karmada-scheduler-estimator. Scheduler option '--enable-scheduler-estimator' After all member clusters has been joined and estimators are all ready, please specify the option --enable-scheduler-estimator=true to enable scheduler estimator. # edit the deployment of karmada-scheduler $ kubectl --context karmada-host edit -n karmada-system deployments.apps karmada-scheduler And then add the option --enable-scheduler-estimator=true into the command of container karmada-scheduler . Example Now we could divide the replicas into different member clusters. Note that propagationPolicy.spec.replicaScheduling.replicaSchedulingType must be Divided and propagationPolicy.spec.replicaScheduling.replicaDivisionPreference must be Aggregated . The scheduler will try to divide the replicas aggregately in terms of all available resources of member clusters. apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: aggregated-policy spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 replicaScheduling: replicaSchedulingType: Divided replicaDivisionPreference: Aggregated apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 name: web-1 resources: requests: cpu: \"1\" memory: 2Gi You will find all replicas have been assigned to as few clusters as possible. $ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 5/5 5 5 2m16s $ kubectl get rb nginx-deployment -o=custom-columns=NAME:.metadata.name,CLUSTER:.spec.clusters NAME CLUSTER nginx-deployment [map[name:member1 replicas:5] map[name:member2] map[name:member3]] After that, we change the resource request of the deployment to a large number and have a try again. apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 name: web-1 resources: requests: cpu: \"100\" memory: 200Gi As any node of member clusters does not have so many cpu and memory, so we will find workload scheduling failed. $ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 0/5 0 0 2m20s $ kubectl get rb nginx-deployment -o=custom-columns=NAME:.metadata.name,CLUSTER:.spec.clusters NAME CLUSTER nginx-deployment <none>","title":"Cluster Accurate Scheduler Estimator"},{"location":"scheduler-estimator/#cluster-accurate-scheduler-estimator","text":"Users could divide their replicas of a workload into different clusters in terms of available resources of member clusters. When some clusters are lack of resources, scheduler would not assign excessive replicas into these clusters by calling karmada-scheduler-estimator.","title":"Cluster Accurate Scheduler Estimator"},{"location":"scheduler-estimator/#prerequisites","text":"","title":"Prerequisites"},{"location":"scheduler-estimator/#karmada-has-been-installed","text":"We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases.","title":"Karmada has been installed"},{"location":"scheduler-estimator/#member-cluster-component-is-ready","text":"Ensure that all member clusters has been joined and their corresponding karmada-scheduler-estimator is installed into karmada-host. You could check by using the following command: # check whether the member cluster has been joined $ kubectl get cluster NAME VERSION MODE READY AGE member1 v1.19.1 Push True 11m member2 v1.19.1 Push True 11m member3 v1.19.1 Pull True 5m12s # check whether the karmada-scheduler-estimator of a member cluster has been working well $ kubectl --context karmada-host get pod -n karmada-system | grep estimator karmada-scheduler-estimator-member1-696b54fd56-xt789 1/1 Running 0 77s karmada-scheduler-estimator-member2-774fb84c5d-md4wt 1/1 Running 0 75s karmada-scheduler-estimator-member3-5c7d87f4b4-76gv9 1/1 Running 0 72s If the cluster has not been joined, you could use hack/deploy-agent-and-estimator.sh to deploy both karmada-agent and karmada-scheduler-estimator. If the cluster has been joined already, you could use hack/deploy-scheduler-estimator.sh to only deploy karmada-scheduler-estimator.","title":"Member cluster component is ready"},{"location":"scheduler-estimator/#scheduler-option-enable-scheduler-estimator","text":"After all member clusters has been joined and estimators are all ready, please specify the option --enable-scheduler-estimator=true to enable scheduler estimator. # edit the deployment of karmada-scheduler $ kubectl --context karmada-host edit -n karmada-system deployments.apps karmada-scheduler And then add the option --enable-scheduler-estimator=true into the command of container karmada-scheduler .","title":"Scheduler option '--enable-scheduler-estimator'"},{"location":"scheduler-estimator/#example","text":"Now we could divide the replicas into different member clusters. Note that propagationPolicy.spec.replicaScheduling.replicaSchedulingType must be Divided and propagationPolicy.spec.replicaScheduling.replicaDivisionPreference must be Aggregated . The scheduler will try to divide the replicas aggregately in terms of all available resources of member clusters. apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: aggregated-policy spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 replicaScheduling: replicaSchedulingType: Divided replicaDivisionPreference: Aggregated apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 name: web-1 resources: requests: cpu: \"1\" memory: 2Gi You will find all replicas have been assigned to as few clusters as possible. $ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 5/5 5 5 2m16s $ kubectl get rb nginx-deployment -o=custom-columns=NAME:.metadata.name,CLUSTER:.spec.clusters NAME CLUSTER nginx-deployment [map[name:member1 replicas:5] map[name:member2] map[name:member3]] After that, we change the resource request of the deployment to a large number and have a try again. apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 5 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ports: - containerPort: 80 name: web-1 resources: requests: cpu: \"100\" memory: 200Gi As any node of member clusters does not have so many cpu and memory, so we will find workload scheduling failed. $ kubectl get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 0/5 0 0 2m20s $ kubectl get rb nginx-deployment -o=custom-columns=NAME:.metadata.name,CLUSTER:.spec.clusters NAME CLUSTER nginx-deployment <none>","title":"Example"},{"location":"troubleshooting/","text":"Troubleshooting I can't access some resources when install Karmada Pulling images from Google Container Registry(k8s.gcr.io) You may run the following command to change the image registry in the mainland China sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/karmada-etcd.yaml sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/karmada-apiserver.yaml sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/kube-controller-manager.yaml Download golang package in the mainland China, run the following command before installing ```shell export GOPROXY=https://goproxy.cn","title":"troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"troubleshooting/#i-cant-access-some-resources-when-install-karmada","text":"Pulling images from Google Container Registry(k8s.gcr.io) You may run the following command to change the image registry in the mainland China sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/karmada-etcd.yaml sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/karmada-apiserver.yaml sed -i'' -e \"s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g\" artifacts/deploy/kube-controller-manager.yaml Download golang package in the mainland China, run the following command before installing ```shell export GOPROXY=https://goproxy.cn","title":"I can't access some resources when install Karmada"},{"location":"working-with-anp/","text":"Deploy apiserver-network-proxy (ANP) Purpose For a member cluster that joins karmada in pull mode, we need to provide a method to connect the network between the karmada control plane and the member cluster, so that karmada-aggregated-apiserver can access this member cluster. Deploying ANP to achieve appeal is one of the methods. This article describes how to deploy ANP in karmada. Environment Karmada deployed using the kind tool. We can directly hack/local-up-karmada.sh to deploy karmada. Actions Step 1: Download code To facilitate demonstration, the code is modified based on ANP v0.0.24 to support access to the front server through HTTP. Here is the code base address: https://github.com/mrlihanbo/apiserver-network-proxy/tree/v0.0.24/dev. git clone -b v0.0.24/dev https://github.com/mrlihanbo/apiserver-network-proxy.git cd apiserver-network-proxy/ Step 2: Compile images Compile the proxy-server and proxy-agent images. docker build . --build-arg ARCH=amd64 -f artifacts/images/agent-build.Dockerfile -t swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 docker build . --build-arg ARCH=amd64 -f artifacts/images/server-build.Dockerfile -t swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24 Step 3: Generate certificate Run the command to check the IP address of karmada-host-control-plane: docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane Run the make certs command to generate a certificate and specify PROXY_SERVER_IP as the IP address obtained in the preceding command. make certs PROXY_SERVER_IP=x.x.x.x The generated certificate is in the certs folder. Step 4: Deploy proxy-server Save the proxy-server.yaml file in the root directory of the ANP code. unfold me to see the yaml # proxy-server.yaml apiVersion: apps/v1 kind: Deployment metadata: name: proxy-server namespace: karmada-system spec: replicas: 1 selector: matchLabels: app: proxy-server template: metadata: labels: app: proxy-server spec: containers: - command: - /proxy-server args: - --health-port=8092 - --cluster-ca-cert=/var/certs/server/cluster-ca-cert.crt - --cluster-cert=/var/certs/server/cluster-cert.crt - --cluster-key=/var/certs/server/cluster-key.key - --mode=http-connect - --proxy-strategies=destHost - --server-ca-cert=/var/certs/server/server-ca-cert.crt - --server-cert=/var/certs/server/server-cert.crt - --server-key=/var/certs/server/server-key.key image: swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 8092 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 60 name: proxy-server volumeMounts: - mountPath: /var/certs/server name: cert restartPolicy: Always hostNetwork: true volumes: - name: cert secret: secretName: proxy-server-cert --- apiVersion: v1 kind: Secret metadata: name: proxy-server-cert namespace: karmada-system type: Opaque data: server-ca-cert.crt: | {{server_ca_cert}} server-cert.crt: | {{server_cert}} server-key.key: | {{server_key}} cluster-ca-cert.crt: | {{cluster_ca_cert}} cluster-cert.crt: | {{cluster_cert}} cluster-key.key: | {{cluster_key}} Save the replace-proxy-server.sh file in the root directory of the ANP code. unfold me to see the shell #!/bin/bash cert_yaml=proxy-server.yaml SERVER_CA_CERT=$(cat certs/frontend/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_ca_cert}}/${SERVER_CA_CERT}/g\" ${cert_yaml} SERVER_CERT=$(cat certs/frontend/issued/proxy-frontend.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_cert}}/${SERVER_CERT}/g\" ${cert_yaml} SERVER_KEY=$(cat certs/frontend/private/proxy-frontend.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_key}}/${SERVER_KEY}/g\" ${cert_yaml} CLUSTER_CA_CERT=$(cat certs/agent/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_ca_cert}}/${CLUSTER_CA_CERT}/g\" ${cert_yaml} CLUSTER_CERT=$(cat certs/agent/issued/proxy-frontend.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_cert}}/${CLUSTER_CERT}/g\" ${cert_yaml} CLUSTER_KEY=$(cat certs/agent/private/proxy-frontend.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_key}}/${CLUSTER_KEY}/g\" ${cert_yaml} Run the following command to run the script: chmod +x replace-proxy-server.sh bash replace-proxy-server.sh Deploying the proxy-server on the karmada control plane: kind load docker-image swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24 --name karmada-host export KUBECONFIG=/root/.kube/karmada.config kubectl --context=karmada-host apply -f proxy-server.yaml Step 5: Deploy proxy-agent Save the proxy-agent.yaml file in the root directory of the ANP code. unfold me to see the yaml # proxy-agent.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: proxy-agent name: proxy-agent namespace: karmada-system spec: replicas: 1 selector: matchLabels: app: proxy-agent template: metadata: labels: app: proxy-agent spec: containers: - command: - /proxy-agent args: - '--ca-cert=/var/certs/agent/ca.crt' - '--agent-cert=/var/certs/agent/proxy-agent.crt' - '--agent-key=/var/certs/agent/proxy-agent.key' - '--proxy-server-host={{proxy_server_addr}}' - '--proxy-server-port=8091' - '--agent-identifiers=host={{identifiers}}' image: swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 imagePullPolicy: IfNotPresent name: proxy-agent livenessProbe: httpGet: scheme: HTTP port: 8093 path: /healthz initialDelaySeconds: 15 timeoutSeconds: 60 volumeMounts: - mountPath: /var/certs/agent name: cert volumes: - name: cert secret: secretName: proxy-agent-cert --- apiVersion: v1 kind: Secret metadata: name: proxy-agent-cert namespace: karmada-system type: Opaque data: ca.crt: | {{proxy_agent_ca_crt}} proxy-agent.crt: | {{proxy_agent_crt}} proxy-agent.key: | {{proxy_agent_key}} Save the replace-proxy-agent.sh file in the root directory of the ANP code. unfold me to see the shell #!/bin/bash cert_yaml=proxy-agent.yaml karmada_controlplan_addr=$(docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane) member3_cluster_addr=$(docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' member3-control-plane) sed -i'' -e \"s/{{proxy_server_addr}}/${karmada_controlplan_addr}/g\" ${cert_yaml} sed -i'' -e \"s/{{identifiers}}/${member3_cluster_addr}/g\" ${cert_yaml} PROXY_AGENT_CA_CRT=$(cat certs/agent/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_ca_crt}}/${PROXY_AGENT_CA_CRT}/g\" ${cert_yaml} PROXY_AGENT_CRT=$(cat certs/agent/issued/proxy-agent.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_crt}}/${PROXY_AGENT_CRT}/g\" ${cert_yaml} PROXY_AGENT_KEY=$(cat certs/agent/private/proxy-agent.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_key}}/${PROXY_AGENT_KEY}/g\" ${cert_yaml} Run the following command to run the script: chmod +x replace-proxy-agent.sh bash replace-proxy-agent.sh Deploying the proxy-agent in the pull mode member cluster (in this example, cluster member3 cluster is in pull mode.): kind load docker-image swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 --name member3 kubectl --kubeconfig=/root/.kube/members.config --context=member3 apply -f proxy-agent.yaml The ANP deployment is complete. Step 6: Add command flags for karmada-agent deployment After deploying the ANP deployment, we need to add extra command flags --cluster-api-endpoint and --proxy-server-address for karmada-agent deployment in member3 cluster. Where --cluster-api-endpoint is the APIEndpoint of the cluster. You can obtain it from the KubeConfig file of the member3 cluster. Where --proxy-server-address is the address of the proxy server that is used to proxy the cluster. In current case, we can set --proxy-server-address to http://<karmada_controlplan_addr>:8088 . Get karmada_controlplan_addr value through the following command: docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane Port 8088 is set by our code modification in ANP: https://github.com/mrlihanbo/apiserver-network-proxy/blob/v0.0.24/dev/cmd/server/app/server.go#L267. You can also modify it to a different value.","title":"Deploy apiserver-network-proxy (ANP)"},{"location":"working-with-anp/#deploy-apiserver-network-proxy-anp","text":"","title":"Deploy apiserver-network-proxy (ANP)"},{"location":"working-with-anp/#purpose","text":"For a member cluster that joins karmada in pull mode, we need to provide a method to connect the network between the karmada control plane and the member cluster, so that karmada-aggregated-apiserver can access this member cluster. Deploying ANP to achieve appeal is one of the methods. This article describes how to deploy ANP in karmada.","title":"Purpose"},{"location":"working-with-anp/#environment","text":"Karmada deployed using the kind tool. We can directly hack/local-up-karmada.sh to deploy karmada.","title":"Environment"},{"location":"working-with-anp/#actions","text":"","title":"Actions"},{"location":"working-with-anp/#step-1-download-code","text":"To facilitate demonstration, the code is modified based on ANP v0.0.24 to support access to the front server through HTTP. Here is the code base address: https://github.com/mrlihanbo/apiserver-network-proxy/tree/v0.0.24/dev. git clone -b v0.0.24/dev https://github.com/mrlihanbo/apiserver-network-proxy.git cd apiserver-network-proxy/","title":"Step 1: Download code"},{"location":"working-with-anp/#step-2-compile-images","text":"Compile the proxy-server and proxy-agent images. docker build . --build-arg ARCH=amd64 -f artifacts/images/agent-build.Dockerfile -t swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 docker build . --build-arg ARCH=amd64 -f artifacts/images/server-build.Dockerfile -t swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24","title":"Step 2: Compile images"},{"location":"working-with-anp/#step-3-generate-certificate","text":"Run the command to check the IP address of karmada-host-control-plane: docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane Run the make certs command to generate a certificate and specify PROXY_SERVER_IP as the IP address obtained in the preceding command. make certs PROXY_SERVER_IP=x.x.x.x The generated certificate is in the certs folder.","title":"Step 3: Generate certificate"},{"location":"working-with-anp/#step-4-deploy-proxy-server","text":"Save the proxy-server.yaml file in the root directory of the ANP code. unfold me to see the yaml # proxy-server.yaml apiVersion: apps/v1 kind: Deployment metadata: name: proxy-server namespace: karmada-system spec: replicas: 1 selector: matchLabels: app: proxy-server template: metadata: labels: app: proxy-server spec: containers: - command: - /proxy-server args: - --health-port=8092 - --cluster-ca-cert=/var/certs/server/cluster-ca-cert.crt - --cluster-cert=/var/certs/server/cluster-cert.crt - --cluster-key=/var/certs/server/cluster-key.key - --mode=http-connect - --proxy-strategies=destHost - --server-ca-cert=/var/certs/server/server-ca-cert.crt - --server-cert=/var/certs/server/server-cert.crt - --server-key=/var/certs/server/server-key.key image: swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 8092 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 60 name: proxy-server volumeMounts: - mountPath: /var/certs/server name: cert restartPolicy: Always hostNetwork: true volumes: - name: cert secret: secretName: proxy-server-cert --- apiVersion: v1 kind: Secret metadata: name: proxy-server-cert namespace: karmada-system type: Opaque data: server-ca-cert.crt: | {{server_ca_cert}} server-cert.crt: | {{server_cert}} server-key.key: | {{server_key}} cluster-ca-cert.crt: | {{cluster_ca_cert}} cluster-cert.crt: | {{cluster_cert}} cluster-key.key: | {{cluster_key}} Save the replace-proxy-server.sh file in the root directory of the ANP code. unfold me to see the shell #!/bin/bash cert_yaml=proxy-server.yaml SERVER_CA_CERT=$(cat certs/frontend/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_ca_cert}}/${SERVER_CA_CERT}/g\" ${cert_yaml} SERVER_CERT=$(cat certs/frontend/issued/proxy-frontend.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_cert}}/${SERVER_CERT}/g\" ${cert_yaml} SERVER_KEY=$(cat certs/frontend/private/proxy-frontend.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{server_key}}/${SERVER_KEY}/g\" ${cert_yaml} CLUSTER_CA_CERT=$(cat certs/agent/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_ca_cert}}/${CLUSTER_CA_CERT}/g\" ${cert_yaml} CLUSTER_CERT=$(cat certs/agent/issued/proxy-frontend.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_cert}}/${CLUSTER_CERT}/g\" ${cert_yaml} CLUSTER_KEY=$(cat certs/agent/private/proxy-frontend.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{cluster_key}}/${CLUSTER_KEY}/g\" ${cert_yaml} Run the following command to run the script: chmod +x replace-proxy-server.sh bash replace-proxy-server.sh Deploying the proxy-server on the karmada control plane: kind load docker-image swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-server:0.0.24 --name karmada-host export KUBECONFIG=/root/.kube/karmada.config kubectl --context=karmada-host apply -f proxy-server.yaml","title":"Step 4: Deploy proxy-server"},{"location":"working-with-anp/#step-5-deploy-proxy-agent","text":"Save the proxy-agent.yaml file in the root directory of the ANP code. unfold me to see the yaml # proxy-agent.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: proxy-agent name: proxy-agent namespace: karmada-system spec: replicas: 1 selector: matchLabels: app: proxy-agent template: metadata: labels: app: proxy-agent spec: containers: - command: - /proxy-agent args: - '--ca-cert=/var/certs/agent/ca.crt' - '--agent-cert=/var/certs/agent/proxy-agent.crt' - '--agent-key=/var/certs/agent/proxy-agent.key' - '--proxy-server-host={{proxy_server_addr}}' - '--proxy-server-port=8091' - '--agent-identifiers=host={{identifiers}}' image: swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 imagePullPolicy: IfNotPresent name: proxy-agent livenessProbe: httpGet: scheme: HTTP port: 8093 path: /healthz initialDelaySeconds: 15 timeoutSeconds: 60 volumeMounts: - mountPath: /var/certs/agent name: cert volumes: - name: cert secret: secretName: proxy-agent-cert --- apiVersion: v1 kind: Secret metadata: name: proxy-agent-cert namespace: karmada-system type: Opaque data: ca.crt: | {{proxy_agent_ca_crt}} proxy-agent.crt: | {{proxy_agent_crt}} proxy-agent.key: | {{proxy_agent_key}} Save the replace-proxy-agent.sh file in the root directory of the ANP code. unfold me to see the shell #!/bin/bash cert_yaml=proxy-agent.yaml karmada_controlplan_addr=$(docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane) member3_cluster_addr=$(docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' member3-control-plane) sed -i'' -e \"s/{{proxy_server_addr}}/${karmada_controlplan_addr}/g\" ${cert_yaml} sed -i'' -e \"s/{{identifiers}}/${member3_cluster_addr}/g\" ${cert_yaml} PROXY_AGENT_CA_CRT=$(cat certs/agent/issued/ca.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_ca_crt}}/${PROXY_AGENT_CA_CRT}/g\" ${cert_yaml} PROXY_AGENT_CRT=$(cat certs/agent/issued/proxy-agent.crt | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_crt}}/${PROXY_AGENT_CRT}/g\" ${cert_yaml} PROXY_AGENT_KEY=$(cat certs/agent/private/proxy-agent.key | base64 | tr \"\\n\" \" \"|sed s/[[:space:]]//g) sed -i'' -e \"s/{{proxy_agent_key}}/${PROXY_AGENT_KEY}/g\" ${cert_yaml} Run the following command to run the script: chmod +x replace-proxy-agent.sh bash replace-proxy-agent.sh Deploying the proxy-agent in the pull mode member cluster (in this example, cluster member3 cluster is in pull mode.): kind load docker-image swr.ap-southeast-1.myhuaweicloud.com/karmada/proxy-agent:0.0.24 --name member3 kubectl --kubeconfig=/root/.kube/members.config --context=member3 apply -f proxy-agent.yaml The ANP deployment is complete.","title":"Step 5: Deploy proxy-agent"},{"location":"working-with-anp/#step-6-add-command-flags-for-karmada-agent-deployment","text":"After deploying the ANP deployment, we need to add extra command flags --cluster-api-endpoint and --proxy-server-address for karmada-agent deployment in member3 cluster. Where --cluster-api-endpoint is the APIEndpoint of the cluster. You can obtain it from the KubeConfig file of the member3 cluster. Where --proxy-server-address is the address of the proxy server that is used to proxy the cluster. In current case, we can set --proxy-server-address to http://<karmada_controlplan_addr>:8088 . Get karmada_controlplan_addr value through the following command: docker inspect --format='{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' karmada-host-control-plane Port 8088 is set by our code modification in ANP: https://github.com/mrlihanbo/apiserver-network-proxy/blob/v0.0.24/dev/cmd/server/app/server.go#L267. You can also modify it to a different value.","title":"Step 6: Add command flags for karmada-agent deployment"},{"location":"working-with-argocd/","text":"Working with Argo CD This topic walks you through how to use the Argo CD to manage your workload across clusters with Karmada . Prerequisites Argo CD Installation You have installed Argo CD following the instructions in Getting Started . Karmada Installation In this example, we are using a Karmada environment with at lease 3 member clusters joined. You can set up the environment by hack/local-up-karmada.sh , which is also used to run our E2E cases. # kubectl get clusters NAME VERSION MODE READY AGE member1 v1.19.1 Push True 18h member2 v1.19.1 Push True 18h member3 v1.19.1 Pull True 17h Registering Karmada to Argo CD This step registers Karmada control plane to Argo CD. First list the contexts of all clusters in your current kubeconfig: kubectl config get-contexts -o name Choose the context of the Karmada control plane from the list and add it to argocd cluster add CONTEXTNAME . For example, for karmada-apiserver context, run: argocd cluster add karmada-apiserver If everything goes well, you can see the registered Karmada control plane from the Argo CD UI, e.g.: Creating Apps Via UI Preparing Apps Take the guestbook as example. First, fork the argocd-example-apps repo and create a branch karmada-demo . Then, create a PropagationPolicy manifest under the guestbook directory. Creating Apps Click the + New App button as shown below: Give your app the name guestbook-multi-cluster , use the project default , and leave the sync policy as Manual : Connect the forked repo to Argo CD by setting repository url to the github repo url, set revision as karmada-demo , and set the path to guestbook : For Destination, set cluster to karmada and namespace to default : Syncing Apps You can sync your applications via UI by simply clicking the SYNC button and following the pop-up instructions, e.g.: More details please refer to argocd guide: sync the application . Checking Apps Status For deployment running in more than one clusters, you don't need to create applications for each cluster. You can get the overall and detailed status from one Application . The svc/guestbook-ui , deploy/guestbook-ui and propagationpolicy/guestbook in the middle of the picture are the resources created by the manifest in the forked repo. And the resourcebinding/guestbook-ui-service and resourcebinding/guestbook-ui-deployment in the right of the picture are the resources created by Karmada. Checking Detailed Status You can obtain the Deployment's detailed status by resourcebinding/guestbook-ui-deployment . Checking Aggregated Status You can obtain the aggregated status of the Deployment from UI by deploy/guestbook-ui .","title":"Working with Argo CD"},{"location":"working-with-argocd/#working-with-argo-cd","text":"This topic walks you through how to use the Argo CD to manage your workload across clusters with Karmada .","title":"Working with Argo CD"},{"location":"working-with-argocd/#prerequisites","text":"","title":"Prerequisites"},{"location":"working-with-argocd/#argo-cd-installation","text":"You have installed Argo CD following the instructions in Getting Started .","title":"Argo CD Installation"},{"location":"working-with-argocd/#karmada-installation","text":"In this example, we are using a Karmada environment with at lease 3 member clusters joined. You can set up the environment by hack/local-up-karmada.sh , which is also used to run our E2E cases. # kubectl get clusters NAME VERSION MODE READY AGE member1 v1.19.1 Push True 18h member2 v1.19.1 Push True 18h member3 v1.19.1 Pull True 17h","title":"Karmada Installation"},{"location":"working-with-argocd/#registering-karmada-to-argo-cd","text":"This step registers Karmada control plane to Argo CD. First list the contexts of all clusters in your current kubeconfig: kubectl config get-contexts -o name Choose the context of the Karmada control plane from the list and add it to argocd cluster add CONTEXTNAME . For example, for karmada-apiserver context, run: argocd cluster add karmada-apiserver If everything goes well, you can see the registered Karmada control plane from the Argo CD UI, e.g.:","title":"Registering Karmada to Argo CD"},{"location":"working-with-argocd/#creating-apps-via-ui","text":"","title":"Creating Apps Via UI"},{"location":"working-with-argocd/#preparing-apps","text":"Take the guestbook as example. First, fork the argocd-example-apps repo and create a branch karmada-demo . Then, create a PropagationPolicy manifest under the guestbook directory.","title":"Preparing Apps"},{"location":"working-with-argocd/#creating-apps","text":"Click the + New App button as shown below: Give your app the name guestbook-multi-cluster , use the project default , and leave the sync policy as Manual : Connect the forked repo to Argo CD by setting repository url to the github repo url, set revision as karmada-demo , and set the path to guestbook : For Destination, set cluster to karmada and namespace to default :","title":"Creating Apps"},{"location":"working-with-argocd/#syncing-apps","text":"You can sync your applications via UI by simply clicking the SYNC button and following the pop-up instructions, e.g.: More details please refer to argocd guide: sync the application .","title":"Syncing Apps"},{"location":"working-with-argocd/#checking-apps-status","text":"For deployment running in more than one clusters, you don't need to create applications for each cluster. You can get the overall and detailed status from one Application . The svc/guestbook-ui , deploy/guestbook-ui and propagationpolicy/guestbook in the middle of the picture are the resources created by the manifest in the forked repo. And the resourcebinding/guestbook-ui-service and resourcebinding/guestbook-ui-deployment in the right of the picture are the resources created by Karmada.","title":"Checking Apps Status"},{"location":"working-with-argocd/#checking-detailed-status","text":"You can obtain the Deployment's detailed status by resourcebinding/guestbook-ui-deployment .","title":"Checking Detailed Status"},{"location":"working-with-argocd/#checking-aggregated-status","text":"You can obtain the aggregated status of the Deployment from UI by deploy/guestbook-ui .","title":"Checking Aggregated Status"},{"location":"working-with-filebeat/","text":"Use Filebeat to collect logs of Karmada member clusters Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or kafka for indexing. This document gives an example to demonstrate how to use the Filebeat to collect logs of karmada member clusters. Start up Karmada clusters You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh Start Filebeat Create resource objects of Filebeat, the content is as follows. You can specify a list of inputs in the filebeat.inputs section of the filebeat.yml . Inputs specify how Filebeat locates and processes input data. also you can configure Filebeat to write to a specific output by setting options in the Outputs section of the filebeat.yml config file. The example will collect the log information of each container and write the collected logs to a file. More detailed information about the input and output configuration, please refer to: https://github.com/elastic/beats/tree/master/filebeat/docs ``` apiVersion: v1 kind: Namespace metadata: name: logging apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: logging labels: k8s-app: filebeat apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: kube-system roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: logging labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" data: filebeat.yml: |- filebeat.inputs: - type: container paths: - /var/log/containers/ .log processors: - add_kubernetes_metadata: host: ${NODE_NAME} matchers: - logs_path: logs_path: \"/var/log/containers/\" # To enable hints based autodiscover, remove filebeat.inputs configuration and uncomment this: #filebeat.autodiscover: # providers: # - type: kubernetes # node: ${NODE_NAME} # hints.enabled: true # hints.default_config: # type: container # paths: # - /var/log/containers/ ${data.kubernetes.container.id}.log processors: - add_cloud_metadata: - add_host_metadata: #output.elasticsearch: # hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}'] # username: ${ELASTICSEARCH_USERNAME} # password: ${ELASTICSEARCH_PASSWORD} output.file: path: \"/tmp/filebeat\" filename: filebeat apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: logging labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master containers: - name: filebeat image: docker.elastic.co/beats/filebeat:8.0.0-beta1-amd64 imagePullPolicy: IfNotPresent args: [ \"-c\", \"/usr/share/filebeat/filebeat.yml\", \"-e\",] env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /usr/share/filebeat/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log - name: inputs configMap: defaultMode: 0600 name: filebeat-config # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate ``` Run the below command to execute karmada PropagationPolicy and ClusterPropagationPolicy. cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: filebeat-propagation namespace: logging spec: resourceSelectors: - apiVersion: v1 kind: Namespace name: logging - apiVersion: v1 kind: ServiceAccount name: filebeat namespace: logging - apiVersion: v1 kind: ConfigMap name: filebeat-config namespace: logging - apiVersion: apps/v1 kind: DaemonSet name: filebeat namespace: logging placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: filebeatsrbac-propagation spec: resourceSelectors: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole name: filebeat - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding name: filebeat placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF Obtain the collected logs according to the output configuration of the filebeat.yml . Reference https://github.com/elastic/beats/tree/master/filebeat https://github.com/elastic/beats/tree/master/filebeat/docs","title":"Use Filebeat to collect logs of Karmada member clusters"},{"location":"working-with-filebeat/#use-filebeat-to-collect-logs-of-karmada-member-clusters","text":"Filebeat is a lightweight shipper for forwarding and centralizing log data. Installed as an agent on your servers, Filebeat monitors the log files or locations that you specify, collects log events, and forwards them either to Elasticsearch or kafka for indexing. This document gives an example to demonstrate how to use the Filebeat to collect logs of karmada member clusters.","title":"Use Filebeat to collect logs of Karmada member clusters"},{"location":"working-with-filebeat/#start-up-karmada-clusters","text":"You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh","title":"Start up Karmada clusters"},{"location":"working-with-filebeat/#start-filebeat","text":"Create resource objects of Filebeat, the content is as follows. You can specify a list of inputs in the filebeat.inputs section of the filebeat.yml . Inputs specify how Filebeat locates and processes input data. also you can configure Filebeat to write to a specific output by setting options in the Outputs section of the filebeat.yml config file. The example will collect the log information of each container and write the collected logs to a file. More detailed information about the input and output configuration, please refer to: https://github.com/elastic/beats/tree/master/filebeat/docs ``` apiVersion: v1 kind: Namespace metadata: name: logging apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: logging labels: k8s-app: filebeat apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: filebeat rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: kube-system roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: logging labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" data: filebeat.yml: |- filebeat.inputs: - type: container paths: - /var/log/containers/ .log processors: - add_kubernetes_metadata: host: ${NODE_NAME} matchers: - logs_path: logs_path: \"/var/log/containers/\" # To enable hints based autodiscover, remove filebeat.inputs configuration and uncomment this: #filebeat.autodiscover: # providers: # - type: kubernetes # node: ${NODE_NAME} # hints.enabled: true # hints.default_config: # type: container # paths: # - /var/log/containers/ ${data.kubernetes.container.id}.log processors: - add_cloud_metadata: - add_host_metadata: #output.elasticsearch: # hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}'] # username: ${ELASTICSEARCH_USERNAME} # password: ${ELASTICSEARCH_PASSWORD} output.file: path: \"/tmp/filebeat\" filename: filebeat apiVersion: apps/v1 kind: DaemonSet metadata: name: filebeat namespace: logging labels: k8s-app: filebeat spec: selector: matchLabels: k8s-app: filebeat template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master containers: - name: filebeat image: docker.elastic.co/beats/filebeat:8.0.0-beta1-amd64 imagePullPolicy: IfNotPresent args: [ \"-c\", \"/usr/share/filebeat/filebeat.yml\", \"-e\",] env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /usr/share/filebeat/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true - name: varlog mountPath: /var/log readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: varlog hostPath: path: /var/log - name: inputs configMap: defaultMode: 0600 name: filebeat-config # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate ``` Run the below command to execute karmada PropagationPolicy and ClusterPropagationPolicy. cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: filebeat-propagation namespace: logging spec: resourceSelectors: - apiVersion: v1 kind: Namespace name: logging - apiVersion: v1 kind: ServiceAccount name: filebeat namespace: logging - apiVersion: v1 kind: ConfigMap name: filebeat-config namespace: logging - apiVersion: apps/v1 kind: DaemonSet name: filebeat namespace: logging placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: filebeatsrbac-propagation spec: resourceSelectors: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole name: filebeat - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding name: filebeat placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF Obtain the collected logs according to the output configuration of the filebeat.yml .","title":"Start Filebeat"},{"location":"working-with-filebeat/#reference","text":"https://github.com/elastic/beats/tree/master/filebeat https://github.com/elastic/beats/tree/master/filebeat/docs","title":"Reference"},{"location":"working-with-flux/","text":"Use Flux to support Helm chart propagation Flux is most useful when used as a deployment tool at the end of a Continuous Delivery pipeline. Flux will make sure that your new container images and config changes are propagated to the cluster. With Flux, Karmada can easily realize the ability to distribute applications packaged by helm across clusters. Not only that, with Karmada's OverridePolicy, users can customize applications for specific clusters and manage cross-cluster applications on the unified Karmada control plane. Start up Karmada clusters You just need to clone the Karmada repo, and run the following script in the karmada directory. hack/local-up-karmada.sh Start up Flux Install the flux binary: curl -s https://fluxcd.io/install.sh | sudo bash Install the toolkit controllers in the flux-system namespace: flux install Tips: The Flux toolkit controllers need to be installed on each cluster using the flux install command. If the Flux toolkit controllers are successfully installed, you should see the following Pods: $ kubectl get pod -n flux-system NAME READY STATUS RESTARTS AGE helm-controller-55896d6ccf-dlf8b 1/1 Running 0 15d kustomize-controller-76795877c9-mbrsk 1/1 Running 0 15d notification-controller-7ccfbfbb98-lpgjl 1/1 Running 0 15d source-controller-6b8d9cb5cc-7dbcb 1/1 Running 0 15d Helm chart propagation If you want to propagate helm applications to member clusters, you can refer to the guide below. Define a HelmRepository source apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository metadata: name: podinfo spec: interval: 1m url: https://stefanprodan.github.io/podinfo --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: helm-repo spec: resourceSelectors: - apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Define a HelmRelease source apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease metadata: name: podinfo spec: interval: 5m chart: spec: chart: podinfo version: 5.0.3 sourceRef: kind: HelmRepository name: podinfo --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: helm-release spec: resourceSelectors: - apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Apply those YAMLs to the karmada-apiserver $ kubectl apply -f ../helm/ helmrelease.helm.toolkit.fluxcd.io/podinfo created helmrepository.source.toolkit.fluxcd.io/podinfo created propagationpolicy.policy.karmada.io/helm-release created propagationpolicy.policy.karmada.io/helm-repo created Switch to the distributed cluster helm --kubeconfig=/root/.kube/members.config --kube-context member1 list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION podinfo default 1 2022-05-27 01:44:35.24229175 +0000 UTC deployed podinfo-5.0.3 5.0.3 Also, you can use Karmada's OverridePolicy to customize applications for specific clusters. For example, if you just want to change replicas in member1, you can refer to the overridePolicy below. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example-override namespace: default spec: resourceSelectors: - apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease name: podinfo overrideRules: - targetCluster: clusterNames: - member1 overriders: plaintext: - path: \"/spec/values\" operator: add value: replicaCount: 2 After that, you can find that replicas in member1 has changed. $ kubectl --kubeconfig ~/.kube/members.config --context member1 get po NAME READY STATUS RESTARTS AGE podinfo-68979685bc-6wz6s 1/1 Running 0 6m28s podinfo-68979685bc-dz9f6 1/1 Running 0 7m42s Kustomize propagation Kustomize propagation is basically the same as helm chart propagation above. You can refer to the guide below. Define a Git repository source apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: podinfo spec: interval: 1m url: https://github.com/stefanprodan/podinfo ref: branch: master --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: kust-git spec: resourceSelectors: - apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Define a kustomization apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization metadata: name: podinfo-dev spec: interval: 5m path: \"./deploy/overlays/dev/\" prune: true sourceRef: kind: GitRepository name: podinfo validation: client timeout: 80s --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: kust-release spec: resourceSelectors: - apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization name: podinfo-dev placement: clusterAffinity: clusterNames: - member1 - member2 Apply those YAMLs to the karmada-apiserver $ kubectl apply -f kust/ gitrepository.source.toolkit.fluxcd.io/podinfo created kustomization.kustomize.toolkit.fluxcd.io/podinfo-dev created propagationpolicy.policy.karmada.io/kust-git created propagationpolicy.policy.karmada.io/kust-release created Switch to the distributed cluster $ kubectl get pod -n dev NAME READY STATUS RESTARTS AGE backend-69c7655cb-rbtrq 1/1 Running 0 15s cache-bdff5c8dc-mmnbm 1/1 Running 0 15s frontend-7f98bf6f85-dw4vq 1/1 Running 0 15s Reference https://fluxcd.io https://github.com/fluxcd","title":"Use Flux to support Helm chart propagation"},{"location":"working-with-flux/#use-flux-to-support-helm-chart-propagation","text":"Flux is most useful when used as a deployment tool at the end of a Continuous Delivery pipeline. Flux will make sure that your new container images and config changes are propagated to the cluster. With Flux, Karmada can easily realize the ability to distribute applications packaged by helm across clusters. Not only that, with Karmada's OverridePolicy, users can customize applications for specific clusters and manage cross-cluster applications on the unified Karmada control plane.","title":"Use Flux to support Helm chart propagation"},{"location":"working-with-flux/#start-up-karmada-clusters","text":"You just need to clone the Karmada repo, and run the following script in the karmada directory. hack/local-up-karmada.sh","title":"Start up Karmada clusters"},{"location":"working-with-flux/#start-up-flux","text":"Install the flux binary: curl -s https://fluxcd.io/install.sh | sudo bash Install the toolkit controllers in the flux-system namespace: flux install Tips: The Flux toolkit controllers need to be installed on each cluster using the flux install command. If the Flux toolkit controllers are successfully installed, you should see the following Pods: $ kubectl get pod -n flux-system NAME READY STATUS RESTARTS AGE helm-controller-55896d6ccf-dlf8b 1/1 Running 0 15d kustomize-controller-76795877c9-mbrsk 1/1 Running 0 15d notification-controller-7ccfbfbb98-lpgjl 1/1 Running 0 15d source-controller-6b8d9cb5cc-7dbcb 1/1 Running 0 15d","title":"Start up Flux"},{"location":"working-with-flux/#helm-chart-propagation","text":"If you want to propagate helm applications to member clusters, you can refer to the guide below. Define a HelmRepository source apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository metadata: name: podinfo spec: interval: 1m url: https://stefanprodan.github.io/podinfo --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: helm-repo spec: resourceSelectors: - apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Define a HelmRelease source apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease metadata: name: podinfo spec: interval: 5m chart: spec: chart: podinfo version: 5.0.3 sourceRef: kind: HelmRepository name: podinfo --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: helm-release spec: resourceSelectors: - apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Apply those YAMLs to the karmada-apiserver $ kubectl apply -f ../helm/ helmrelease.helm.toolkit.fluxcd.io/podinfo created helmrepository.source.toolkit.fluxcd.io/podinfo created propagationpolicy.policy.karmada.io/helm-release created propagationpolicy.policy.karmada.io/helm-repo created Switch to the distributed cluster helm --kubeconfig=/root/.kube/members.config --kube-context member1 list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION podinfo default 1 2022-05-27 01:44:35.24229175 +0000 UTC deployed podinfo-5.0.3 5.0.3 Also, you can use Karmada's OverridePolicy to customize applications for specific clusters. For example, if you just want to change replicas in member1, you can refer to the overridePolicy below. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example-override namespace: default spec: resourceSelectors: - apiVersion: helm.toolkit.fluxcd.io/v2beta1 kind: HelmRelease name: podinfo overrideRules: - targetCluster: clusterNames: - member1 overriders: plaintext: - path: \"/spec/values\" operator: add value: replicaCount: 2 After that, you can find that replicas in member1 has changed. $ kubectl --kubeconfig ~/.kube/members.config --context member1 get po NAME READY STATUS RESTARTS AGE podinfo-68979685bc-6wz6s 1/1 Running 0 6m28s podinfo-68979685bc-dz9f6 1/1 Running 0 7m42s","title":"Helm chart propagation"},{"location":"working-with-flux/#kustomize-propagation","text":"Kustomize propagation is basically the same as helm chart propagation above. You can refer to the guide below. Define a Git repository source apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository metadata: name: podinfo spec: interval: 1m url: https://github.com/stefanprodan/podinfo ref: branch: master --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: kust-git spec: resourceSelectors: - apiVersion: source.toolkit.fluxcd.io/v1beta1 kind: GitRepository name: podinfo placement: clusterAffinity: clusterNames: - member1 - member2 Define a kustomization apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization metadata: name: podinfo-dev spec: interval: 5m path: \"./deploy/overlays/dev/\" prune: true sourceRef: kind: GitRepository name: podinfo validation: client timeout: 80s --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: kust-release spec: resourceSelectors: - apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization name: podinfo-dev placement: clusterAffinity: clusterNames: - member1 - member2 Apply those YAMLs to the karmada-apiserver $ kubectl apply -f kust/ gitrepository.source.toolkit.fluxcd.io/podinfo created kustomization.kustomize.toolkit.fluxcd.io/podinfo-dev created propagationpolicy.policy.karmada.io/kust-git created propagationpolicy.policy.karmada.io/kust-release created Switch to the distributed cluster $ kubectl get pod -n dev NAME READY STATUS RESTARTS AGE backend-69c7655cb-rbtrq 1/1 Running 0 15s cache-bdff5c8dc-mmnbm 1/1 Running 0 15s frontend-7f98bf6f85-dw4vq 1/1 Running 0 15s","title":"Kustomize propagation"},{"location":"working-with-flux/#reference","text":"https://fluxcd.io https://github.com/fluxcd","title":"Reference"},{"location":"working-with-gatekeeper/","text":"Working with Gatekeeper(OPA) Gatekeeper , is a customizable admission webhook for Kubernetes that enforces policies executed by the Open Policy Agent (OPA), a policy engine for Cloud Native environments hosted by Cloud Native Computing Foundation . This document gives an example to demonstrate how to use the Gatekeeper to manage OPA policy. Prerequisites Start up Karmada clusters You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh Gatekeeper Installations In this case, you will use Gatekeeper v3.7.2. Related deployment files are from here . Install Gatekeeper APIs on Karmada Create resource objects of Gatekeeper in karmada controller plane, the content is as follows. console kubectl config use-context karmada-apiserver Deploy namespace: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1-L9 Deploy Gatekeeper CRDs: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L27-L1999 Deploy Gatekeeper secrets: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L2261-L2267 Deploy webhook config: ```yaml apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-mutating-webhook-configuration webhooks: - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/mutate failurePolicy: Ignore matchPolicy: Exact name: mutation.gatekeeper.sh namespaceSelector: matchExpressions: - key: admission.gatekeeper.sh/ignore operator: DoesNotExist rules: - apiGroups: - ' ' apiVersions: - ' ' operations: - CREATE - UPDATE resources: - '*' sideEffects: None timeoutSeconds: 1 apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-validating-webhook-configuration webhooks: - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admit failurePolicy: Ignore matchPolicy: Exact name: validation.gatekeeper.sh namespaceSelector: matchExpressions: - key: admission.gatekeeper.sh/ignore operator: DoesNotExist rules: - apiGroups: - ' ' apiVersions: - ' ' operations: - CREATE - UPDATE resources: - ' ' sideEffects: None timeoutSeconds: 3 - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admitlabel failurePolicy: Fail matchPolicy: Exact name: check-ignore-label.gatekeeper.sh rules: - apiGroups: - \"\" apiVersions: - ' ' operations: - CREATE - UPDATE resources: - namespaces sideEffects: None timeoutSeconds: 3 ``` You need to change the clientconfig from service type to url type for multi-cluster deployment. Also, you need to deploy a dummy pod in gatekeeper-system namespace in karmada-apiserver context because when Gatekeeper generates a policy template CRD, a status object is generated to monitor the status of the policy template, and the status object is bound by the controller Pod through the OwnerReference. Therefore, when the CRD and the controller are not in the same cluster, a dummy Pod needs to be used instead of the controller. The Pod enables the status object to be successfully generated. For example: yaml apiVersion: v1 kind: Pod metadata: name: dummpy-pod namespace: gatekeeper-system spec: containers: - name: dummpy-pod image: nginx:latest imagePullPolicy: Always Install GateKeeper components on host cluster console kubectl config use-context karmada-host Deploy namespace: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1-L9 Deploy RBAC resources for deployment: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1999-L2375 Deploy Gatekeeper controllers and secret as kubeconfig: ```yaml apiVersion: apps/v1 kind: Deployment metadata: labels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" name: gatekeeper-audit namespace: gatekeeper-system spec: replicas: 1 selector: matchLabels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" template: metadata: annotations: container.seccomp.security.alpha.kubernetes.io/manager: runtime/default labels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" spec: automountServiceAccountToken: true containers: - args: - --operation=audit - --operation=status - --operation=mutation-status - --logtostderr - --disable-opa-builtin={http.send} - --kubeconfig=/etc/kubeconfig command: - /manager env: - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME value: {{POD_NAME}} image: openpolicyagent/gatekeeper:v3.7.2 imagePullPolicy: Always livenessProbe: httpGet: path: /healthz port: 9090 name: manager ports: - containerPort: 8888 name: metrics protocol: TCP - containerPort: 9090 name: healthz protocol: TCP readinessProbe: httpGet: path: /readyz port: 9090 resources: limits: cpu: 1000m memory: 512Mi requests: cpu: 100m memory: 256Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - all readOnlyRootFilesystem: true runAsGroup: 999 runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp/audit name: tmp-volume - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: gatekeeper-admin terminationGracePeriodSeconds: 60 volumes: - emptyDir: {} name: tmp-volume - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: apps/v1 kind: Deployment metadata: labels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" name: gatekeeper-controller-manager namespace: gatekeeper-system spec: replicas: 3 selector: matchLabels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" template: metadata: annotations: container.seccomp.security.alpha.kubernetes.io/manager: runtime/default labels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: gatekeeper.sh/operation operator: In values: - webhook topologyKey: kubernetes.io/hostname weight: 100 automountServiceAccountToken: true containers: - args: - --port=8443 - --logtostderr - --exempt-namespace=gatekeeper-system - --operation=webhook - --operation=mutation-webhook - --disable-opa-builtin={http.send} - --kubeconfig=/etc/kubeconfig command: - /manager env: - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME value: {{POD_NAME}} image: openpolicyagent/gatekeeper:v3.7.2 imagePullPolicy: Always livenessProbe: httpGet: path: /healthz port: 9090 name: manager ports: - containerPort: 8443 name: webhook-server protocol: TCP - containerPort: 8888 name: metrics protocol: TCP - containerPort: 9090 name: healthz protocol: TCP readinessProbe: httpGet: path: /readyz port: 9090 resources: limits: cpu: 1000m memory: 512Mi requests: cpu: 100m memory: 256Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - all readOnlyRootFilesystem: true runAsGroup: 999 runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /certs name: cert readOnly: true - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: gatekeeper-admin terminationGracePeriodSeconds: 60 volumes: - name: cert secret: defaultMode: 420 secretName: gatekeeper-webhook-server-cert - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-controller-manager namespace: gatekeeper-system spec: minAvailable: 1 selector: matchLabels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" apiVersion: v1 stringData: kubeconfig: |- apiVersion: v1 clusters: - cluster: certificate-authority-data: {{ca_crt}} server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443 name: kind-karmada contexts: - context: cluster: kind-karmada user: kind-karmada name: karmada current-context: karmada kind: Config preferences: {} users: - name: kind-karmada user: client-certificate-data: {{client_cer}} client-key-data: {{client_key}} kind: Secret metadata: name: kubeconfig namespace: gatekeeper-system ``` You need to fill in the dummy pod created in step 1 to {{ POD_NAME }} and fill in the secret which represents kubeconfig pointing to karmada-apiserver. Deploy resourcequota: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L10-L26 Extra steps After all, we need to copy the secret gatekeeper-webhook-server-cert in karmada-apiserver context to that in karmada-host context to keep secrets stored in etcd and volumes mounted in controller the same. Run demo Create k8srequiredlabels template ```yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels validation: openAPIV3Schema: type: object description: Describe K8sRequiredLabels crd parameters properties: labels: type: array items: type: string description: A label string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) } ``` Create k8srequiredlabels constraint yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: ns-must-have-gk spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"gatekeepers\"] Create a bad namespace console kubectl create ns test Error from server ([ns-must-have-gk] you must provide labels: {\"gatekeepers\"}): admission webhook \"validation.gatekeeper.sh\" denied the request: [ns-must-have-gk] you must provide labels: {\"gatekeepers\"} Reference https://github.com/open-policy-agent/gatekeeper","title":"Working with Gatekeeper(OPA)"},{"location":"working-with-gatekeeper/#working-with-gatekeeperopa","text":"Gatekeeper , is a customizable admission webhook for Kubernetes that enforces policies executed by the Open Policy Agent (OPA), a policy engine for Cloud Native environments hosted by Cloud Native Computing Foundation . This document gives an example to demonstrate how to use the Gatekeeper to manage OPA policy.","title":"Working with Gatekeeper(OPA)"},{"location":"working-with-gatekeeper/#prerequisites","text":"","title":"Prerequisites"},{"location":"working-with-gatekeeper/#start-up-karmada-clusters","text":"You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh","title":"Start up Karmada clusters"},{"location":"working-with-gatekeeper/#gatekeeper-installations","text":"In this case, you will use Gatekeeper v3.7.2. Related deployment files are from here .","title":"Gatekeeper Installations"},{"location":"working-with-gatekeeper/#install-gatekeeper-apis-on-karmada","text":"Create resource objects of Gatekeeper in karmada controller plane, the content is as follows. console kubectl config use-context karmada-apiserver Deploy namespace: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1-L9 Deploy Gatekeeper CRDs: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L27-L1999 Deploy Gatekeeper secrets: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L2261-L2267 Deploy webhook config: ```yaml apiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-mutating-webhook-configuration webhooks: - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/mutate failurePolicy: Ignore matchPolicy: Exact name: mutation.gatekeeper.sh namespaceSelector: matchExpressions: - key: admission.gatekeeper.sh/ignore operator: DoesNotExist rules: - apiGroups: - ' ' apiVersions: - ' ' operations: - CREATE - UPDATE resources: - '*' sideEffects: None timeoutSeconds: 1 apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-validating-webhook-configuration webhooks: - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admit failurePolicy: Ignore matchPolicy: Exact name: validation.gatekeeper.sh namespaceSelector: matchExpressions: - key: admission.gatekeeper.sh/ignore operator: DoesNotExist rules: - apiGroups: - ' ' apiVersions: - ' ' operations: - CREATE - UPDATE resources: - ' ' sideEffects: None timeoutSeconds: 3 - admissionReviewVersions: - v1 - v1beta1 clientConfig: #Change the clientconfig from service type to url type cause webhook config and service are not in the same cluster. url: https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admitlabel failurePolicy: Fail matchPolicy: Exact name: check-ignore-label.gatekeeper.sh rules: - apiGroups: - \"\" apiVersions: - ' ' operations: - CREATE - UPDATE resources: - namespaces sideEffects: None timeoutSeconds: 3 ``` You need to change the clientconfig from service type to url type for multi-cluster deployment. Also, you need to deploy a dummy pod in gatekeeper-system namespace in karmada-apiserver context because when Gatekeeper generates a policy template CRD, a status object is generated to monitor the status of the policy template, and the status object is bound by the controller Pod through the OwnerReference. Therefore, when the CRD and the controller are not in the same cluster, a dummy Pod needs to be used instead of the controller. The Pod enables the status object to be successfully generated. For example: yaml apiVersion: v1 kind: Pod metadata: name: dummpy-pod namespace: gatekeeper-system spec: containers: - name: dummpy-pod image: nginx:latest imagePullPolicy: Always","title":"Install Gatekeeper APIs on Karmada"},{"location":"working-with-gatekeeper/#install-gatekeeper-components-on-host-cluster","text":"console kubectl config use-context karmada-host Deploy namespace: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1-L9 Deploy RBAC resources for deployment: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L1999-L2375 Deploy Gatekeeper controllers and secret as kubeconfig: ```yaml apiVersion: apps/v1 kind: Deployment metadata: labels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" name: gatekeeper-audit namespace: gatekeeper-system spec: replicas: 1 selector: matchLabels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" template: metadata: annotations: container.seccomp.security.alpha.kubernetes.io/manager: runtime/default labels: control-plane: audit-controller gatekeeper.sh/operation: audit gatekeeper.sh/system: \"yes\" spec: automountServiceAccountToken: true containers: - args: - --operation=audit - --operation=status - --operation=mutation-status - --logtostderr - --disable-opa-builtin={http.send} - --kubeconfig=/etc/kubeconfig command: - /manager env: - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME value: {{POD_NAME}} image: openpolicyagent/gatekeeper:v3.7.2 imagePullPolicy: Always livenessProbe: httpGet: path: /healthz port: 9090 name: manager ports: - containerPort: 8888 name: metrics protocol: TCP - containerPort: 9090 name: healthz protocol: TCP readinessProbe: httpGet: path: /readyz port: 9090 resources: limits: cpu: 1000m memory: 512Mi requests: cpu: 100m memory: 256Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - all readOnlyRootFilesystem: true runAsGroup: 999 runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /tmp/audit name: tmp-volume - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: gatekeeper-admin terminationGracePeriodSeconds: 60 volumes: - emptyDir: {} name: tmp-volume - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: apps/v1 kind: Deployment metadata: labels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" name: gatekeeper-controller-manager namespace: gatekeeper-system spec: replicas: 3 selector: matchLabels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" template: metadata: annotations: container.seccomp.security.alpha.kubernetes.io/manager: runtime/default labels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: gatekeeper.sh/operation operator: In values: - webhook topologyKey: kubernetes.io/hostname weight: 100 automountServiceAccountToken: true containers: - args: - --port=8443 - --logtostderr - --exempt-namespace=gatekeeper-system - --operation=webhook - --operation=mutation-webhook - --disable-opa-builtin={http.send} - --kubeconfig=/etc/kubeconfig command: - /manager env: - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace - name: POD_NAME value: {{POD_NAME}} image: openpolicyagent/gatekeeper:v3.7.2 imagePullPolicy: Always livenessProbe: httpGet: path: /healthz port: 9090 name: manager ports: - containerPort: 8443 name: webhook-server protocol: TCP - containerPort: 8888 name: metrics protocol: TCP - containerPort: 9090 name: healthz protocol: TCP readinessProbe: httpGet: path: /readyz port: 9090 resources: limits: cpu: 1000m memory: 512Mi requests: cpu: 100m memory: 256Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - all readOnlyRootFilesystem: true runAsGroup: 999 runAsNonRoot: true runAsUser: 1000 volumeMounts: - mountPath: /certs name: cert readOnly: true - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig nodeSelector: kubernetes.io/os: linux priorityClassName: system-cluster-critical serviceAccountName: gatekeeper-admin terminationGracePeriodSeconds: 60 volumes: - name: cert secret: defaultMode: 420 secretName: gatekeeper-webhook-server-cert - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: labels: gatekeeper.sh/system: \"yes\" name: gatekeeper-controller-manager namespace: gatekeeper-system spec: minAvailable: 1 selector: matchLabels: control-plane: controller-manager gatekeeper.sh/operation: webhook gatekeeper.sh/system: \"yes\" apiVersion: v1 stringData: kubeconfig: |- apiVersion: v1 clusters: - cluster: certificate-authority-data: {{ca_crt}} server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443 name: kind-karmada contexts: - context: cluster: kind-karmada user: kind-karmada name: karmada current-context: karmada kind: Config preferences: {} users: - name: kind-karmada user: client-certificate-data: {{client_cer}} client-key-data: {{client_key}} kind: Secret metadata: name: kubeconfig namespace: gatekeeper-system ``` You need to fill in the dummy pod created in step 1 to {{ POD_NAME }} and fill in the secret which represents kubeconfig pointing to karmada-apiserver. Deploy resourcequota: https://github.com/open-policy-agent/gatekeeper/blob/0d239574f8e71908325391d49cb8dd8e4ed6f6fa/deploy/gatekeeper.yaml#L10-L26","title":"Install GateKeeper components on host cluster"},{"location":"working-with-gatekeeper/#extra-steps","text":"After all, we need to copy the secret gatekeeper-webhook-server-cert in karmada-apiserver context to that in karmada-host context to keep secrets stored in etcd and volumes mounted in controller the same.","title":"Extra steps"},{"location":"working-with-gatekeeper/#run-demo","text":"","title":"Run demo"},{"location":"working-with-gatekeeper/#create-k8srequiredlabels-template","text":"```yaml apiVersion: templates.gatekeeper.sh/v1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels validation: openAPIV3Schema: type: object description: Describe K8sRequiredLabels crd parameters properties: labels: type: array items: type: string description: A label string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] { provided := {label | input.review.object.metadata.labels[label]} required := {label | label := input.parameters.labels[_]} missing := required - provided count(missing) > 0 msg := sprintf(\"you must provide labels: %v\", [missing]) } ```","title":"Create k8srequiredlabels template"},{"location":"working-with-gatekeeper/#create-k8srequiredlabels-constraint","text":"yaml apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: ns-must-have-gk spec: match: kinds: - apiGroups: [\"\"] kinds: [\"Namespace\"] parameters: labels: [\"gatekeepers\"]","title":"Create k8srequiredlabels constraint"},{"location":"working-with-gatekeeper/#create-a-bad-namespace","text":"console kubectl create ns test Error from server ([ns-must-have-gk] you must provide labels: {\"gatekeepers\"}): admission webhook \"validation.gatekeeper.sh\" denied the request: [ns-must-have-gk] you must provide labels: {\"gatekeepers\"}","title":"Create a bad namespace"},{"location":"working-with-gatekeeper/#reference","text":"https://github.com/open-policy-agent/gatekeeper","title":"Reference"},{"location":"working-with-kyverno/","text":"Working with Kyverno Kyverno , a Cloud Native Computing Foundation project, is a policy engine designed for Kubernetes. It can validate, mutate, and generate configurations using admission controls and background scans. Kyverno policies are Kubernetes resources and do not require learning a new language. Kyverno is designed to work nicely with tools you already use like kubectl, kustomize, and Git. This document gives an example to demonstrate how to use the Kyverno to manage policy. Prerequisites Setup Karmada You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh Kyverno Installations In this case, we will use Kyverno v1.6.2. Related deployment files are from here . Install Kyverno APIs on Karmada Create resource objects of Kyverno in karmada controller plane, the content is as follows. console kubectl config use-context karmada-apiserver Deploy namespace: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L1-L12 Deploy configmap: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L12144-L12176 Deploy Kyverno CRDs: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L12-L11656 Install Kyverno components on host cluster Create resource objects of Kyverno in karmada-host context, the content is as follows. console kubectl config use-context karmada-host Deploy namespace: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L1-L12 Deploy RBAC resources: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L11657-L12143 Deploy Kyverno controllers and service: ```yaml apiVersion: v1 kind: Service metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno-svc namespace: kyverno spec: type: NodePort ports: - name: https port: 443 targetPort: https nodePort: {{nodePort}} selector: app: kyverno app.kubernetes.io/name: kyverno apiVersion: v1 kind: Service metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno-svc-metrics namespace: kyverno spec: ports: - name: metrics-port port: 8000 targetPort: metrics-port selector: app: kyverno app.kubernetes.io/name: kyverno apiVersion: apps/v1 kind: Deployment metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno namespace: kyverno spec: replicas: 1 selector: matchLabels: app: kyverno app.kubernetes.io/name: kyverno strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 40% type: RollingUpdate template: metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - kyverno topologyKey: kubernetes.io/hostname weight: 1 containers: - args: - --filterK8sResources=[Event, , ][ ,kube-system, ][ ,kube-public, ][ ,kube-node-lease, ][Node, , ][APIService, , ][TokenReview, , ][SubjectAccessReview, , ][ ,kyverno,kyverno ][Binding, , ][ReplicaSet, , ][ReportChangeRequest, , ][ClusterReportChangeRequest, , ][PolicyReport, , ][ClusterPolicyReport, , ] - -v=2 - --autogenInternals=false - --kubeconfig=/etc/kubeconfig - --serverIP={{nodeIP}}:{{nodePort}} env: - name: INIT_CONFIG value: kyverno - name: METRICS_CONFIG value: kyverno-metrics - name: KYVERNO_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: KYVERNO_SVC value: kyverno-svc - name: TUF_ROOT value: /.sigstore image: ghcr.io/kyverno/kyverno:latest imagePullPolicy: Always livenessProbe: failureThreshold: 2 httpGet: path: /health/liveness port: 9443 scheme: HTTPS initialDelaySeconds: 15 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 5 name: kyverno ports: - containerPort: 9443 name: https protocol: TCP - containerPort: 8000 name: metrics-port protocol: TCP readinessProbe: failureThreshold: 4 httpGet: path: /health/readiness port: 9443 scheme: HTTPS initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: limits: memory: 384Mi requests: cpu: 100m memory: 128Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL privileged: false readOnlyRootFilesystem: true runAsNonRoot: true volumeMounts: - mountPath: /.sigstore name: sigstore - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig initContainers: - args: - env: - name: METRICS_CONFIG value: kyverno-metrics - name: KYVERNO_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: ghcr.io/kyverno/kyvernopre:latest imagePullPolicy: Always name: kyverno-pre resources: limits: cpu: 100m memory: 256Mi requests: cpu: 10m memory: 64Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL privileged: false readOnlyRootFilesystem: true runAsNonRoot: true securityContext: runAsNonRoot: true serviceAccountName: kyverno-service-account volumes: - emptyDir: {} name: sigstore - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: v1 stringData: kubeconfig: |- apiVersion: v1 clusters: - cluster: certificate-authority-data: {{ca_crt}} server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443 name: kind-karmada contexts: - context: cluster: kind-karmada user: kind-karmada name: karmada current-context: karmada kind: Config preferences: {} users: - name: kind-karmada user: client-certificate-data: {{client_cer}} client-key-data: {{client_key}} kind: Secret metadata: name: kubeconfig namespace: kyverno ``` For multi-cluster deployment, We need to add the config of --serverIP which is the address of the webhook server. So you need to ensure that the network from node in karmada control plane to those in karmada-host cluster is connected and expose kyverno controller pods to control plane, for example, using nodePort above. Then, fill in the secret which represents kubeconfig pointing to karmada-apiserver, such as ca_crt, client_cer and client_key above. Run demo Create require-labels ClusterPolicy ClusterPolicy is a CRD which kyverno offers to support different kinds of rules. Here is an example ClusterPolicy which means that you must create pod with app.kubernetes.io/name label. console kubectl config use-context karmada-apiserver console kubectl create -f- << EOF apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-labels spec: validationFailureAction: enforce rules: - name: check-for-labels match: any: - resources: kinds: - Pod validate: message: \"label 'app.kubernetes.io/name' is required\" pattern: metadata: labels: app.kubernetes.io/name: \"?*\" EOF Create a bad deployment without labels console kubectl create deployment nginx --image=nginx error: failed to create deployment: admission webhook \"validate.kyverno.svc-fail\" denied the request Reference https://github.com/kyverno/kyverno","title":"Working with Kyverno"},{"location":"working-with-kyverno/#working-with-kyverno","text":"Kyverno , a Cloud Native Computing Foundation project, is a policy engine designed for Kubernetes. It can validate, mutate, and generate configurations using admission controls and background scans. Kyverno policies are Kubernetes resources and do not require learning a new language. Kyverno is designed to work nicely with tools you already use like kubectl, kustomize, and Git. This document gives an example to demonstrate how to use the Kyverno to manage policy.","title":"Working with Kyverno"},{"location":"working-with-kyverno/#prerequisites","text":"","title":"Prerequisites"},{"location":"working-with-kyverno/#setup-karmada","text":"You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh","title":"Setup Karmada"},{"location":"working-with-kyverno/#kyverno-installations","text":"In this case, we will use Kyverno v1.6.2. Related deployment files are from here .","title":"Kyverno Installations"},{"location":"working-with-kyverno/#install-kyverno-apis-on-karmada","text":"Create resource objects of Kyverno in karmada controller plane, the content is as follows. console kubectl config use-context karmada-apiserver Deploy namespace: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L1-L12 Deploy configmap: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L12144-L12176 Deploy Kyverno CRDs: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L12-L11656","title":"Install Kyverno APIs on Karmada"},{"location":"working-with-kyverno/#install-kyverno-components-on-host-cluster","text":"Create resource objects of Kyverno in karmada-host context, the content is as follows. console kubectl config use-context karmada-host Deploy namespace: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L1-L12 Deploy RBAC resources: https://github.com/kyverno/kyverno/blob/61a1d40e5ea5ff4875a084b6dc3ef1fdcca1ee27/config/install.yaml#L11657-L12143 Deploy Kyverno controllers and service: ```yaml apiVersion: v1 kind: Service metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno-svc namespace: kyverno spec: type: NodePort ports: - name: https port: 443 targetPort: https nodePort: {{nodePort}} selector: app: kyverno app.kubernetes.io/name: kyverno apiVersion: v1 kind: Service metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno-svc-metrics namespace: kyverno spec: ports: - name: metrics-port port: 8000 targetPort: metrics-port selector: app: kyverno app.kubernetes.io/name: kyverno apiVersion: apps/v1 kind: Deployment metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest name: kyverno namespace: kyverno spec: replicas: 1 selector: matchLabels: app: kyverno app.kubernetes.io/name: kyverno strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 40% type: RollingUpdate template: metadata: labels: app: kyverno app.kubernetes.io/component: kyverno app.kubernetes.io/instance: kyverno app.kubernetes.io/name: kyverno app.kubernetes.io/part-of: kyverno app.kubernetes.io/version: latest spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - kyverno topologyKey: kubernetes.io/hostname weight: 1 containers: - args: - --filterK8sResources=[Event, , ][ ,kube-system, ][ ,kube-public, ][ ,kube-node-lease, ][Node, , ][APIService, , ][TokenReview, , ][SubjectAccessReview, , ][ ,kyverno,kyverno ][Binding, , ][ReplicaSet, , ][ReportChangeRequest, , ][ClusterReportChangeRequest, , ][PolicyReport, , ][ClusterPolicyReport, , ] - -v=2 - --autogenInternals=false - --kubeconfig=/etc/kubeconfig - --serverIP={{nodeIP}}:{{nodePort}} env: - name: INIT_CONFIG value: kyverno - name: METRICS_CONFIG value: kyverno-metrics - name: KYVERNO_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: KYVERNO_SVC value: kyverno-svc - name: TUF_ROOT value: /.sigstore image: ghcr.io/kyverno/kyverno:latest imagePullPolicy: Always livenessProbe: failureThreshold: 2 httpGet: path: /health/liveness port: 9443 scheme: HTTPS initialDelaySeconds: 15 periodSeconds: 30 successThreshold: 1 timeoutSeconds: 5 name: kyverno ports: - containerPort: 9443 name: https protocol: TCP - containerPort: 8000 name: metrics-port protocol: TCP readinessProbe: failureThreshold: 4 httpGet: path: /health/readiness port: 9443 scheme: HTTPS initialDelaySeconds: 5 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: limits: memory: 384Mi requests: cpu: 100m memory: 128Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL privileged: false readOnlyRootFilesystem: true runAsNonRoot: true volumeMounts: - mountPath: /.sigstore name: sigstore - mountPath: /etc/kubeconfig name: kubeconfig subPath: kubeconfig initContainers: - args: - env: - name: METRICS_CONFIG value: kyverno-metrics - name: KYVERNO_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace image: ghcr.io/kyverno/kyvernopre:latest imagePullPolicy: Always name: kyverno-pre resources: limits: cpu: 100m memory: 256Mi requests: cpu: 10m memory: 64Mi securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL privileged: false readOnlyRootFilesystem: true runAsNonRoot: true securityContext: runAsNonRoot: true serviceAccountName: kyverno-service-account volumes: - emptyDir: {} name: sigstore - name: kubeconfig secret: defaultMode: 420 secretName: kubeconfig apiVersion: v1 stringData: kubeconfig: |- apiVersion: v1 clusters: - cluster: certificate-authority-data: {{ca_crt}} server: https://karmada-apiserver.karmada-system.svc.cluster.local:5443 name: kind-karmada contexts: - context: cluster: kind-karmada user: kind-karmada name: karmada current-context: karmada kind: Config preferences: {} users: - name: kind-karmada user: client-certificate-data: {{client_cer}} client-key-data: {{client_key}} kind: Secret metadata: name: kubeconfig namespace: kyverno ``` For multi-cluster deployment, We need to add the config of --serverIP which is the address of the webhook server. So you need to ensure that the network from node in karmada control plane to those in karmada-host cluster is connected and expose kyverno controller pods to control plane, for example, using nodePort above. Then, fill in the secret which represents kubeconfig pointing to karmada-apiserver, such as ca_crt, client_cer and client_key above.","title":"Install Kyverno components on host cluster"},{"location":"working-with-kyverno/#run-demo","text":"","title":"Run demo"},{"location":"working-with-kyverno/#create-require-labels-clusterpolicy","text":"ClusterPolicy is a CRD which kyverno offers to support different kinds of rules. Here is an example ClusterPolicy which means that you must create pod with app.kubernetes.io/name label. console kubectl config use-context karmada-apiserver console kubectl create -f- << EOF apiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-labels spec: validationFailureAction: enforce rules: - name: check-for-labels match: any: - resources: kinds: - Pod validate: message: \"label 'app.kubernetes.io/name' is required\" pattern: metadata: labels: app.kubernetes.io/name: \"?*\" EOF","title":"Create require-labels ClusterPolicy"},{"location":"working-with-kyverno/#create-a-bad-deployment-without-labels","text":"console kubectl create deployment nginx --image=nginx error: failed to create deployment: admission webhook \"validate.kyverno.svc-fail\" denied the request","title":"Create a bad deployment without labels"},{"location":"working-with-kyverno/#reference","text":"https://github.com/kyverno/kyverno","title":"Reference"},{"location":"working-with-prometheus/","text":"Use Prometheus to monitor Karmada member clusters Prometheus , a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed. This document gives an example to demonstrate how to use the Prometheus to monitor karmada member clusters. Start up Karmada clusters You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh Start Prometheus Create resource objects of prometheus, the content is as follows. ``` apiVersion: v1 kind: Namespace metadata: name: monitor labels: name: monitor apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] - apiGroups: - extensions resources: - ingresses verbs: [\"get\", \"list\", \"watch\"] - nonResourceURLs: [\"/metrics\"] verbs: [\"get\"] apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: monitor apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: monitor apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: monitor data: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [ meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address replacement: kubernetes.default.svc:443 - source_labels: [ meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: meta_kubernetes_node_label_(.+) - target_label: __address replacement: kubernetes.default.svc:443 - source_labels: [ meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [ meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme regex: (https?) - source_labels: [ meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path regex: (.+) - source_labels: [ address , meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address ] target_label: param_target - target_label: __address replacement: blackbox-exporter.example.com:9115 - source_labels: [ param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address , meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: ${1}://${2}${3} target_label: __param_target - target_label: __address replacement: blackbox-exporter.example.com:9115 - source_labels: [ param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path regex: (.+) - source_labels: [ address , meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: kube-state-metrics static_configs: - targets: ['kube-state-metrics.monitor.svc.cluster.local:8080'] kind: Service apiVersion: v1 metadata: labels: app: prometheus name: prometheus namespace: monitor spec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus apiVersion: apps/v1 kind: Deployment metadata: labels: name: prometheus-deployment name: prometheus namespace: monitor spec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus imagePullPolicy: IfNotPresent name: prometheus command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/home/prometheus\" - \"--storage.tsdb.retention=168h\" - \"--web.enable-lifecycle\" ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: \"/home/prometheus\" name: data - mountPath: \"/etc/prometheus\" name: config-volume resources: requests: cpu: 100m memory: 256Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus securityContext: runAsUser: 0 volumes: - name: data hostPath: path: \"/data/prometheus/data\" - name: config-volume configMap: name: prometheus-config ``` Run the below command to execute karmada PropagationPolicy and ClusterPropagationPolicy. cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: prometheus-propagation namespace: monitor spec: resourceSelectors: - apiVersion: v1 kind: Namespace name: monitor - apiVersion: v1 kind: ServiceAccount name: prometheus namespace: monitor - apiVersion: v1 kind: ConfigMap name: prometheus-config namespace: monitor - apiVersion: v1 kind: Service name: prometheus namespace: monitor - apiVersion: apps/v1 kind: Deployment name: prometheus namespace: monitor placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: prometheusrbac-propagation spec: resourceSelectors: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole name: prometheus - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding name: prometheus placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF Use any node IP of the member cluster and the port number (default 30003) to enter the Prometheus monitoring page of the member cluster Reference https://github.com/prometheus/prometheus https://prometheus.io","title":"Use Prometheus to monitor Karmada member clusters"},{"location":"working-with-prometheus/#use-prometheus-to-monitor-karmada-member-clusters","text":"Prometheus , a Cloud Native Computing Foundation project, is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts when specified conditions are observed. This document gives an example to demonstrate how to use the Prometheus to monitor karmada member clusters.","title":"Use Prometheus to monitor Karmada member clusters"},{"location":"working-with-prometheus/#start-up-karmada-clusters","text":"You just need to clone Karmada repo, and run the following script in Karmada directory. hack/local-up-karmada.sh","title":"Start up Karmada clusters"},{"location":"working-with-prometheus/#start-prometheus","text":"Create resource objects of prometheus, the content is as follows. ``` apiVersion: v1 kind: Namespace metadata: name: monitor labels: name: monitor apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\"\"] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] - apiGroups: - extensions resources: - ingresses verbs: [\"get\", \"list\", \"watch\"] - nonResourceURLs: [\"/metrics\"] verbs: [\"get\"] apiVersion: v1 kind: ServiceAccount metadata: name: prometheus namespace: monitor apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: prometheus namespace: monitor apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: monitor data: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [ meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address replacement: kubernetes.default.svc:443 - source_labels: [ meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: meta_kubernetes_node_label_(.+) - target_label: __address replacement: kubernetes.default.svc:443 - source_labels: [ meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [ meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme regex: (https?) - source_labels: [ meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path regex: (.+) - source_labels: [ address , meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'kubernetes-services' kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address ] target_label: param_target - target_label: __address replacement: blackbox-exporter.example.com:9115 - source_labels: [ param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: 'kubernetes-ingresses' kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address , meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: ${1}://${2}${3} target_label: __param_target - target_label: __address replacement: blackbox-exporter.example.com:9115 - source_labels: [ param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path regex: (.+) - source_labels: [ address , meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - job_name: kube-state-metrics static_configs: - targets: ['kube-state-metrics.monitor.svc.cluster.local:8080'] kind: Service apiVersion: v1 metadata: labels: app: prometheus name: prometheus namespace: monitor spec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus apiVersion: apps/v1 kind: Deployment metadata: labels: name: prometheus-deployment name: prometheus namespace: monitor spec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus imagePullPolicy: IfNotPresent name: prometheus command: - \"/bin/prometheus\" args: - \"--config.file=/etc/prometheus/prometheus.yml\" - \"--storage.tsdb.path=/home/prometheus\" - \"--storage.tsdb.retention=168h\" - \"--web.enable-lifecycle\" ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: \"/home/prometheus\" name: data - mountPath: \"/etc/prometheus\" name: config-volume resources: requests: cpu: 100m memory: 256Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus securityContext: runAsUser: 0 volumes: - name: data hostPath: path: \"/data/prometheus/data\" - name: config-volume configMap: name: prometheus-config ``` Run the below command to execute karmada PropagationPolicy and ClusterPropagationPolicy. cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: prometheus-propagation namespace: monitor spec: resourceSelectors: - apiVersion: v1 kind: Namespace name: monitor - apiVersion: v1 kind: ServiceAccount name: prometheus namespace: monitor - apiVersion: v1 kind: ConfigMap name: prometheus-config namespace: monitor - apiVersion: v1 kind: Service name: prometheus namespace: monitor - apiVersion: apps/v1 kind: Deployment name: prometheus namespace: monitor placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF cat <<EOF | kubectl apply -f - apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: prometheusrbac-propagation spec: resourceSelectors: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole name: prometheus - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding name: prometheus placement: clusterAffinity: clusterNames: - member1 - member2 - member3 EOF Use any node IP of the member cluster and the port number (default 30003) to enter the Prometheus monitoring page of the member cluster","title":"Start Prometheus"},{"location":"working-with-prometheus/#reference","text":"https://github.com/prometheus/prometheus https://prometheus.io","title":"Reference"},{"location":"working-with-submariner/","text":"Use Submariner to connect the network between Karmada member clusters This document uses an example to demonstrate how to use the Submariner to connect the network between member clusters. Submariner flattens the networks between the connected clusters, and enables IP reachability between Pods and Services. Install Karmada Install karmada control plane Following the steps Install karmada control plane in Quick Start, you can get a Karmada. Join member cluster In the following steps, we are going to create a member cluster and then join the cluster to karmada control plane. Create member cluster We are going to create a cluster named cluster1 and we want the KUBECONFIG file in $HOME/.kube/cluster.config. Run following command: # hack/create-cluster.sh cluster1 $HOME/.kube/cluster1.config This will create a cluster by kind. Join member cluster to karmada control plane Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Then, install karmadactl command and join the member cluster: # go install github.com/karmada-io/karmada/cmd/karmadactl # karmadactl join cluster1 --cluster-kubeconfig=$HOME/.kube/cluster1.config In addition to the original member clusters, ensure that at least two member clusters are joined to the Karmada. In this example, we have joined two member clusters to the Karmada: # kubectl get clusters NAME VERSION MODE READY AGE cluster1 v1.21.1 Push True 16s cluster2 v1.21.1 Push True 5s ... Deploy Submariner We are going to deploy Submariner componnets on the host cluster and member clusters by using the subctl CLI as it's the recommended deployment method according to Submariner official documentation . Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker must be deployed on a single Kubernetes cluster. This cluster\u2019s API server must be reachable by all Kubernetes clusters connected by Submariner, therefore, we deployed it on the karmada-host cluster. Install subctl Please refer to the SUBCTL Installation . Use karmada-host as Broker subctl deploy-broker --kubeconfig /root/.kube/karmada.config --kubecontext karmada-host Join cluster1 and cluster2 to the Broker subctl join --kubeconfig /root/.kube/cluster1.config broker-info.subm --natt=false subctl join --kubeconfig /root/.kube/cluster2.config broker-info.subm --natt=false Connectivity test Please refer to the Multi-cluster Service Discovery .","title":"Use Submariner to connect the network between Karmada member clusters"},{"location":"working-with-submariner/#use-submariner-to-connect-the-network-between-karmada-member-clusters","text":"This document uses an example to demonstrate how to use the Submariner to connect the network between member clusters. Submariner flattens the networks between the connected clusters, and enables IP reachability between Pods and Services.","title":"Use Submariner to connect the network between Karmada member clusters"},{"location":"working-with-submariner/#install-karmada","text":"","title":"Install Karmada"},{"location":"working-with-submariner/#install-karmada-control-plane","text":"Following the steps Install karmada control plane in Quick Start, you can get a Karmada.","title":"Install karmada control plane"},{"location":"working-with-submariner/#join-member-cluster","text":"In the following steps, we are going to create a member cluster and then join the cluster to karmada control plane. Create member cluster We are going to create a cluster named cluster1 and we want the KUBECONFIG file in $HOME/.kube/cluster.config. Run following command: # hack/create-cluster.sh cluster1 $HOME/.kube/cluster1.config This will create a cluster by kind. Join member cluster to karmada control plane Export KUBECONFIG and switch to karmada apiserver : # export KUBECONFIG=$HOME/.kube/karmada.config # kubectl config use-context karmada-apiserver Then, install karmadactl command and join the member cluster: # go install github.com/karmada-io/karmada/cmd/karmadactl # karmadactl join cluster1 --cluster-kubeconfig=$HOME/.kube/cluster1.config In addition to the original member clusters, ensure that at least two member clusters are joined to the Karmada. In this example, we have joined two member clusters to the Karmada: # kubectl get clusters NAME VERSION MODE READY AGE cluster1 v1.21.1 Push True 16s cluster2 v1.21.1 Push True 5s ...","title":"Join member cluster"},{"location":"working-with-submariner/#deploy-submariner","text":"We are going to deploy Submariner componnets on the host cluster and member clusters by using the subctl CLI as it's the recommended deployment method according to Submariner official documentation . Submariner uses a central Broker component to facilitate the exchange of metadata information between Gateway Engines deployed in participating clusters. The Broker must be deployed on a single Kubernetes cluster. This cluster\u2019s API server must be reachable by all Kubernetes clusters connected by Submariner, therefore, we deployed it on the karmada-host cluster.","title":"Deploy Submariner"},{"location":"working-with-submariner/#install-subctl","text":"Please refer to the SUBCTL Installation .","title":"Install subctl"},{"location":"working-with-submariner/#use-karmada-host-as-broker","text":"subctl deploy-broker --kubeconfig /root/.kube/karmada.config --kubecontext karmada-host","title":"Use karmada-host as Broker"},{"location":"working-with-submariner/#join-cluster1-and-cluster2-to-the-broker","text":"subctl join --kubeconfig /root/.kube/cluster1.config broker-info.subm --natt=false subctl join --kubeconfig /root/.kube/cluster2.config broker-info.subm --natt=false","title":"Join cluster1 and cluster2 to the Broker"},{"location":"working-with-submariner/#connectivity-test","text":"Please refer to the Multi-cluster Service Discovery .","title":"Connectivity test"},{"location":"working-with-velero/","text":"Integrate Velero to back up and restore Karmada resources Velero gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a public cloud platform or on-premises. Velero lets you: Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. This document gives an example to demonstrate how to use the Velero to back up and restore Kubernetes cluster resources and persistent volumes. Following example backups resources in cluster member1 , and then restores those to cluster member2 . Start up Karamda clusters You just need to clone Karamda repo, and run the following script in Karamda directory. hack/local-up-karmada.sh And then run the below command to switch to the member cluster member1 . export KUBECONFIG=/root/.kube/members.config kubectl config use-context member1 Install MinIO Velero uses Object Storage Services from different cloud providers to support backup and snapshot operations. For simplicity, here takes, one object storage that runs locally on k8s clusters, for an example. Download the binary from the official site: wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio Run the below command to set MinIO username and password: export MINIO_ROOT_USER=minio export MINIO_ROOT_PASSWORD=minio123 Run this command to start MinIO : ./minio server /data --console-address=\"0.0.0.0:20001\" --address=\"0.0.0.0:9000\" Replace /data with the path to the drive or directory in which you want MinIO to store data. And now we can visit http://{SERVER_EXTERNAL_IP}/20001 in the browser to visit MinIO console UI. And Velero can use http://{SERVER_EXTERNAL_IP}/9000 to connect MinIO . The two configuration will make our follow-up work easier and more convenient. Please visit MinIO console to create region minio and bucket velero , these will be used by Velero . For more details about how to install MinIO , please run minio server --help for help, or you can visit MinIO Github Repo . Install Velero Velero consists of two components: - ### A command-line client that runs locally. 1. Download the release tarball for your client platform shell wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz Extract the tarball: shell tar -zxvf velero-v1.7.0-linux-amd64.tar.gz Move the extracted velero binary to somewhere in your $PATH (/usr/local/bin for most users). shell cp velero-v1.7.0-linux-amd64/velero /usr/local/bin/ A server that runs on your cluster We will use velero install to set up server components. For more details about how to use MinIO and Velero to backup resources, please ref: https://velero.io/docs/v1.7/contributions/minio/ Create a Velero-specific credentials file (credentials-velero) in your local directory: shell [default] aws_access_key_id = minio aws_secret_access_key = minio123 The two values should keep the same with MinIO username and password that we set when we install MinIO Start the server. We need to install Velero in both member1 and member2 , so we should run the below command in shell for both two clusters, this will start Velero server. Please run kubectl config use-context member1 and kubectl config use-context member2 to switch to the different member clusters: member1 or member2 . shell velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.2.1 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://{SERVER_EXTERNAL_IP}:9000 Replace {SERVER_EXTERNAL_IP} with your own server external IP. Deploy the nginx application to cluster member1 : Run the below command in the Karmada directory. shell kubectl apply -f samples/nginx/deployment.yaml And then you will find nginx is deployed successfully. shell # kubectl get deployment.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 17s Back up and restore Karmada resources Create a backup in member1 : velero backup create nginx-backup --selector app=nginx Restore the backup in member2 Run this command to switch to member2 export KUBECONFIG=/root/.kube/members.config kubectl config use-context member2 In member2 , we can also get the backup that we created in member1 : # velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021-12-10 15:16:46 +0800 CST 29d default app=nginx Restore member1 resources to member2 : # velero restore create --from-backup nginx-backup Restore request \"nginx-backup-20211210151807\" submitted successfully. Watch restore result, you'll find that the status is Completed. # velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20211210151807 nginx-backup Completed 2021-12-10 15:18:07 +0800 CST 2021-12-10 15:18:07 +0800 CST 0 0 2021-12-10 15:18:07 +0800 CST <none> And then you can find deployment nginx will be restored successfully. # kubectl get deployment.apps/nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 21s Reference The above introductions about Velero and MinIO are only a summary from the official website and repos, for more details please refer to: - Velero: https://velero.io/ - MinIO: https://min.io/","title":"Integrate Velero to back up and restore Karmada resources"},{"location":"working-with-velero/#integrate-velero-to-back-up-and-restore-karmada-resources","text":"Velero gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a public cloud platform or on-premises. Velero lets you: Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. This document gives an example to demonstrate how to use the Velero to back up and restore Kubernetes cluster resources and persistent volumes. Following example backups resources in cluster member1 , and then restores those to cluster member2 .","title":"Integrate Velero to back up and restore Karmada resources"},{"location":"working-with-velero/#start-up-karamda-clusters","text":"You just need to clone Karamda repo, and run the following script in Karamda directory. hack/local-up-karmada.sh And then run the below command to switch to the member cluster member1 . export KUBECONFIG=/root/.kube/members.config kubectl config use-context member1","title":"Start up Karamda clusters"},{"location":"working-with-velero/#install-minio","text":"Velero uses Object Storage Services from different cloud providers to support backup and snapshot operations. For simplicity, here takes, one object storage that runs locally on k8s clusters, for an example. Download the binary from the official site: wget https://dl.min.io/server/minio/release/linux-amd64/minio chmod +x minio Run the below command to set MinIO username and password: export MINIO_ROOT_USER=minio export MINIO_ROOT_PASSWORD=minio123 Run this command to start MinIO : ./minio server /data --console-address=\"0.0.0.0:20001\" --address=\"0.0.0.0:9000\" Replace /data with the path to the drive or directory in which you want MinIO to store data. And now we can visit http://{SERVER_EXTERNAL_IP}/20001 in the browser to visit MinIO console UI. And Velero can use http://{SERVER_EXTERNAL_IP}/9000 to connect MinIO . The two configuration will make our follow-up work easier and more convenient. Please visit MinIO console to create region minio and bucket velero , these will be used by Velero . For more details about how to install MinIO , please run minio server --help for help, or you can visit MinIO Github Repo .","title":"Install MinIO"},{"location":"working-with-velero/#install-velero","text":"Velero consists of two components: - ### A command-line client that runs locally. 1. Download the release tarball for your client platform shell wget https://github.com/vmware-tanzu/velero/releases/download/v1.7.0/velero-v1.7.0-linux-amd64.tar.gz Extract the tarball: shell tar -zxvf velero-v1.7.0-linux-amd64.tar.gz Move the extracted velero binary to somewhere in your $PATH (/usr/local/bin for most users). shell cp velero-v1.7.0-linux-amd64/velero /usr/local/bin/","title":"Install Velero"},{"location":"working-with-velero/#a-server-that-runs-on-your-cluster","text":"We will use velero install to set up server components. For more details about how to use MinIO and Velero to backup resources, please ref: https://velero.io/docs/v1.7/contributions/minio/ Create a Velero-specific credentials file (credentials-velero) in your local directory: shell [default] aws_access_key_id = minio aws_secret_access_key = minio123 The two values should keep the same with MinIO username and password that we set when we install MinIO Start the server. We need to install Velero in both member1 and member2 , so we should run the below command in shell for both two clusters, this will start Velero server. Please run kubectl config use-context member1 and kubectl config use-context member2 to switch to the different member clusters: member1 or member2 . shell velero install \\ --provider aws \\ --plugins velero/velero-plugin-for-aws:v1.2.1 \\ --bucket velero \\ --secret-file ./credentials-velero \\ --use-volume-snapshots=false \\ --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://{SERVER_EXTERNAL_IP}:9000 Replace {SERVER_EXTERNAL_IP} with your own server external IP. Deploy the nginx application to cluster member1 : Run the below command in the Karmada directory. shell kubectl apply -f samples/nginx/deployment.yaml And then you will find nginx is deployed successfully. shell # kubectl get deployment.apps NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 17s","title":"A server that runs on your cluster"},{"location":"working-with-velero/#back-up-and-restore-karmada-resources","text":"Create a backup in member1 : velero backup create nginx-backup --selector app=nginx Restore the backup in member2 Run this command to switch to member2 export KUBECONFIG=/root/.kube/members.config kubectl config use-context member2 In member2 , we can also get the backup that we created in member1 : # velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021-12-10 15:16:46 +0800 CST 29d default app=nginx Restore member1 resources to member2 : # velero restore create --from-backup nginx-backup Restore request \"nginx-backup-20211210151807\" submitted successfully. Watch restore result, you'll find that the status is Completed. # velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20211210151807 nginx-backup Completed 2021-12-10 15:18:07 +0800 CST 2021-12-10 15:18:07 +0800 CST 0 0 2021-12-10 15:18:07 +0800 CST <none> And then you can find deployment nginx will be restored successfully. # kubectl get deployment.apps/nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 21s","title":"Back up and restore Karmada resources"},{"location":"working-with-velero/#reference","text":"The above introductions about Velero and MinIO are only a summary from the official website and repos, for more details please refer to: - Velero: https://velero.io/ - MinIO: https://min.io/","title":"Reference"},{"location":"CHANGELOG/CHANGELOG-0.10/","text":"What's New Resource Interpreter Webhook The newly introduced Resource Interpreter Webhook framework allows users to implement their own CRD plugins that will be consulted at all parts of propagation process. With this feature, CRDs and CRs will be propagated just like Kubernetes native resources, which means all scheduling primitives also support custom resources. An example as well as some helpful utilities are provided to help users better understand how this framework works. Refer to Proposal for more details. Significant Scheduling Enhancement Introduced dynamicWeight primitive to PropagationPolicy and ClusterPropagationPolicy . With this feature, replicas could be divided by a dynamic weight list, and the weight of each cluster will be calculated based on the available replicas during scheduling. This feature can balance the cluster's utilization significantly. #841 Introduced Job schedule (divide) support. A Job that desires many replicas now could be divided into many clusters just like Deployment . This feature makes it possible to run huge Jobs across small clusters. #898 Workloads Observation from Karmada Control Plane After workloads (e.g. Deployments) are propagated to member clusters, users may also want to get the overall workload status across many clusters, especially the status of each pod . In this release, a get subcommand was introduced to the kubectl-karmada . With this command, user are now able get all kinds of resources deployed in member clusters from the Karmada control plane. For example (get deployment and pods across clusters): $ kubectl karmada get deployment NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 1/1 1 1 19m Y nginx member1 1/1 1 1 19m Y $ kubectl karmada get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-vzdvt member1 1/1 Running 0 31m nginx-6799fc88d8-l55kk member2 1/1 Running 0 31m Other Notable Changes karmada-scheduler-estimator: The number of pods becomes an important reference when calculating available replicas for the cluster. #777 The labels ( resourcebinding.karmada.io/namespace , resourcebinding.karmada.io/name , clusterresourcebinding.karmada.io/name ) which were previously added on the Work object now have been moved to annotations. #752 Bugfix: Fixed the impact of cluster unjoining on resource status aggregation. #817 Instrumentation: Introduced events ( SyncFailed and SyncSucceed ) to the Work object. #800 Instrumentation: Introduced condition ( Scheduled ) to the ResourceBinding and ClusterResourceBinding . #823 Instrumentation: Introduced events ( CreateExecutionNamespaceFailed and RemoveExecutionNamespaceFailed ) to the Cluster object. #749 Instrumentation: Introduced several metrics ( workqueue_adds_total , workqueue_depth , workqueue_longest_running_processor_seconds , workqueue_queue_duration_seconds_bucket ) for karmada-agent and karmada-controller-manager . #831 Instrumentation: Introduced condition ( FullyApplied ) to the ResourceBinding and ClusterResourceBinding . #825 karmada-scheduler: Introduced feature gates. #805 karmada-controller-manager: Deleted resources from member clusters that use \"Background\" as the default delete option. #970","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-0.10/#whats-new","text":"","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-0.10/#resource-interpreter-webhook","text":"The newly introduced Resource Interpreter Webhook framework allows users to implement their own CRD plugins that will be consulted at all parts of propagation process. With this feature, CRDs and CRs will be propagated just like Kubernetes native resources, which means all scheduling primitives also support custom resources. An example as well as some helpful utilities are provided to help users better understand how this framework works. Refer to Proposal for more details.","title":"Resource Interpreter Webhook"},{"location":"CHANGELOG/CHANGELOG-0.10/#significant-scheduling-enhancement","text":"Introduced dynamicWeight primitive to PropagationPolicy and ClusterPropagationPolicy . With this feature, replicas could be divided by a dynamic weight list, and the weight of each cluster will be calculated based on the available replicas during scheduling. This feature can balance the cluster's utilization significantly. #841 Introduced Job schedule (divide) support. A Job that desires many replicas now could be divided into many clusters just like Deployment . This feature makes it possible to run huge Jobs across small clusters. #898","title":"Significant Scheduling Enhancement"},{"location":"CHANGELOG/CHANGELOG-0.10/#workloads-observation-from-karmada-control-plane","text":"After workloads (e.g. Deployments) are propagated to member clusters, users may also want to get the overall workload status across many clusters, especially the status of each pod . In this release, a get subcommand was introduced to the kubectl-karmada . With this command, user are now able get all kinds of resources deployed in member clusters from the Karmada control plane. For example (get deployment and pods across clusters): $ kubectl karmada get deployment NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 1/1 1 1 19m Y nginx member1 1/1 1 1 19m Y $ kubectl karmada get pods NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-vzdvt member1 1/1 Running 0 31m nginx-6799fc88d8-l55kk member2 1/1 Running 0 31m","title":"Workloads Observation from Karmada Control Plane"},{"location":"CHANGELOG/CHANGELOG-0.10/#other-notable-changes","text":"karmada-scheduler-estimator: The number of pods becomes an important reference when calculating available replicas for the cluster. #777 The labels ( resourcebinding.karmada.io/namespace , resourcebinding.karmada.io/name , clusterresourcebinding.karmada.io/name ) which were previously added on the Work object now have been moved to annotations. #752 Bugfix: Fixed the impact of cluster unjoining on resource status aggregation. #817 Instrumentation: Introduced events ( SyncFailed and SyncSucceed ) to the Work object. #800 Instrumentation: Introduced condition ( Scheduled ) to the ResourceBinding and ClusterResourceBinding . #823 Instrumentation: Introduced events ( CreateExecutionNamespaceFailed and RemoveExecutionNamespaceFailed ) to the Cluster object. #749 Instrumentation: Introduced several metrics ( workqueue_adds_total , workqueue_depth , workqueue_longest_running_processor_seconds , workqueue_queue_duration_seconds_bucket ) for karmada-agent and karmada-controller-manager . #831 Instrumentation: Introduced condition ( FullyApplied ) to the ResourceBinding and ClusterResourceBinding . #825 karmada-scheduler: Introduced feature gates. #805 karmada-controller-manager: Deleted resources from member clusters that use \"Background\" as the default delete option. #970","title":"Other Notable Changes"},{"location":"CHANGELOG/CHANGELOG-0.9/","text":"What's New Upgrading support Users are now able to upgrade from the previous version smoothly. With the multiple version feature of CRD, objects with different schemas can be automatically converted between versions. Karmada uses the semantic versioning and will provide workarounds for inevitable breaking changes. In this release, the ResourceBining and ClusterResourceBinding promote to v1alpha2 and the previous v1alpha1 version is still available for one more release. With the upgrading instruction , the previous version of Karmada can promote smoothly. Introduced karmada-scheduler-estimator to facilitate end-to-end scheduling accuracy Karmada scheduler aims to assign workload to clusters according to constraints and available resources of each member cluster. The kube-scheduler working on each cluster takes the responsibility to assign Pods to Nodes. Even though Karmada has the capacity to reschedule failure workload between member clusters, but the community still commits lots of effort to improve the accuracy of the end-to-end scheduling. The karmada-scheduler-estimator is the effective assistant of karmada-scheduler , it provides prediction-based scheduling decisions that can significantly improve the scheduling efficiency and avoid the wave of rescheduling among clusters. Note that this feature is implemented as a pluggable add-on. For the instructions please refer to scheduler estimator guideline . Maintainability improvements A bunch of significant maintainability improvements were added to this release, including: Simplified Karmada installation with helm chart . Provided metrics to observe scheduler status, the metrics API now served at /metrics of karmada-scheduler . With these metrics, users are now able to evaluate the scheduler's performance and identify the bottlenecks. Provided events to Karmada API objects as supplemental information to debug problems. Other Notable Changes karmada-controller-manager: The ResourceBinding/ClusterResourceBinding won't be deleted after associate PropagationPolicy/ClusterPropagationPolicy is removed and is still available until resource template is removed.( #601 ) Introduced --leader-elect-resource-namespace which is used to specify the namespace of election object to components karmada-controller-manager/karmada-scheduler/karmada-agent`. ( #698 ) Deprecation: The API ReplicaSchedulingPolicy has been deprecated and will be removed from the following release. The feature now has been integrated into ReplicaScheduling. Introduced kubectl-karmada commands as the extensions for kubectl. ( #686 ) karmada-controller-manager introduced a version command to represent version information. ( #717 ) karmada-scheduler/karmada-webhook/karmada-agent/karmada-scheduler-estimator introduced a version command to represent version information. ( #719 ) Provided instructions about how to use the Submariner to connect the network between member clusters. ( #737 ) Added four metrics to the karmada-scheduler to monitor scheduler performance. ( #747 )","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-0.9/#whats-new","text":"","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-0.9/#upgrading-support","text":"Users are now able to upgrade from the previous version smoothly. With the multiple version feature of CRD, objects with different schemas can be automatically converted between versions. Karmada uses the semantic versioning and will provide workarounds for inevitable breaking changes. In this release, the ResourceBining and ClusterResourceBinding promote to v1alpha2 and the previous v1alpha1 version is still available for one more release. With the upgrading instruction , the previous version of Karmada can promote smoothly.","title":"Upgrading support"},{"location":"CHANGELOG/CHANGELOG-0.9/#introduced-karmada-scheduler-estimator-to-facilitate-end-to-end-scheduling-accuracy","text":"Karmada scheduler aims to assign workload to clusters according to constraints and available resources of each member cluster. The kube-scheduler working on each cluster takes the responsibility to assign Pods to Nodes. Even though Karmada has the capacity to reschedule failure workload between member clusters, but the community still commits lots of effort to improve the accuracy of the end-to-end scheduling. The karmada-scheduler-estimator is the effective assistant of karmada-scheduler , it provides prediction-based scheduling decisions that can significantly improve the scheduling efficiency and avoid the wave of rescheduling among clusters. Note that this feature is implemented as a pluggable add-on. For the instructions please refer to scheduler estimator guideline .","title":"Introduced karmada-scheduler-estimator to facilitate end-to-end scheduling accuracy"},{"location":"CHANGELOG/CHANGELOG-0.9/#maintainability-improvements","text":"A bunch of significant maintainability improvements were added to this release, including: Simplified Karmada installation with helm chart . Provided metrics to observe scheduler status, the metrics API now served at /metrics of karmada-scheduler . With these metrics, users are now able to evaluate the scheduler's performance and identify the bottlenecks. Provided events to Karmada API objects as supplemental information to debug problems.","title":"Maintainability improvements"},{"location":"CHANGELOG/CHANGELOG-0.9/#other-notable-changes","text":"karmada-controller-manager: The ResourceBinding/ClusterResourceBinding won't be deleted after associate PropagationPolicy/ClusterPropagationPolicy is removed and is still available until resource template is removed.( #601 ) Introduced --leader-elect-resource-namespace which is used to specify the namespace of election object to components karmada-controller-manager/karmada-scheduler/karmada-agent`. ( #698 ) Deprecation: The API ReplicaSchedulingPolicy has been deprecated and will be removed from the following release. The feature now has been integrated into ReplicaScheduling. Introduced kubectl-karmada commands as the extensions for kubectl. ( #686 ) karmada-controller-manager introduced a version command to represent version information. ( #717 ) karmada-scheduler/karmada-webhook/karmada-agent/karmada-scheduler-estimator introduced a version command to represent version information. ( #719 ) Provided instructions about how to use the Submariner to connect the network between member clusters. ( #737 ) Added four metrics to the karmada-scheduler to monitor scheduler performance. ( #747 )","title":"Other Notable Changes"},{"location":"CHANGELOG/CHANGELOG-1.0/","text":"What's New Aggregated Kubernetes API Endpoint The newly introduced karmada-aggregated-apiserver component aggregates all registered clusters and allows users to access member clusters through Karmada by the proxy endpoint, e.g. Retrieve Node from member1 : /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes Retrieve Pod from member2 : /apis/cluster.karmada.io/v1alpha1/clusters/member2/proxy/api/v1/namespaces/default/pods Please refer to user guide for more details. Promoting Workloads from Legacy Clusters to Karmada Legacy workloads running in Kubernetes now can be promoted to Karmada smoothly without container restart. In favor of promote commands added to Karmada CLI, any kind of Kubernetes resources can be promoted to Karmada easily, e.g. # Promote deployment(default/nginx) from cluster1 to Karmada kubectl karmada promote deployment nginx -n default -c cluster1 Verified Integration with Ecosystem Benefiting from the Kubernetes native API support, Karmada can easily integrate the single cluster ecosystem for multi-cluster, multi-cloud purpose. The following components have been verified by the Karmada community: argo-cd: refer to working with argo-cd Flux: refer to propagating helm charts with flux Istio: refer to working with Istio Filebeat: refer to working with Filebeat Submariner: refer to working with Submariner Velero: refer to working with Velero Prometheus: refer to working with Prometheus OverridePolicy Improvements By leverage of the new-introduced RuleWithCluster fields to OverridePolicy and ClusterOverridePolicy , users are now able to define override policies with a single policy for specified workloads. Karmada Installation Improvements Introduced init command to Karmada CLI . Users are now able to install Karmada by a single command. Please refer to Installing Karmada for more details. Configuring Karmada Controllers Now all controllers provided by Karmada work as plug-ins. Users are now able to turn off any of them from the default enabled list. See --controllers flag of karmada-controller-manager and karmada-agent for more details. Resource Interpreter Webhook Enhancement Introduced ReviseReplica support for the Resource Interpreter Webhook framework, which enables scheduling all customized workloads just like Kubernetes native ones. Refer to Resource Interpreter Webhook Proposal for more design details. Other Notable Changes Bug Fixes karmada-controller-manager : Fixed the issue that the annotation of resource template cannot be updated. #1012 karmada-controller-manager : Fixed the issue of generating binding reference key. #1003 karmada-controller-manager : Fixed the inefficiency of en-queue failed task issue. #1068 Features & Enhancements Karmada CLI : Introduced --cluster-provider flag to join command to specify provider of joining cluster. #1025 Karmada CLI : Introduced taint command to set taints for clusters. #889 Karmada CLI : The Applied condition of Work and Scheduled/FullyApplied of ResourceBinding are available for kubectl get . #1110 karmada-controller-manager : The cluster discovery feature now supports v1beta1 of cluster-api . #1029 karmada-controller-manager : The Job 's startTime and completionTime now available at resource template. #1034 karmada-controller-manager : introduced --controllers flag to enable or disable controllers. #1083 karmada-controller-manager : Support retain ownerReference from observed objects. #1116 karmada-controller-manager and karmada-agent : Introduced cluster-cache-sync-timeout flag to specify the time waiting for cache sync. #1112 Instrumentation (Metrics and Events) karmada-scheduler-estimator : Introduced /metrics endpoint to emit metrics. #1030 Introduced ApplyPolicy and ScheduleBinding events for resource template. #1070 Deprecation The ReplicaSchedulingPolicy API deprecated at v0.9.0 now has been removed in favor of ReplicaScheduling of PropagationPolicy . #1161","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-1.0/#whats-new","text":"","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-1.0/#aggregated-kubernetes-api-endpoint","text":"The newly introduced karmada-aggregated-apiserver component aggregates all registered clusters and allows users to access member clusters through Karmada by the proxy endpoint, e.g. Retrieve Node from member1 : /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes Retrieve Pod from member2 : /apis/cluster.karmada.io/v1alpha1/clusters/member2/proxy/api/v1/namespaces/default/pods Please refer to user guide for more details.","title":"Aggregated Kubernetes API Endpoint"},{"location":"CHANGELOG/CHANGELOG-1.0/#promoting-workloads-from-legacy-clusters-to-karmada","text":"Legacy workloads running in Kubernetes now can be promoted to Karmada smoothly without container restart. In favor of promote commands added to Karmada CLI, any kind of Kubernetes resources can be promoted to Karmada easily, e.g. # Promote deployment(default/nginx) from cluster1 to Karmada kubectl karmada promote deployment nginx -n default -c cluster1","title":"Promoting Workloads from Legacy Clusters to Karmada"},{"location":"CHANGELOG/CHANGELOG-1.0/#verified-integration-with-ecosystem","text":"Benefiting from the Kubernetes native API support, Karmada can easily integrate the single cluster ecosystem for multi-cluster, multi-cloud purpose. The following components have been verified by the Karmada community: argo-cd: refer to working with argo-cd Flux: refer to propagating helm charts with flux Istio: refer to working with Istio Filebeat: refer to working with Filebeat Submariner: refer to working with Submariner Velero: refer to working with Velero Prometheus: refer to working with Prometheus","title":"Verified Integration with Ecosystem"},{"location":"CHANGELOG/CHANGELOG-1.0/#overridepolicy-improvements","text":"By leverage of the new-introduced RuleWithCluster fields to OverridePolicy and ClusterOverridePolicy , users are now able to define override policies with a single policy for specified workloads.","title":"OverridePolicy Improvements"},{"location":"CHANGELOG/CHANGELOG-1.0/#karmada-installation-improvements","text":"Introduced init command to Karmada CLI . Users are now able to install Karmada by a single command. Please refer to Installing Karmada for more details.","title":"Karmada Installation Improvements"},{"location":"CHANGELOG/CHANGELOG-1.0/#configuring-karmada-controllers","text":"Now all controllers provided by Karmada work as plug-ins. Users are now able to turn off any of them from the default enabled list. See --controllers flag of karmada-controller-manager and karmada-agent for more details.","title":"Configuring Karmada Controllers"},{"location":"CHANGELOG/CHANGELOG-1.0/#resource-interpreter-webhook-enhancement","text":"Introduced ReviseReplica support for the Resource Interpreter Webhook framework, which enables scheduling all customized workloads just like Kubernetes native ones. Refer to Resource Interpreter Webhook Proposal for more design details.","title":"Resource Interpreter Webhook Enhancement"},{"location":"CHANGELOG/CHANGELOG-1.0/#other-notable-changes","text":"","title":"Other Notable Changes"},{"location":"CHANGELOG/CHANGELOG-1.0/#bug-fixes","text":"karmada-controller-manager : Fixed the issue that the annotation of resource template cannot be updated. #1012 karmada-controller-manager : Fixed the issue of generating binding reference key. #1003 karmada-controller-manager : Fixed the inefficiency of en-queue failed task issue. #1068","title":"Bug Fixes"},{"location":"CHANGELOG/CHANGELOG-1.0/#features-enhancements","text":"Karmada CLI : Introduced --cluster-provider flag to join command to specify provider of joining cluster. #1025 Karmada CLI : Introduced taint command to set taints for clusters. #889 Karmada CLI : The Applied condition of Work and Scheduled/FullyApplied of ResourceBinding are available for kubectl get . #1110 karmada-controller-manager : The cluster discovery feature now supports v1beta1 of cluster-api . #1029 karmada-controller-manager : The Job 's startTime and completionTime now available at resource template. #1034 karmada-controller-manager : introduced --controllers flag to enable or disable controllers. #1083 karmada-controller-manager : Support retain ownerReference from observed objects. #1116 karmada-controller-manager and karmada-agent : Introduced cluster-cache-sync-timeout flag to specify the time waiting for cache sync. #1112","title":"Features &amp; Enhancements"},{"location":"CHANGELOG/CHANGELOG-1.0/#instrumentation-metrics-and-events","text":"karmada-scheduler-estimator : Introduced /metrics endpoint to emit metrics. #1030 Introduced ApplyPolicy and ScheduleBinding events for resource template. #1070","title":"Instrumentation (Metrics and Events)"},{"location":"CHANGELOG/CHANGELOG-1.0/#deprecation","text":"The ReplicaSchedulingPolicy API deprecated at v0.9.0 now has been removed in favor of ReplicaScheduling of PropagationPolicy . #1161","title":"Deprecation"},{"location":"CHANGELOG/CHANGELOG-1.1/","text":"What's New Multi-Cluster Ingress The newly introduced MultiClusterIngress API exposes HTTP and HTTPS routes that target multi-cluster services within the Karmada control plane. The specification of MultiClusterIngress is compatible with Kubernetes Ingress . Traffic routing is controlled by rules defined on the MultiClusterIngress resource, an MultiClusterIngress controller is responsible for fulfilling the ingress. The Multi-Cluster-Nginx Ingress Controller is one of the MultiClusterIngress controller implementations maintained by the community. Federated ResourceQuota The newly introduced FederatedResourceQuota provides constraints that limit total resource consumption per namespace across all clusters . It can limit the number of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace. Configurability improvement for performance tuning The default number of reconciling workers has been enlarged and configurable. A larger number of workers means higher responsiveness but heavier CPU and network load. The number of concurrent workers could be configured by the flags introduced to karmada-controller-manager and karmada-agent . Flags introduced to karmada-controller-manager : --concurrent-work-syncs --concurrent-namespace-syncs --concurrent-resource-template-syncs --concurrent-cluster-syncs --concurrent-clusterresourcebinding-syncs --concurrent-resourcebinding-syncs Flags introduced to karmada-agent : --concurrent-work-syncs --concurrent-cluster-syncs Resource Interpreter Webhook Enhancement Introduced AggregateStatus support for the Resource Interpreter Webhook framework, which enables customized resource status aggregating. Introduced InterpreterOperationInterpretDependency support for the Resource Interpreter Webhook framework, which enables propagating workload's dependencies automatically. Refer to Customizing Resource Interpreter for more details. Other Notable Changes Bug Fixes karmadactl and kubectl-karmada : Fixed that init cannot update the APIService . #1207 karmada-controller-manager : Fixed ApplyPolicySucceed event type mistake (should be Normal but not Warning ). #1267 karmada-controller-manager and karmada-agent : Fixed that resync slows down reconciliation. 1265 karmada-controller-manager / karmada-agent : Fixed continually updating cluster status due to unordered apiEnablements. #1304 karmada-controller-manager : Fixed that Replicas set by OverridePolicy will be reset by the ReviseReplica interpreterhook. #1352 karmada-controller-manager : Fixed that ResourceBinding couldn't be created in a corner case. #1368 karmada-scheduler : Fixed inaccuracy in requested resources in the case that pod limits are specified but requests are not. #1225 karmada-scheduler : Fixed spreadconstraints[i].MaxGroups is invalidated in some scenarios. #1324 Features & Enhancements karmadactl : Introduced --tls-min-version flag to specify the minimum TLS version. #1278 karmadactl : Improved the get command to show more useful information. #1270 karmada-controller-manager / karmada-agent : Introduced --resync-period flag to specify reflector resync period (defaults to 0, meaning no resync). #1261 karmada-controller-manager : Introduced --metrics-bind-address flag to specify the customized address for metrics. #1341 karmada-webhook : Introduced --metrics-bind-address and --health-probe-bind-address flags. #1346 Instrumentation (Metrics and Events) karmada-controller-manager : Fixed ApplyPolicySucceed event type mistake (should be Normal but not Warning). #1267 Deprecation OverridePolicy / ClusterOverridePolicy : The .spec.targetCluster and spec.overriders have been deprecated in favor of spec.overrideRules . #1238 karmada-aggregate-apiserver : Deprecated --master and --karmada-config flags. Please use --kubeconfig instead. #1336","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-1.1/#whats-new","text":"","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-1.1/#multi-cluster-ingress","text":"The newly introduced MultiClusterIngress API exposes HTTP and HTTPS routes that target multi-cluster services within the Karmada control plane. The specification of MultiClusterIngress is compatible with Kubernetes Ingress . Traffic routing is controlled by rules defined on the MultiClusterIngress resource, an MultiClusterIngress controller is responsible for fulfilling the ingress. The Multi-Cluster-Nginx Ingress Controller is one of the MultiClusterIngress controller implementations maintained by the community.","title":"Multi-Cluster Ingress"},{"location":"CHANGELOG/CHANGELOG-1.1/#federated-resourcequota","text":"The newly introduced FederatedResourceQuota provides constraints that limit total resource consumption per namespace across all clusters . It can limit the number of objects that can be created in a namespace by type, as well as the total amount of compute resources that may be consumed by resources in that namespace.","title":"Federated ResourceQuota"},{"location":"CHANGELOG/CHANGELOG-1.1/#configurability-improvement-for-performance-tuning","text":"The default number of reconciling workers has been enlarged and configurable. A larger number of workers means higher responsiveness but heavier CPU and network load. The number of concurrent workers could be configured by the flags introduced to karmada-controller-manager and karmada-agent . Flags introduced to karmada-controller-manager : --concurrent-work-syncs --concurrent-namespace-syncs --concurrent-resource-template-syncs --concurrent-cluster-syncs --concurrent-clusterresourcebinding-syncs --concurrent-resourcebinding-syncs Flags introduced to karmada-agent : --concurrent-work-syncs --concurrent-cluster-syncs","title":"Configurability improvement for performance tuning"},{"location":"CHANGELOG/CHANGELOG-1.1/#resource-interpreter-webhook-enhancement","text":"Introduced AggregateStatus support for the Resource Interpreter Webhook framework, which enables customized resource status aggregating. Introduced InterpreterOperationInterpretDependency support for the Resource Interpreter Webhook framework, which enables propagating workload's dependencies automatically. Refer to Customizing Resource Interpreter for more details.","title":"Resource Interpreter Webhook Enhancement"},{"location":"CHANGELOG/CHANGELOG-1.1/#other-notable-changes","text":"","title":"Other Notable Changes"},{"location":"CHANGELOG/CHANGELOG-1.1/#bug-fixes","text":"karmadactl and kubectl-karmada : Fixed that init cannot update the APIService . #1207 karmada-controller-manager : Fixed ApplyPolicySucceed event type mistake (should be Normal but not Warning ). #1267 karmada-controller-manager and karmada-agent : Fixed that resync slows down reconciliation. 1265 karmada-controller-manager / karmada-agent : Fixed continually updating cluster status due to unordered apiEnablements. #1304 karmada-controller-manager : Fixed that Replicas set by OverridePolicy will be reset by the ReviseReplica interpreterhook. #1352 karmada-controller-manager : Fixed that ResourceBinding couldn't be created in a corner case. #1368 karmada-scheduler : Fixed inaccuracy in requested resources in the case that pod limits are specified but requests are not. #1225 karmada-scheduler : Fixed spreadconstraints[i].MaxGroups is invalidated in some scenarios. #1324","title":"Bug Fixes"},{"location":"CHANGELOG/CHANGELOG-1.1/#features-enhancements","text":"karmadactl : Introduced --tls-min-version flag to specify the minimum TLS version. #1278 karmadactl : Improved the get command to show more useful information. #1270 karmada-controller-manager / karmada-agent : Introduced --resync-period flag to specify reflector resync period (defaults to 0, meaning no resync). #1261 karmada-controller-manager : Introduced --metrics-bind-address flag to specify the customized address for metrics. #1341 karmada-webhook : Introduced --metrics-bind-address and --health-probe-bind-address flags. #1346","title":"Features &amp; Enhancements"},{"location":"CHANGELOG/CHANGELOG-1.1/#instrumentation-metrics-and-events","text":"karmada-controller-manager : Fixed ApplyPolicySucceed event type mistake (should be Normal but not Warning). #1267","title":"Instrumentation (Metrics and Events)"},{"location":"CHANGELOG/CHANGELOG-1.1/#deprecation","text":"OverridePolicy / ClusterOverridePolicy : The .spec.targetCluster and spec.overriders have been deprecated in favor of spec.overrideRules . #1238 karmada-aggregate-apiserver : Deprecated --master and --karmada-config flags. Please use --kubeconfig instead. #1336","title":"Deprecation"},{"location":"CHANGELOG/CHANGELOG-1.2/","text":"v1.2.0 Downloads for v1.2.0 Karmada v1.2 Release Notes 1.2 What's New Other Notable Changes v1.2.0 Downloads for v1.2.0 Download v1.2.0 in the v1.2.0 release page . Karmada v1.2 Release Notes What's New Significant improvement on scheduling capability and scalability 1. Karmada Descheduler A new component karmada-descheduler was introduced, for rebalancing the scheduling decisions over time. One example use case is: it helps evict pending replicas (Pods) from resource-starved clusters so that karmada-scheduler can \"reschedule\" these replicas (Pods) to a cluster with sufficient resources. For more details please refer to Descheduler user guide. 2. Multi region HA support By leveraging the newly added spread-by-region constraint, users are now able to deploy workloads across regions , e.g. people may want their workloads always running on different regions for HA purposes. We also introduced two plugins to karmada-scheduler , which add to accurate scheduling. ClusterLocality is a scoring plugin that favors clusters already assigned. SpreadConstraint is a filter plugin that filters clusters as per spread constraints. We are also in the progress of enhancing the multi-cluster failover mechanism. Part of the work has been included in this release. For example: A new flag( --cluster-failure-threshold ) has been added to both karmada-controller-manager and karmada-agent , which specifies the cluster failure threshold (defaults to 30s). A cluster will be considered not-ready only when it stays unhealthy longer than supposed. A new flag( --failover-eviction-timeout ) has been added to karmada-controller-manager , which specifies the grace period of eviction (defaults to 5 minutes). If a cluster stays not-ready longer than supposed, the controller taints the cluster. (Note: The taint is essentially the eviction order and the implementation is planned for the next release.) Fully adopted aggregated API The Aggregated API was initially introduced in Release 1.0, which allows users to access clusters through Karmada by a single aggregated API endpoint. By leveraging this feature, we introduced a lot of interesting features to karmadactl and kubectl-karmada . The get sub-command now supports clusters both in push and pull mode. # karmadactl get deployment -n default NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member1 2/2 2 2 33h N nginx member2 1/1 1 1 4m38s Y podinfo member3 2/2 2 2 27h N The newly added logs command prints the container logs in a specific cluster. # ./karmadactl logs nginx-6799fc88d8-9mpxn -c nginx -C member1 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf ... We also added watch and exec commands to karmadactl , in addition to get and logs . They all use the aggregated API. Distributed search and analytics engine for Kubernetes resources ( alpha ) The newly introduced karmada-search caches resources in clusters and allows users to search for resources without directly touching real clusters. # kubectl get --raw /apis/search.karmada.io/v1alpha1/search/cache/apis/apps/v1/deployments { \"apiVersion\": \"v1\", \"kind\": \"List\", \"metadata\": {}, \"items\": [{ \"apiVersion\": \"apps/v1\", \"kind\": \"Deployment\", \"metadata\": { \"annotations\": { \"cluster.karmada.io/name\": \"member1\", }, } }, ] } The karmada-search also supports syncing cached resources to backend stores like Elasticsearch or OpenSearch . By leveraging the search engine, you can perform full-text searches with all desired features, by field, and by indice; rank results by score, sort results by field, and aggregate results. Resource Interpreter Webhook enhancement Introduced InterpretStatus for the Resource Interpreter Webhook framework, which enables customized resource status collection. Karmada can thereby learn how to collect status for your resources, especially custom resources. For example, a custom resource may have many status fields and only Karmada can collect only those you want. Refer to Customizing Resource Interpreter for more details. Integrating verification with the ecosystem Benefiting from the Kubernetes native APIs, Karmada can easily integrate the Kubernetes ecosystem. The following components are verified by the Karmada community: Kyverno : policy engine. Refer to working with kyverno for more details. Gatekeeper : another policy engine. Refer to working with gatekeeper for more details. fluxcd : GitOps tooling for helm chart. Refer to working with fluxcd for more details. Other Notable Changes Bug Fixes karmadactl: Fixed the cluster joining failures in the case of legacy secrets. ( #1306 ) karmadactl: Fixed the issue that you cannot use the '-v 6' log level. ( #1426 ) karmadactl: Fixed the issue that the --namespace flag of init command did not work. ( #1416 ) karmadactl: Allowed namespaces to be customized. ( #1449 ) karmadactl: Fixed the init failure due to data path not clean. ( #1455 ) karmadactl: Fixed the init failure to read the KUBECONFIG environment variable. ( #1437 ) karmadactl: Fixed the init command failure to select the default release version. ( #1456 ) karmadactl: Fixed the issue that the karmada-system namespace already exists when deploying karmada-agent. ( #1604 ) karmadactl: Fixed the issue that the karmada-controller-manager args did not honor customized namespaces.` ( #1683 ) karmadactl: Fixed a panic due to nil annotation when promoting resources to Karmada.` ( #1759 ) karmadactl: Fixed the promote command failure to migrate cluster-scoped resources. ( #1766 ) karmadactl: fixed the karmadactl taint failure while the karmada control plane config is not located in the default path. ( #1825 ) helm-chart: Fixed the karmada-agent installation failure due to the lack of permission. ( #1457 )) helm-chart: Fixed the issue that version constraints skip pre-releases. ( #1444 ) karmada-controller-manager: Fixed the issue that ResourceBinding may hinder en-queue in the case of schedule failures. ( #1499 ) karmada-controller-manager: Fixed the panic when the interpreter webhook returns nil patch. ( #1584 ) karmada-controller-manager: Fixed the RB/CRB controller failure to aggregate status in the case of work condition update. ( #1513 ) karmada-aggregate-apiserver: Fixed timeout issue when requesting cluster/proxy with options -w or logs -f from karmadactl get. ( #1620 ) karmada-aggregate-apiserver: Fixed exec failed: error: unable to upgrade connection: you must specify at least 1 of stdin, stdout, stderr. ( #1632 ) Features & Enhancements karmada-controller-manager: Introduced several flags to specify controller's concurrent capacities(--rate-limiter-base-delay, --rate-limiter-max-delay, --rate-limiter-qps, --rate-limiter-bucket-size). ( #1399 ) karmada-controller-manager: The klog flags now have been grouped for better readability. ( #1468 ) karmada-controller-manager: Fixed the FullyApplied condition of ResourceBinding/ClusterResourceBinding mislabeling issue in the case of non-scheduling. ( #1512 ) karmada-controller-manager: Added default AggregateStatus webhook for DaemonSet and StatefulSet. ( #1586 ) karmada-controller-manager: OverridePolicy with empty ResourceSelector will be considered to match all resources just like nil. ( #1706 ) karmada-controller-manager: Introduced --failover-eviction-timeout to specify the grace period of eviction. Tants(cluster.karmada.io/not-ready or cluster.karmada.io/unreachable) will be set on unhealthy clusters after the period. ( #1781 ) karmada-controller-manager/karmada-agent: Introduced --cluster-failure-threshold flag to specify cluster failure threshold. ( #1829 ) karmada-scheduler: Workloads can now be rescheduled after the cluster is unregistered. ( #1383 ) karmada-scheduler: The klog flags now have been grouped for better readability. ( #1491 ) karmada-scheduler: Added a scoring plugin ClusterLocality to favor clusters already requested. ( #1334 ) karmada-scheduler: Introduced filter plugin SpreadConstraint to filter clusters that do not meet the spread constraints. ( #1570 ) karmada-scheduler: Supported spread constraints by region strategy. ( #1646 ) karmada-webhook: Introduced --tls-cert-file-name and --tls-private-key-file-name flags to specify the server certificate and private key. ( #1464 ) karmada-agent: The klog flags now have been grouped for better readability. ( #1389 ) karmada-agent: Introduced several flags to specify controller's concurrent capacities(--rate-limiter-base-delay, --rate-limiter-max-delay, --rate-limiter-qps, --rate-limiter-bucket-size). ( #1505 ) karmada-scheduler-estimator: The klog flags now have been grouped for better readability. ( #1493 ) karmadactl: Introduced --context flag to specify the context name to use. ( #1748 ) karmadactl: Introduced --kube-image-mirror-country and --kube-image-registry flags to init subcommand for Chinese mainland users. ( #1764 ) karmadactl: Introduced deinit sub-command to uninstall Karmada. ( #1337 ) Introduced Swagger docs for Karmada API. ( #1401 ) Other (Dependencies) The base image alpine has been promoted to v3.15.1. ( #1519 ) Deprecation karmada-controller-manager: The hpa controller is disabled by default now. ( #1580 ) karmada-aggregated-apiserver: The deprecated flags --karmada-config and --master in v1.1 have been removed from the codebase. ( #1834 )","title":"CHANGELOG 1.2"},{"location":"CHANGELOG/CHANGELOG-1.2/#v120","text":"","title":"v1.2.0"},{"location":"CHANGELOG/CHANGELOG-1.2/#downloads-for-v120","text":"Download v1.2.0 in the v1.2.0 release page .","title":"Downloads for v1.2.0"},{"location":"CHANGELOG/CHANGELOG-1.2/#karmada-v12-release-notes","text":"","title":"Karmada v1.2 Release Notes"},{"location":"CHANGELOG/CHANGELOG-1.2/#whats-new","text":"","title":"What's New"},{"location":"CHANGELOG/CHANGELOG-1.2/#significant-improvement-on-scheduling-capability-and-scalability","text":"","title":"Significant improvement on scheduling capability and scalability"},{"location":"CHANGELOG/CHANGELOG-1.2/#1-karmada-descheduler","text":"A new component karmada-descheduler was introduced, for rebalancing the scheduling decisions over time. One example use case is: it helps evict pending replicas (Pods) from resource-starved clusters so that karmada-scheduler can \"reschedule\" these replicas (Pods) to a cluster with sufficient resources. For more details please refer to Descheduler user guide.","title":"1. Karmada Descheduler"},{"location":"CHANGELOG/CHANGELOG-1.2/#2-multi-region-ha-support","text":"By leveraging the newly added spread-by-region constraint, users are now able to deploy workloads across regions , e.g. people may want their workloads always running on different regions for HA purposes. We also introduced two plugins to karmada-scheduler , which add to accurate scheduling. ClusterLocality is a scoring plugin that favors clusters already assigned. SpreadConstraint is a filter plugin that filters clusters as per spread constraints. We are also in the progress of enhancing the multi-cluster failover mechanism. Part of the work has been included in this release. For example: A new flag( --cluster-failure-threshold ) has been added to both karmada-controller-manager and karmada-agent , which specifies the cluster failure threshold (defaults to 30s). A cluster will be considered not-ready only when it stays unhealthy longer than supposed. A new flag( --failover-eviction-timeout ) has been added to karmada-controller-manager , which specifies the grace period of eviction (defaults to 5 minutes). If a cluster stays not-ready longer than supposed, the controller taints the cluster. (Note: The taint is essentially the eviction order and the implementation is planned for the next release.)","title":"2. Multi region HA support"},{"location":"CHANGELOG/CHANGELOG-1.2/#fully-adopted-aggregated-api","text":"The Aggregated API was initially introduced in Release 1.0, which allows users to access clusters through Karmada by a single aggregated API endpoint. By leveraging this feature, we introduced a lot of interesting features to karmadactl and kubectl-karmada . The get sub-command now supports clusters both in push and pull mode. # karmadactl get deployment -n default NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member1 2/2 2 2 33h N nginx member2 1/1 1 1 4m38s Y podinfo member3 2/2 2 2 27h N The newly added logs command prints the container logs in a specific cluster. # ./karmadactl logs nginx-6799fc88d8-9mpxn -c nginx -C member1 /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf ... We also added watch and exec commands to karmadactl , in addition to get and logs . They all use the aggregated API.","title":"Fully adopted aggregated API"},{"location":"CHANGELOG/CHANGELOG-1.2/#distributed-search-and-analytics-engine-for-kubernetes-resources-alpha","text":"The newly introduced karmada-search caches resources in clusters and allows users to search for resources without directly touching real clusters. # kubectl get --raw /apis/search.karmada.io/v1alpha1/search/cache/apis/apps/v1/deployments { \"apiVersion\": \"v1\", \"kind\": \"List\", \"metadata\": {}, \"items\": [{ \"apiVersion\": \"apps/v1\", \"kind\": \"Deployment\", \"metadata\": { \"annotations\": { \"cluster.karmada.io/name\": \"member1\", }, } }, ] } The karmada-search also supports syncing cached resources to backend stores like Elasticsearch or OpenSearch . By leveraging the search engine, you can perform full-text searches with all desired features, by field, and by indice; rank results by score, sort results by field, and aggregate results.","title":"Distributed search and analytics engine for Kubernetes resources (alpha)"},{"location":"CHANGELOG/CHANGELOG-1.2/#resource-interpreter-webhook-enhancement","text":"Introduced InterpretStatus for the Resource Interpreter Webhook framework, which enables customized resource status collection. Karmada can thereby learn how to collect status for your resources, especially custom resources. For example, a custom resource may have many status fields and only Karmada can collect only those you want. Refer to Customizing Resource Interpreter for more details.","title":"Resource Interpreter Webhook enhancement"},{"location":"CHANGELOG/CHANGELOG-1.2/#integrating-verification-with-the-ecosystem","text":"Benefiting from the Kubernetes native APIs, Karmada can easily integrate the Kubernetes ecosystem. The following components are verified by the Karmada community: Kyverno : policy engine. Refer to working with kyverno for more details. Gatekeeper : another policy engine. Refer to working with gatekeeper for more details. fluxcd : GitOps tooling for helm chart. Refer to working with fluxcd for more details.","title":"Integrating verification with the ecosystem"},{"location":"CHANGELOG/CHANGELOG-1.2/#other-notable-changes","text":"","title":"Other Notable Changes"},{"location":"CHANGELOG/CHANGELOG-1.2/#bug-fixes","text":"karmadactl: Fixed the cluster joining failures in the case of legacy secrets. ( #1306 ) karmadactl: Fixed the issue that you cannot use the '-v 6' log level. ( #1426 ) karmadactl: Fixed the issue that the --namespace flag of init command did not work. ( #1416 ) karmadactl: Allowed namespaces to be customized. ( #1449 ) karmadactl: Fixed the init failure due to data path not clean. ( #1455 ) karmadactl: Fixed the init failure to read the KUBECONFIG environment variable. ( #1437 ) karmadactl: Fixed the init command failure to select the default release version. ( #1456 ) karmadactl: Fixed the issue that the karmada-system namespace already exists when deploying karmada-agent. ( #1604 ) karmadactl: Fixed the issue that the karmada-controller-manager args did not honor customized namespaces.` ( #1683 ) karmadactl: Fixed a panic due to nil annotation when promoting resources to Karmada.` ( #1759 ) karmadactl: Fixed the promote command failure to migrate cluster-scoped resources. ( #1766 ) karmadactl: fixed the karmadactl taint failure while the karmada control plane config is not located in the default path. ( #1825 ) helm-chart: Fixed the karmada-agent installation failure due to the lack of permission. ( #1457 )) helm-chart: Fixed the issue that version constraints skip pre-releases. ( #1444 ) karmada-controller-manager: Fixed the issue that ResourceBinding may hinder en-queue in the case of schedule failures. ( #1499 ) karmada-controller-manager: Fixed the panic when the interpreter webhook returns nil patch. ( #1584 ) karmada-controller-manager: Fixed the RB/CRB controller failure to aggregate status in the case of work condition update. ( #1513 ) karmada-aggregate-apiserver: Fixed timeout issue when requesting cluster/proxy with options -w or logs -f from karmadactl get. ( #1620 ) karmada-aggregate-apiserver: Fixed exec failed: error: unable to upgrade connection: you must specify at least 1 of stdin, stdout, stderr. ( #1632 )","title":"Bug Fixes"},{"location":"CHANGELOG/CHANGELOG-1.2/#features-enhancements","text":"karmada-controller-manager: Introduced several flags to specify controller's concurrent capacities(--rate-limiter-base-delay, --rate-limiter-max-delay, --rate-limiter-qps, --rate-limiter-bucket-size). ( #1399 ) karmada-controller-manager: The klog flags now have been grouped for better readability. ( #1468 ) karmada-controller-manager: Fixed the FullyApplied condition of ResourceBinding/ClusterResourceBinding mislabeling issue in the case of non-scheduling. ( #1512 ) karmada-controller-manager: Added default AggregateStatus webhook for DaemonSet and StatefulSet. ( #1586 ) karmada-controller-manager: OverridePolicy with empty ResourceSelector will be considered to match all resources just like nil. ( #1706 ) karmada-controller-manager: Introduced --failover-eviction-timeout to specify the grace period of eviction. Tants(cluster.karmada.io/not-ready or cluster.karmada.io/unreachable) will be set on unhealthy clusters after the period. ( #1781 ) karmada-controller-manager/karmada-agent: Introduced --cluster-failure-threshold flag to specify cluster failure threshold. ( #1829 ) karmada-scheduler: Workloads can now be rescheduled after the cluster is unregistered. ( #1383 ) karmada-scheduler: The klog flags now have been grouped for better readability. ( #1491 ) karmada-scheduler: Added a scoring plugin ClusterLocality to favor clusters already requested. ( #1334 ) karmada-scheduler: Introduced filter plugin SpreadConstraint to filter clusters that do not meet the spread constraints. ( #1570 ) karmada-scheduler: Supported spread constraints by region strategy. ( #1646 ) karmada-webhook: Introduced --tls-cert-file-name and --tls-private-key-file-name flags to specify the server certificate and private key. ( #1464 ) karmada-agent: The klog flags now have been grouped for better readability. ( #1389 ) karmada-agent: Introduced several flags to specify controller's concurrent capacities(--rate-limiter-base-delay, --rate-limiter-max-delay, --rate-limiter-qps, --rate-limiter-bucket-size). ( #1505 ) karmada-scheduler-estimator: The klog flags now have been grouped for better readability. ( #1493 ) karmadactl: Introduced --context flag to specify the context name to use. ( #1748 ) karmadactl: Introduced --kube-image-mirror-country and --kube-image-registry flags to init subcommand for Chinese mainland users. ( #1764 ) karmadactl: Introduced deinit sub-command to uninstall Karmada. ( #1337 ) Introduced Swagger docs for Karmada API. ( #1401 )","title":"Features &amp; Enhancements"},{"location":"CHANGELOG/CHANGELOG-1.2/#other-dependencies","text":"The base image alpine has been promoted to v3.15.1. ( #1519 )","title":"Other (Dependencies)"},{"location":"CHANGELOG/CHANGELOG-1.2/#deprecation","text":"karmada-controller-manager: The hpa controller is disabled by default now. ( #1580 ) karmada-aggregated-apiserver: The deprecated flags --karmada-config and --master in v1.1 have been removed from the codebase. ( #1834 )","title":"Deprecation"},{"location":"adoptions/vipkid-en/","text":"Table of Contents generated with DocToc VIPKID: Building a PaaS Platform with Karmada to Run Containers Background Born Multi-Cloud and Cross-Region Multi-Cluster Policy Cluster Disaster Recovery Challenges and Pain Points Running the Same Application in Different Clusters Quickly Migrating Applications upon Faults Why Karmada Any Solutions Available? Karmada, the Solution of Choice Karmada at VIPKID Containerization Based on Karmada Benefits Gains VIPKID: Building a PaaS Platform with Karmada to Run Containers Author: Ci Yiheng, Backend R&D Expert, VIPKID Background VIPKID is an online English education platform with more than 80,000 teachers and 1 million trainees. It has delivered 150 million training sessions across countries and regions. To provide better services, VIPKID deploys applications by region and close to teachers and trainees. Therefore, VIPKID purchased dozens of clusters from multiple cloud providers around the world to build its internal infrastructure. Born Multi-Cloud and Cross-Region VIPKID provides services internationally. Native speakers can be both teaching students in China and studying with Chinese teachers. To provide optimal online class experience, VIPKID sets up a low-latency network and deploys computing services close to teachers and trainees separately. Such deployment depends on resources from multiple public cloud vendors. Managing multi-cloud resources has long become a part of VIPKID's IaaS operations. Multi-Cluster Policy We first tried the single-cluster mode to containerize our platform, simple and low-cost. We dropped it after evaluating the network quality and infrastructure (network and storage) solutions across clouds and regions, and our project period. There are two major reasons: 1) Network latency and stability between clouds cannot be guaranteed. 2) Different vendors have different solutions for container networking and storage. Costs would be high if we wanted to resolve these problems. Finally, we decided to configure Kubernetes clusters by cloud vendor and region. That's why we have so many clusters. Cluster Disaster Recovery DR(Disaster Recovery) becomes easier for containers than VMs. Kubernetes provides DR solutions for pods and nodes, but not single clusters. Thanks to the microservice reconstruction, we can quickly create a cluster or scale an existing one to transfer computing services. Challenges and Pain Points Running the Same Application in Different Clusters During deployment, we found that the workloads of the same application vary greatly in different clusters in terms of images, startup parameters (configurations), and release versions. In the early stage, we wanted that our developers can directly manage applications on our own PaaS platform. However, the increasing customization made it more and more difficult to abstract the differences. We had to turn to our O&M team, but they also failed in some complex scenarios. This is not DevOps. It does not reduce costs or increase efficiency. Quickly Migrating Applications upon Faults Fault migration can be focused on applications or clusters. The application-centric approach focuses on the self-healing of key applications and the overall load in multi-cluster mode. The cluster-centric approach focuses on the disasters (such as network faults) that may impact all clusters or on the delivery requirements when creating new clusters. You need to set different policies for these approaches. Application-centric: Dynamic Migration Flexibly deploying an application in multiple clusters can ensure its stability. For example, if an instance in a cluster is faulty and cannot be quickly recovered, a new instance needs to be created automatically in another cluster of the same vendor or region based on the preset policy. Cluster-centric: Quick Cluster Startup Commonly, we start a new cluster to replace the unavailable one or to deliver services which depend on a specific cloud vendor or region. It would be best if clusters can be started as fast as pods. Why Karmada Any Solutions Available? Your service systems may evolve fast and draw clear lines for modules. To address the pain points, you need to, to some extent, abstract, decouple and reconstruct your systems. For us, service requirements were deeply coupled with cluster resources. We wanted to decouple them via multi-cluster management. Specifically, use the self-developed platform to manage the application lifecycle, and use a system to manage operation instructions on cluster resources. We probed into the open source communities to find products that support multi-cluster management. However, most products either serve as a platform like ours or manage resources by cluster. We wanted to manage multiple Kubernetes clusters like one single, large cluster. In this way, a workload can be regarded as an independent application (or a version of an application) instead of a replica of an application in multiple clusters. We also wanted to lower the access costs as much as possible. We surveyed and evaluated many solutions in the communities and decided on Karmada. Karmada, the Solution of Choice Karmada has the following advantages: 1) Karmada allows us to manage multiple clusters like one single cluster and manage resources in an application-centric approach. In addition, almost all configuration differences can be independently declared through the Override policies in Karmada, simple, intuitive, and easy to manage. 2) Karmada uses native Kubernetes APIs. We need no adaption and the access cost is low. Karmada also manifests configurations through CRDs. It dynamically turns distribution and differentiated configurations into Propagation and Override policies and delivers them to the Karmada control plane. 3) Karmada sits under the open governance of a neutral community. The community welcomes open discussions on requirements and ideas and we got technically improved while contributing to the community. Karmada at VIPKID Our platform caters to all container-based deployments, covering stateful or stateless applications, hybrid deployment of online and offline jobs, AI, and big data services. This platform does not rely on any public cloud. Therefore, we cannot use any encapsulated products of cloud vendors. We use the internal IaaS platform to create and scale out clusters, configure VPCs, subnets, and security groups of different vendors. In this way, vendor differences become the least of worries for our PaaS platform. In addition, we provide GitOps for developers to manage system applications and components. This is more user-friendly and efficient for skilled developers. Containerization Based on Karmada At the beginning, we designed a component (cluster aggregation API) in the platform to interact with Kubernetes clusters. We retained the native Kubernetes APIs and added some cluster-related information. However, there were complex problems during the implementation. For example, as the PaaS system needed to render declarations of different resources to multiple clusters, the applications we maintained in different clusters were irrelevant. We made much effort to solve these problems, even after CRDs were introduced. The system still needed to keep track of the details of each cluster, which goes against what cluster aggregation API is supposed to do. When there are a large number of clusters that go online and offline frequently, we need to change the configurations in batches for applications in the GitOps model to ensure normal cluster running. However, GitOps did not cope with the increasing complexity as expected. The following figure shows the differences before and after we used Karmada. After Karmada is introduced, the multi-cluster aggregation layer is truly unified. We can manage resources by application on the Karmada control plane. We only need to interact with Karmada, not the clusters, which simplifies containerized application management and enables our PaaS platform to fully focus on service requirements. With Karmada integrated into GitOps, system components can be easily released and upgraded in each cluster, exponentially more efficient than before. Benefits Managing Kubernetes resources by application simplifies the platform and greatly improves utilization. Here are the improvements brought by Karmada. 1) Higher deployment efficiency Before then, we needed to send deployment instructions to each cluster and monitor the deployment status, which required us to continuously check resources and handle exceptions. Now, application statuses are automatically collected and detected by Karmada. 2) Differentiated control on applications Adopting DevOps means developers can easily manage the lifecycle of applications. We leverage Karmada Override policies to directly interconnect with application profiles such as environment variables, startup parameters, and image repositories so that developers can better control the differences of applications in different clusters. 3) Quick cluster startup and adaptation to GitOps Basic services (system and common services) are configured for all clusters in Karmada Propagation policies and managed by Karmada when a new cluster is created. These basic services can be delivered along with the cluster, requiring no manual initialization and greatly shortening the delivery process. Most basic services are managed by the GitOps system, which is convenient and intuitive. 4) Short reconstruction period and no impact on services Thanks to the support of native Kubernetes APIs, we can quickly integrate Karmada into our platform. We use Karmada the way we use Kubernetes. The only thing we need to configure is Propagation policies, which can be customized by resource name, resource type, or LabelSelector. Gains Since February 2021, three of us have become contributors to the Karmada community. We witness the releases of Karmada from version 0.5.0 to 1.0.0. To write codes that satisfy all is challenging. We have learned a lot from the community during the practice, and we always welcome more of you to join us.","title":"Vipkid en"},{"location":"adoptions/vipkid-en/#vipkid-building-a-paas-platform-with-karmada-to-run-containers","text":"Author: Ci Yiheng, Backend R&D Expert, VIPKID","title":"VIPKID: Building a PaaS Platform with Karmada to Run Containers"},{"location":"adoptions/vipkid-en/#background","text":"VIPKID is an online English education platform with more than 80,000 teachers and 1 million trainees. It has delivered 150 million training sessions across countries and regions. To provide better services, VIPKID deploys applications by region and close to teachers and trainees. Therefore, VIPKID purchased dozens of clusters from multiple cloud providers around the world to build its internal infrastructure.","title":"Background"},{"location":"adoptions/vipkid-en/#born-multi-cloud-and-cross-region","text":"VIPKID provides services internationally. Native speakers can be both teaching students in China and studying with Chinese teachers. To provide optimal online class experience, VIPKID sets up a low-latency network and deploys computing services close to teachers and trainees separately. Such deployment depends on resources from multiple public cloud vendors. Managing multi-cloud resources has long become a part of VIPKID's IaaS operations.","title":"Born Multi-Cloud and Cross-Region"},{"location":"adoptions/vipkid-en/#multi-cluster-policy","text":"We first tried the single-cluster mode to containerize our platform, simple and low-cost. We dropped it after evaluating the network quality and infrastructure (network and storage) solutions across clouds and regions, and our project period. There are two major reasons: 1) Network latency and stability between clouds cannot be guaranteed. 2) Different vendors have different solutions for container networking and storage. Costs would be high if we wanted to resolve these problems. Finally, we decided to configure Kubernetes clusters by cloud vendor and region. That's why we have so many clusters.","title":"Multi-Cluster Policy"},{"location":"adoptions/vipkid-en/#cluster-disaster-recovery","text":"DR(Disaster Recovery) becomes easier for containers than VMs. Kubernetes provides DR solutions for pods and nodes, but not single clusters. Thanks to the microservice reconstruction, we can quickly create a cluster or scale an existing one to transfer computing services.","title":"Cluster Disaster Recovery"},{"location":"adoptions/vipkid-en/#challenges-and-pain-points","text":"","title":"Challenges and Pain Points"},{"location":"adoptions/vipkid-en/#running-the-same-application-in-different-clusters","text":"During deployment, we found that the workloads of the same application vary greatly in different clusters in terms of images, startup parameters (configurations), and release versions. In the early stage, we wanted that our developers can directly manage applications on our own PaaS platform. However, the increasing customization made it more and more difficult to abstract the differences. We had to turn to our O&M team, but they also failed in some complex scenarios. This is not DevOps. It does not reduce costs or increase efficiency.","title":"Running the Same Application in Different Clusters"},{"location":"adoptions/vipkid-en/#quickly-migrating-applications-upon-faults","text":"Fault migration can be focused on applications or clusters. The application-centric approach focuses on the self-healing of key applications and the overall load in multi-cluster mode. The cluster-centric approach focuses on the disasters (such as network faults) that may impact all clusters or on the delivery requirements when creating new clusters. You need to set different policies for these approaches. Application-centric: Dynamic Migration Flexibly deploying an application in multiple clusters can ensure its stability. For example, if an instance in a cluster is faulty and cannot be quickly recovered, a new instance needs to be created automatically in another cluster of the same vendor or region based on the preset policy. Cluster-centric: Quick Cluster Startup Commonly, we start a new cluster to replace the unavailable one or to deliver services which depend on a specific cloud vendor or region. It would be best if clusters can be started as fast as pods.","title":"Quickly Migrating Applications upon Faults"},{"location":"adoptions/vipkid-en/#why-karmada","text":"","title":"Why Karmada"},{"location":"adoptions/vipkid-en/#any-solutions-available","text":"Your service systems may evolve fast and draw clear lines for modules. To address the pain points, you need to, to some extent, abstract, decouple and reconstruct your systems. For us, service requirements were deeply coupled with cluster resources. We wanted to decouple them via multi-cluster management. Specifically, use the self-developed platform to manage the application lifecycle, and use a system to manage operation instructions on cluster resources. We probed into the open source communities to find products that support multi-cluster management. However, most products either serve as a platform like ours or manage resources by cluster. We wanted to manage multiple Kubernetes clusters like one single, large cluster. In this way, a workload can be regarded as an independent application (or a version of an application) instead of a replica of an application in multiple clusters. We also wanted to lower the access costs as much as possible. We surveyed and evaluated many solutions in the communities and decided on Karmada.","title":"Any Solutions Available?"},{"location":"adoptions/vipkid-en/#karmada-the-solution-of-choice","text":"Karmada has the following advantages: 1) Karmada allows us to manage multiple clusters like one single cluster and manage resources in an application-centric approach. In addition, almost all configuration differences can be independently declared through the Override policies in Karmada, simple, intuitive, and easy to manage. 2) Karmada uses native Kubernetes APIs. We need no adaption and the access cost is low. Karmada also manifests configurations through CRDs. It dynamically turns distribution and differentiated configurations into Propagation and Override policies and delivers them to the Karmada control plane. 3) Karmada sits under the open governance of a neutral community. The community welcomes open discussions on requirements and ideas and we got technically improved while contributing to the community.","title":"Karmada, the Solution of Choice"},{"location":"adoptions/vipkid-en/#karmada-at-vipkid","text":"Our platform caters to all container-based deployments, covering stateful or stateless applications, hybrid deployment of online and offline jobs, AI, and big data services. This platform does not rely on any public cloud. Therefore, we cannot use any encapsulated products of cloud vendors. We use the internal IaaS platform to create and scale out clusters, configure VPCs, subnets, and security groups of different vendors. In this way, vendor differences become the least of worries for our PaaS platform. In addition, we provide GitOps for developers to manage system applications and components. This is more user-friendly and efficient for skilled developers.","title":"Karmada at VIPKID"},{"location":"adoptions/vipkid-en/#containerization-based-on-karmada","text":"At the beginning, we designed a component (cluster aggregation API) in the platform to interact with Kubernetes clusters. We retained the native Kubernetes APIs and added some cluster-related information. However, there were complex problems during the implementation. For example, as the PaaS system needed to render declarations of different resources to multiple clusters, the applications we maintained in different clusters were irrelevant. We made much effort to solve these problems, even after CRDs were introduced. The system still needed to keep track of the details of each cluster, which goes against what cluster aggregation API is supposed to do. When there are a large number of clusters that go online and offline frequently, we need to change the configurations in batches for applications in the GitOps model to ensure normal cluster running. However, GitOps did not cope with the increasing complexity as expected. The following figure shows the differences before and after we used Karmada. After Karmada is introduced, the multi-cluster aggregation layer is truly unified. We can manage resources by application on the Karmada control plane. We only need to interact with Karmada, not the clusters, which simplifies containerized application management and enables our PaaS platform to fully focus on service requirements. With Karmada integrated into GitOps, system components can be easily released and upgraded in each cluster, exponentially more efficient than before.","title":"Containerization Based on Karmada"},{"location":"adoptions/vipkid-en/#benefits","text":"Managing Kubernetes resources by application simplifies the platform and greatly improves utilization. Here are the improvements brought by Karmada. 1) Higher deployment efficiency Before then, we needed to send deployment instructions to each cluster and monitor the deployment status, which required us to continuously check resources and handle exceptions. Now, application statuses are automatically collected and detected by Karmada. 2) Differentiated control on applications Adopting DevOps means developers can easily manage the lifecycle of applications. We leverage Karmada Override policies to directly interconnect with application profiles such as environment variables, startup parameters, and image repositories so that developers can better control the differences of applications in different clusters. 3) Quick cluster startup and adaptation to GitOps Basic services (system and common services) are configured for all clusters in Karmada Propagation policies and managed by Karmada when a new cluster is created. These basic services can be delivered along with the cluster, requiring no manual initialization and greatly shortening the delivery process. Most basic services are managed by the GitOps system, which is convenient and intuitive. 4) Short reconstruction period and no impact on services Thanks to the support of native Kubernetes APIs, we can quickly integrate Karmada into our platform. We use Karmada the way we use Kubernetes. The only thing we need to configure is Propagation policies, which can be customized by resource name, resource type, or LabelSelector.","title":"Benefits"},{"location":"adoptions/vipkid-en/#gains","text":"Since February 2021, three of us have become contributors to the Karmada community. We witness the releases of Karmada from version 0.5.0 to 1.0.0. To write codes that satisfy all is challenging. We have learned a lot from the community during the practice, and we always welcome more of you to join us.","title":"Gains"},{"location":"adoptions/vipkid-zh/","text":"VIPKID\u57fa\u4e8eKarmada\u7684\u5bb9\u5668PaaS\u5e73\u53f0\u843d\u5730\u5b9e\u8df5 \u672c\u7bc7\u6587\u7ae0\u6765\u81ea\u5728\u7ebf\u6559\u80b2\u5e73\u53f0VIPKID\u5728\u5bb9\u5668\u4f53\u7cfb\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u843d\u5730\u5b9e\u8df5\uff0c\u4eceVIPKID\u7684\u4e1a\u52a1\u80cc\u666f\u3001\u4e1a\u52a1\u6311\u6218\u3001\u9009\u578b\u8fc7\u7a0b\u3001\u5f15\u5165Karmada\u524d\u540e\u7684\u5bf9\u6bd4\u4ee5\u53ca\u6536\u76ca\u7b49\u65b9\u9762\u6df1\u5165\u5256\u6790\u4e86VIPKID\u5bb9\u5668\u5316\u6539\u9020\u8fc7\u7a0b\u3002 \u4e1a\u52a1\u80cc\u666f VIPKID\u7684\u4e1a\u52a1\u8986\u76d6\u6570\u5341\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\uff0c\u7b7e\u7ea6\u6559\u5e08\u6570\u91cf\u8d85\u8fc78\u4e07\u540d\uff0c\u4e3a\u5168\u7403100\u4e07\u5b66\u5458\u63d0\u4f9b\u6559\u80b2\u670d\u52a1\uff0c\u7d2f\u8ba1\u6388\u8bfe1.5\u4ebf\u8282\u3002\u4e3a\u4e86\u66f4\u597d\u7684\u4e3a\u6559\u5e08\u548c\u5b66\u5458\u63d0\u4f9b\u670d\u52a1\uff0c\u76f8\u5173\u5e94\u7528\u4f1a\u6309\u5730\u57df\u5c31\u8fd1\u90e8\u7f72\uff0c\u6bd4\u5982\u9762\u5411\u6559\u5e08\u7684\u670d\u52a1\u4f1a\u8d34\u8fd1\u6559\u5e08\u6240\u5728\u7684\u5730\u57df\u90e8\u7f72\uff0c\u9762\u5411\u5b66\u5458\u7684\u670d\u52a1\u76f4\u63a5\u90e8\u7f72\u5728\u5b66\u5458\u4fa7\u3002\u4e3a\u6b64\uff0cVIPKID\u4ece\u5168\u7403\u591a\u4e2a\u4e91\u4f9b\u5e94\u5546\u91c7\u8d2d\u4e86\u6570\u5341\u4e2a\u96c6\u7fa4\u7528\u4e8e\u6784\u5efaVIPKID\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u3002 \u65e0\u6cd5\u8eb2\u5f00\u7684\u591a\u4e91\uff0c\u591aRegion VIPKID\u7684\u4e1a\u52a1\u5f62\u6001\u662f\u5c06\u56fd\u5185\u5916\u7684\u6559\u80b2\u8d44\u6e90\u8fdb\u884c\u4e92\u6362\u4e92\u8865\uff0c\u5305\u62ec\u8fc7\u53bb\u7684\u5317\u7f8e\u5916\u6559\u8d44\u6e90\u5f15\u5165\u5230\u56fd\u5185\u548c\u5c06\u73b0\u5728\u56fd\u5185\u7684\u6559\u80b2\u670d\u52a1\u63d0\u4f9b\u5230\u6d77\u5916\u3002 \u4e3a\u4e86\u8ffd\u6c42\u4f18\u8d28\u7684\u4e0a\u8bfe\u4f53\u9a8c\uff0c\u9664\u4e86\u7ec4\u5efa\u4e86\u4e00\u4e2a\u7a33\u5b9a\u4f4e\u5ef6\u8fdf\u7684\u4e92\u901a\u94fe\u8def\u5916\uff0c\u4e0d\u540c\u5f62\u6001\u7684\u8ba1\u7b97\u670d\u52a1\u90fd\u8981\u5c31\u8fd1\u90e8\u7f72\uff0c\u6bd4\u5982\u6559\u5e08\u5ba2\u6237\u7aef\u7684\u4f9d\u8d56\u670d\u52a1\uff0c\u8981\u4f18\u5148\u90e8\u7f72\u5728\u6d77\u5916\uff0c\u800c\u5bb6\u957f\u5ba2\u6237\u7aef\u7684\u4f9d\u8d56\u670d\u52a1\u5219\u9996\u9009\u56fd\u5185\u3002 \u56e0\u6b64VIPKID\u4f7f\u7528\u4e86\u56fd\u5185\u5916\u591a\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u7f51\u7edc\u8d44\u6e90\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u5bf9\u591a\u4e91\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u5f88\u65e9\u5c31\u6210\u4e3a\u4e86VIPKID\u7684IaaS\u7ba1\u7406\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u3002 K8s\u591a\u96c6\u7fa4\u7b56\u7565 \u5728VIPKID\u5bb9\u5668\u4f53\u7cfb\u8bbe\u8ba1\u4e4b\u521d\uff0c\u6211\u4eec\u9996\u9009\u7684\u65b9\u6848\u662f\u5355\u96c6\u7fa4\u6a21\u5f0f\uff0c\u8be5\u65b9\u6848\u7684\u4f18\u52bf\u662f\u7ed3\u6784\u7b80\u5355\u4e14\u7ba1\u7406\u6210\u672c\u4f4e\u3002\u4f46\u7efc\u5408\u8bc4\u4f30\u591a\u4e91\u4e4b\u95f4\u548c\u591aRegion\u7684\u7f51\u7edc\u8d28\u91cf\u4e0e\u57fa\u7840\u8bbe\u65bd\uff08\u7f51\u7edc\u548c\u5b58\u50a8\uff09\u65b9\u6848\uff0c\u5e76\u7efc\u5408\u6211\u4eec\u7684\u9879\u76ee\u5468\u671f\uff0c\u53ea\u80fd\u653e\u5f03\u8fd9\u4e2a\u60f3\u6cd5\u3002\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u70b9\uff1a 1\uff09\u4e0d\u540c\u4e91\u4e4b\u95f4\u7684\u7f51\u7edc\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\u65e0\u6cd5\u4fdd\u8bc1 2\uff09\u5404\u5bb6\u4e91\u5382\u5546\u7684\u5bb9\u5668\u7f51\u7edc\u548c\u5b58\u50a8\u65b9\u6848\u5747\u6709\u5dee\u5f02 \u82e5\u8981\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\uff0c\u9700\u8981\u8017\u8d39\u8f83\u9ad8\u7684\u6210\u672c\u3002\u6700\u540e\uff0c\u6211\u4eec\u6309\u7167\u4e91\u5382\u5546\u548cRegion\u7684\u7ef4\u5ea6\u53bb\u914d\u7f6eK8s\u96c6\u7fa4\uff0c\u5982\u6b64\u5c31\u62e5\u6709\u5f88\u591aK8s\u96c6\u7fa4\u3002 \u96c6\u7fa4\u7684\u5bb9\u707e \u5bb9\u707e\u5bf9\u4e8e\u5bb9\u5668\u800c\u8a00\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684VM\u8d44\u6e90\u5df2\u7ecf\u53cb\u597d\u5f97\u591a\u3002K8s\u89e3\u51b3\u4e86\u5927\u90e8\u5206Pod\u548cNode\u7ea7\u522b\u7684Case\uff0c\u4f46\u5355\u4e2a\u96c6\u7fa4\u7684\u707e\u96be\u5904\u7406\uff0c\u8fd8\u9700\u8981\u6211\u4eec\u81ea\u884c\u89e3\u51b3\uff0c\u7531\u4e8eVIPKID\u65e9\u671f\u5df2\u7ecf\u5b8c\u6210\u4e86\u5fae\u670d\u52a1\u5316\u7684\u6539\u9020\uff0c\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u5feb\u901f\u521b\u5efa\u65b0\u96c6\u7fa4\u6216\u8005\u6269\u5bb9\u73b0\u6709\u7279\u5b9a\u96c6\u7fa4\u7684\u65b9\u5f0f\u6765\u5feb\u901f\u8fdb\u884c\u8ba1\u7b97\u670d\u52a1\u7684\u8f6c\u79fb\u3002 \u4e1a\u52a1\u6311\u6218\u53ca\u75db\u70b9 \u5982\u4f55\u5bf9\u5f85\u4e0d\u540c\u96c6\u7fa4\u4e2d\u7684\u540c\u4e00\u4e2a\u5e94\u7528\uff1f \u5728\u591a\u4e91\u90e8\u7f72\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u9047\u5230\u4e86\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u95ee\u9898\uff1a\u540c\u4e00\u4e2a\u5e94\u7528\u5728\u4e0d\u540c\u96c6\u7fa4\u4e2d\u7684workload\u51e0\u4e4e\u90fd\u662f\u4e0d\u540c\u7684\uff0c\u6bd4\u5982\u4f7f\u7528\u7684\u955c\u50cf\u3001\u542f\u52a8\u53c2\u6570\uff08\u914d\u7f6e\uff09\u751a\u81f3\u6709\u65f6\u5019\u8fde\u53d1\u5e03\u7248\u672c\u90fd\u4e0d\u4e00\u6837\u3002\u524d\u671f\u6211\u4eec\u662f\u4f9d\u8d56\u81ea\u5df1\u5bb9\u5668PaaS\u5e73\u53f0\u6765\u7ba1\u7406\u8fd9\u4e9b\u5dee\u5f02\uff0c\u4f46\u968f\u7740\u5dee\u5f02\u9700\u6c42\u589e\u591a\uff0c\u573a\u666f\u4e5f\u8d8a\u6765\u8d8a\u591a\uff0c\u5dee\u5f02\u62bd\u8c61\u8d8a\u53d1\u56f0\u96be\u3002 \u6211\u4eec\u7684\u521d\u8877\u662f\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u76f4\u63a5\u5728\u6211\u4eec\u7684PaaS\u5e73\u53f0\u4e0a\u7ba1\u7406\u5176\u5e94\u7528\uff0c\u4f46\u56e0\u4e3a\u590d\u6742\u7684\u5dee\u5f02\u8d8a\u6765\u8d8a\u96be\u4ee5\u7ba1\u7406\uff0c\u6700\u7ec8\u53ea\u80fd\u4f9d\u8d56\u8fd0\u7ef4\u540c\u5b66\u534f\u52a9\u64cd\u4f5c\u3002\u53e6\u5916\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u573a\u666f\u4e0b\uff0c\u8fd0\u7ef4\u540c\u5b66\u4e5f\u96be\u4ee5\u5feb\u901f\u3001\u51c6\u786e\u7684\u8fdb\u884c\u7ba1\u7406\uff0c\u5982\u6b64\u504f\u79bb\u4e86DevOps\u7406\u5ff5\uff0c\u4e0d\u4ec5\u589e\u52a0\u4e86\u7ba1\u7406\u6210\u672c\uff0c\u800c\u4e14\u964d\u4f4e\u4e86\u4f7f\u7528\u6548\u7387\u3002 \u5982\u4f55\u5feb\u901f\u5b8c\u6210\u6545\u969c\u8fc1\u79fb\uff1f \u5bf9\u4e8e\u6545\u969c\u8fc1\u79fb\uff0c\u8fd9\u91cc\u6211\u4ece\u5e94\u7528\u548c\u96c6\u7fa4\u4e24\u4e2a\u4e0d\u540c\u89c6\u89d2\u6765\u63cf\u8ff0\uff0c\u5e94\u7528\u89c6\u89d2\u770b\u91cd\u5173\u952e\u5e94\u7528\u7684\u81ea\u6108\u80fd\u529b\u4ee5\u53ca\u80fd\u5426\u5728\u591a\u96c6\u7fa4\u72b6\u6001\u4e0b\u4fdd\u969c\u6574\u4f53\u7684\u8d1f\u8f7d\u80fd\u529b\u3002\u800c\u96c6\u7fa4\u7ef4\u5ea6\u5219\u66f4\u770b\u91cd\u96c6\u7fa4\u6574\u4f53\u7ea7\u522b\u7684\u707e\u96be\u6216\u5bf9\u65b0\u96c6\u7fa4\u7684\u4ea4\u4ed8\u9700\u6c42\uff0c\u6bd4\u5982\u7f51\u7edc\u6545\u969c\uff0c\u6b64\u65f6\u5e94\u5bf9\u7b56\u7565\u4f1a\u6709\u4e0d\u540c\u3002 \u5e94\u7528\u89c6\u89d2\uff1a\u5e94\u7528\u7684\u52a8\u6001\u8fc1\u79fb \u4ece\u5e94\u7528\u51fa\u53d1\uff0c\u4fdd\u969c\u5173\u952e\u5e94\u7528\u7684\u7a33\u5b9a\u6027\uff0c\u53ef\u4ee5\u7075\u6d3b\u7684\u8c03\u6574\u5e94\u7528\u5728\u591a\u96c6\u7fa4\u4e2d\u7684\u90e8\u7f72\u60c5\u51b5\u3002\u4f8b\u5982\u67d0\u5173\u952e\u5e94\u7528\u5728A\u96c6\u7fa4\u7684\u5b9e\u4f8b\u51fa\u73b0\u6545\u969c\u4e14\u65e0\u6cd5\u5feb\u901f\u6062\u590d\uff0c\u90a3\u5c31\u9700\u8981\u6839\u636e\u4e8b\u5148\u5236\u5b9a\u7684\u7b56\u7565\uff0c\u5728\u540c\u5382\u5546\u6216\u540cRegion\u4e0b\u7684\u96c6\u7fa4\u4e2d\u521b\u5efa\u5b9e\u4f8b\uff0c\u5e76\u4e14\u8fd9\u4e00\u5207\u5e94\u8be5\u662f\u81ea\u52a8\u7684\u3002 \u96c6\u7fa4\u89c6\u89d2\uff1a\u65b0\u96c6\u7fa4\u5982\u4f55\u5feb\u901fready \u65b0\u96c6\u7fa4\u7684\u521b\u5efa\u5728\u6211\u4eec\u7684\u4e1a\u52a1\u573a\u666f\u5f88\u5e38\u89c1\u3002\u6bd4\u5982\u5f53\u67d0\u4e2aK8s\u96c6\u7fa4\u4e0d\u53ef\u7528\u65f6\uff0c\u6211\u4eec\u7684\u671f\u671b\u662f\u901a\u8fc7\u62c9\u8d77\u65b0\u96c6\u7fa4\u7684\u65b9\u5f0f\u8fdb\u884c\u5feb\u901f\u4fee\u590d\uff0c\u518d\u5982\uff0c\u4e1a\u52a1\u5bf9\u65b0\u7684\u4e91\u5382\u5546\u6216\u8005Region\u6709\u9700\u6c42\u65f6\u5019\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u80fd\u591f\u5feb\u901f\u4ea4\u4ed8\u96c6\u7fa4\u8d44\u6e90\u3002\u6211\u4eec\u5e0c\u671b\u4ed6\u80fd\u591f\u50cf\u542f\u52a8Pod\u4e00\u6837\u8fc5\u901f\u3002 Why Karmada \u4e0d\u81ea\u5df1\u9020\u8f6e\u5b50\uff0c\u7740\u773c\u5f00\u6e90\u793e\u533a \u4e0a\u8ff0\u5217\u4e3e\u7684\u75db\u70b9\uff0c\u82e5\u53ea\u8bd5\u56fe\u6ee1\u8db3\u6682\u65f6\u7684\u9700\u6c42\u662f\u8fdc\u8fdc\u4e0d\u591f\u7684\uff0c\u7cfb\u7edf\u5728\u5feb\u901f\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u5fc5\u987b\u8981\u9002\u5f53\u7684\u8fdb\u884c\u62bd\u8c61\u548c\u89e3\u8026\uff0c\u5e76\u4e14\u968f\u7740\u7cfb\u7edf\u7ec4\u6210\u6a21\u5757\u7684\u89d2\u8272\u5206\u5de5\u9010\u6e10\u6e05\u6670\uff0c\u4e5f\u9700\u8981\u9002\u5f53\u7684\u91cd\u6784\u3002 \u5bf9\u4e8e\u6211\u4eec\u7684\u5bb9\u5668PaaS\u5e73\u53f0\u800c\u8a00\uff0c\u4e1a\u52a1\u9700\u6c42\u4e0e\u96c6\u7fa4\u8d44\u6e90\u8026\u5408\u8d8a\u53d1\u4e25\u91cd\uff0c\u6211\u4eec\u5c06\u89e3\u8026\u7684\u5207\u9762\u753b\u5728\u4e86\u591a\u96c6\u7fa4\u7684\u7ba1\u7406\u4e0a\uff0c\u7531\u6211\u4eec\u81ea\u7814\u7684\u5e73\u53f0\u7ba1\u7406\u5e94\u7528\u7684\u751f\u547d\u5468\u671f\uff0c\u53e6\u5916\u4e00\u4e2a\u7cfb\u7edf\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u7684\u64cd\u4f5c\u6307\u4ee4\u3002 \u660e\u786e\u9700\u6c42\u540e\uff0c\u6211\u4eec\u5c31\u5f00\u59cb\u5728\u5f00\u6e90\u793e\u533a\u5bfb\u627e\u4e0e\u8c03\u7814\u8fd9\u7c7b\u4ea7\u54c1\uff0c\u4f46\u627e\u5230\u7684\u5f00\u6e90\u4ea7\u54c1\u90fd\u662f\u5e73\u53f0\u5c42\u7684\uff0c\u4e5f\u5c31\u662f\u4e0e\u6211\u4eec\u81ea\u7814\u5e73\u53f0\u89e3\u51b3\u601d\u8def\u7c7b\u4f3c\uff0c\u5e76\u4e14\u5927\u591a\u662f\u4ee5\u96c6\u7fa4\u89c6\u89d2\u6765\u8fdb\u884c\u64cd\u4f5c\u7684\uff0c\u6240\u6709\u8d44\u6e90\u9996\u5148\u5728\u96c6\u7fa4\u7684\u7ef4\u5ea6\u4e0a\u5c31\u88ab\u5272\u88c2\u5f00\u4e86\uff0c\u5e76\u4e0d\u7b26\u5408\u6211\u4eec\u5bf9\u5e94\u7528\u89c6\u89d2\u7684\u8bc9\u6c42\u3002 \u4ee5\u5e94\u7528\u4e3a\u89c6\u89d2\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u5c06\u591a\u4e2aK8s\u96c6\u7fa4\u4f5c\u4e3a\u4e00\u4e2a\u5927\u578b\u96c6\u7fa4\u6765\u7ba1\u7406\uff0c\u8fd9\u6837\u4e00\u4e2aworkload\u5c31\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a\u5e94\u7528\uff08\u6216\u4e00\u4e2a\u5e94\u7528\u7684\u67d0\u4e2a\u7248\u672c\uff09\u800c\u4e0d\u662f\u6563\u843d\u5728\u591a\u4e2a\u96c6\u7fa4\u4e2d\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u591a\u4e2aworkload\u3002 \u53e6\u5916\u4e00\u4e2a\u539f\u5219\u662f\u5c3d\u91cf\u4f4e\u7684\u63a5\u5165\u6210\u672c\u3002\u6211\u4eec\u8c03\u7814\u4e86\u5f00\u6e90\u793e\u533a\u7684\u591a\u79cd\u65b9\u6848\uff0c\u7efc\u5408\u8bc4\u4f30\u540e\uff0c\u53d1\u73b0Karmada\u6bd4\u8f83\u7b26\u5408\u6211\u4eec\u7684\u9700\u6c42\u3002 \u5c31\u5b83\u4e86\uff0c Karmada\uff01 \u8bd5\u7528Karmada\u540e\uff0c\u53d1\u73b0\u6709\u4ee5\u4e0b\u51e0\u65b9\u9762\u4f18\u52bf\uff1a 1\uff09Karmada\u771f\u6b63\u610f\u4e49\u7684\u5b9e\u73b0\u4e86\u4ee5\u4e00\u4e2aK8s\u96c6\u7fa4\u89c6\u89d2\u6765\u7ba1\u7406\u591a\u96c6\u7fa4\u7684\u80fd\u529b \uff0c\u8ba9\u6211\u4eec\u80fd\u591f\u4ee5\u5e94\u7528\u89c6\u89d2\u7ba1\u7406\u591a\u96c6\u7fa4\u4e2d\u7684\u8d44\u6e90\u3002\u53e6\u5916\uff0cKarmada\u7684OverridePolicy\u8bbe\u8ba1\u51e0\u4e4e\u6240\u6709\u5dee\u5f02\u90fd\u53ef\u4ee5\u5355\u72ec\u58f0\u660e\u51fa\u6765\uff0c\u7b80\u5355\u76f4\u89c2\u4e14\u4fbf\u4e8e\u7ba1\u7406\uff0c\u8fd9\u4e0e\u6211\u4eec\u5185\u90e8\u5bf9\u5e94\u7528\u753b\u50cf\u5728\u4e0d\u540c\u96c6\u7fa4\u4e4b\u95f4\u7684\u5e94\u7528\u5dee\u5f02\u4e0d\u8c0b\u800c\u5408\u3002 2\uff09Karmada\u5b8c\u5168\u4f7f\u7528\u4e86K8s\u539f\u751fAPI \uff0c\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u50cf\u539f\u6765\u4e00\u6837\u4f7f\u7528\uff0c\u540c\u65f6\u4e5f\u8868\u660e\u6211\u4eec\u5728\u540e\u7eed\u7684\u63a5\u5165\u6210\u672c\u4f1a\u5f88\u4f4e\u3002\u5e76\u4e14Karmada\u7684CRD\u76f8\u5bf9\u6765\u8bb2\u4e5f\u66f4\u5bb9\u6613\u7406\u89e3\uff0c\u6211\u4eec\u5e73\u53f0\u7684\u670d\u52a1\u753b\u50cf\u6a21\u5757\u53ef\u4ee5\u5f88\u5bb9\u6613\u7684\u5c06\u5206\u53d1\u548c\u5dee\u5f02\u914d\u7f6e\u52a8\u6001\u6e32\u67d3\u6210Propagation\u548cOverride\u7b56\u7565\uff0c\u4e0b\u53d1\u7ed9Karmada\u63a7\u5236\u9762\u3002 3\uff09\u5f00\u6e90\u5f00\u653e\u7684\u793e\u533a\u6cbb\u7406\u6a21\u5f0f \uff0c\u4e5f\u662f\u6211\u4eec\u56e2\u961f\u6700\u770b\u91cd\u7684\u4e00\u70b9\u3002\u5728\u8bd5\u7528Karmada\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u8bba\u662f\u6211\u4eec\u81ea\u5df1\u8fd8\u662f\u793e\u533a\u65b9\u9762\u5bf9\u9700\u6c42\u7684\u7406\u89e3\u548c\u8bbe\u60f3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u90fd\u53ef\u4ee5\u5728\u793e\u533a\u4e2d\u5f00\u653e\u8ba8\u8bba\u3002\u540c\u65f6\uff0c\u5728\u53c2\u4e0e\u4ee3\u7801\u8d21\u732e\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u56e2\u961f\u6574\u4f53\u6280\u672f\u80fd\u529b\u4e5f\u663e\u8457\u63d0\u5347\u3002 Karmada at VIPKID \u6211\u4eec\u7684\u5bb9\u5668\u5e73\u53f0\uff0c\u627f\u8f7d\u4e86\u6574\u4e2a\u516c\u53f8\u6240\u6709\u7684\u5bb9\u5668\u5316\u90e8\u7f72\u8bc9\u6c42\uff0c\u5305\u62ec\u6709\u65e0\u72b6\u6001\u3001\u5728\u79bb\u7ebf\u4e1a\u52a1\u548cAI\u5927\u6570\u636e\u7b49\u3002\u5e76\u4e14\u8981\u6c42PaaS\u5e73\u53f0\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e0d\u4f1a\u5bf9\u67d0\u4e00\u5bb6\u516c\u6709\u4e91\u4ea7\u751f\u4efb\u4f55\u4f9d\u8d56\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u4f7f\u7528\u4e91\u5382\u5546\u5c01\u88c5\u8fc7\u7684\u4e00\u4e9b\u4ea7\u54c1\u3002 \u6211\u4eec\u4f1a\u4f9d\u8d56\u5185\u90e8\u7684IaaS\u5e73\u53f0\u53bb\u7ba1\u7406\u591a\u5bb6\u4e91\u5382\u5546\u7684\u5404\u7c7b\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ecK8s\u96c6\u7fa4\u7684\u521b\u5efa\u3001\u6269\u5bb9\u3001VPC\uff0c\u5b50\u7f51\u4ee5\u53ca\u5b89\u5168\u7ec4\u7684\u914d\u7f6e\u3002\u8fd9\u4e2a\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u8ba9\u6211\u4eec\u53ef\u4ee5\u6807\u51c6\u5316\u591a\u4e2a\u4e91\u5382\u5546\u7684K8s\u96c6\u7fa4\uff0c\u8ba9\u4e0a\u5c42PaaS\u5e73\u53f0\u51e0\u4e4e\u65e0\u9700\u5173\u5fc3\u5382\u5546\u7ea7\u522b\u7684\u5dee\u5f02\u3002 \u53e6\u5916\uff0c\u5bf9\u4e8e\u7cfb\u7edf\u7ea7\u522b\u7684\u5e94\u7528\u548c\u7ec4\u4ef6\uff0c\u6211\u4eec\u4e3a\u5f00\u53d1\u8005\u521b\u5efa\u4e86\u53e6\u5916\u4e00\u6761\u7ba1\u7406\u6e20\u9053\uff0c\u90a3\u5c31\u662f\u4f7f\u7528GitOps\u3002\u8fd9\u5bf9\u9ad8\u9636\u5f00\u53d1\u8005\u6765\u8bf4\u8981\u66f4\u52a0\u53cb\u597d\uff0c\u5bf9\u7cfb\u7edf\u5e94\u7528\u7684\u7ec4\u4ef6\u5b89\u88c5\u90e8\u7f72\u66f4\u4e3a\u9ad8\u6548\u3002 \u57fa\u4e8eKarmada\u7684\u5bb9\u5668\u5316\u6539\u9020\u65b9\u6848 \u5728\u5e73\u53f0\u843d\u5730\u4e4b\u521d\uff0c\u6211\u4eec\u5355\u72ec\u5265\u79bb\u4e86\u4e00\u4e2a\u7ec4\u4ef6\uff08\u4e0a\u56fe\u5de6\u4fa7\u7684\u201c\u96c6\u7fa4\u6c47\u805aAPI\u201d\uff09\uff0c\u4e13\u95e8\u548cK8s\u96c6\u7fa4\u8fdb\u884c\u4ea4\u4e92\uff0c\u5e76\u4e14\u5411\u4e0a\u4fdd\u7559K8s\u539f\u751fAPI\uff0c\u4e5f\u4f1a\u9644\u52a0\u4e00\u4e9b\u548c\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f\u3002 \u4f46\u5728\u843d\u5730\u8fc7\u7a0b\u4e2d\uff0c\u201c\u5bb9\u5668\u5e94\u7528\u7ba1\u7406\u201d\u7cfb\u7edf\u9700\u8981\u8fdb\u884c\u8bb8\u591a\u64cd\u4f5c\u9002\u914d\u591a\u96c6\u7fa4\u4e0b\u7684\u590d\u6742\u60c5\u51b5\u3002\u6bd4\u5982\u867d\u7136PaaS\u7cfb\u7edf\u770b\u8d77\u6765\u662f\u4e00\u4e2a\u5e94\u7528\uff0c\u4f46\u7cfb\u7edf\u9700\u8981\u6e32\u67d3\u4e0d\u540c\u7684\u5b8c\u6574\u8d44\u6e90\u58f0\u660e\u5230\u4e0d\u540c\u7684\u96c6\u7fa4\uff0c\u4f7f\u5f97\u6211\u4eec\u5728\u771f\u6b63\u7ef4\u62a4\u591a\u96c6\u7fa4\u5e94\u7528\u65f6\u4ecd\u7136\u662f\u4e0d\u76f8\u5173\u7684\u3001\u5272\u88c2\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u529e\u6cd5\u5728\u5e95\u5c42\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u8bf8\u5982\u6b64\u7c7b\u95ee\u9898\u7684\u89e3\u51b3\u8fd8\u662f\u5360\u7528\u4e86\u56e2\u961f\u4e0d\u5c11\u7684\u8d44\u6e90\uff0c\u5c24\u5176\u662f\u5f15\u5165CRD\u8d44\u6e90\u540e\uff0c\u8fd8\u9700\u8981\u91cd\u590d\u7684\u89e3\u51b3\u8fd9\u65b9\u9762\u7684\u95ee\u9898\u3002\u5e76\u4e14\u8fd9\u4e2a\u7cfb\u7edf\u65e0\u6cd5\u4e0d\u53bb\u5173\u5fc3\u6bcf\u4e2a\u96c6\u7fa4\u91cc\u9762\u7684\u7ec6\u8282\u72b6\u51b5\uff0c\u5982\u6b64\u80cc\u79bb\u4e86\u6211\u4eec\u8bbe\u8ba1\u201c\u96c6\u7fa4\u6c47\u805aAPI\u201d\u7ec4\u4ef6\u7684\u521d\u8877\u3002 \u53e6\u5916\uff0c\u7531\u4e8eGitOps\u4e5f\u9700\u8981\u4e0e\u96c6\u7fa4\u5f3a\u76f8\u5173\uff0c\u5728\u96c6\u7fa4\u6570\u91cf\u8f83\u5927\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f34\u968f\u96c6\u7fa4\u4e0a\u4e0b\u7ebf\u7684\u60c5\u51b5\u4e0b\uff0c\u6b64\u65f6\uff0c\u82e5\u8981\u6b63\u5e38\u8fd0\u8f6c\u5c31\u9700\u8981\u5bf9GitOps\u7684\u5e94\u7528\u914d\u7f6e\u8fdb\u884c\u6279\u91cf\u53d8\u66f4\uff0c\u968f\u4e4b\u589e\u52a0\u7684\u590d\u6742\u5ea6\uff0c\u8ba9\u6574\u4f53\u6548\u679c\u5e76\u672a\u8fbe\u5230\u9884\u671f\u3002 \u4e0b\u56fe\u662fVIPKID\u5f15\u5165Karmada\u4e4b\u524d\u548c\u4e4b\u540e\u7684\u67b6\u6784\u5bf9\u6bd4\uff1a \u5f15\u5165Karmada\u540e\uff0c\u591a\u96c6\u7fa4\u805a\u5408\u5c42\u5f97\u4ee5\u771f\u6b63\u7684\u7edf\u4e00 \uff0c\u6211\u4eec\u53ef\u4ee5\u5728Karmada\u63a7\u5236\u5e73\u9762\u4ee5\u5e94\u7528\u7ef4\u5ea6\u53bb\u7ba1\u7406\u8d44\u6e90\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u90fd\u4e0d\u9700\u8981\u6df1\u5165\u5230\u53d7\u63a7\u96c6\u7fa4\u4e2d\uff0c\u53ea\u9700\u8981\u4e0eKarmada\u4ea4\u4e92\u5373\u53ef\u3002\u5982\u6b64\u6781\u5927\u7684\u7b80\u5316\u4e86\u6211\u4eec\u7684\u201c\u5bb9\u5668\u5e94\u7528\u7ba1\u7406\u201d\u7cfb\u7edf\u3002\u73b0\u5728\uff0c\u6211\u4eec\u7684PaaS\u5e73\u53f0\u53ef\u4ee5\u5b8c\u5168\u503e\u6ce8\u4e8e\u4e1a\u52a1\u9700\u6c42\uff0cKarmada\u5f3a\u5927\u7684\u80fd\u529b\u5df2\u7ecf\u6ee1\u8db3\u4e86\u5f53\u524d\u6211\u4eec\u7684\u5404\u7c7b\u9700\u6c42\u3002 \u800cGitOps\u4f53\u7cfb\u4f7f\u7528Karmada\u540e\uff0c\u7cfb\u7edf\u7ea7\u7ec4\u4ef6\u4e5f\u53ef\u4ee5\u7b80\u5355\u7684\u5728\u5404\u4e2a\u96c6\u7fa4\u4e2d\u8fdb\u884c\u53d1\u5e03\u548c\u5347\u7ea7\uff0c\u4e0d\u4ec5\u8ba9\u6211\u4eec\u4f53\u9a8c\u5230\u4e86GitOps\u672c\u8eab\u7684\u4fbf\u5229\uff0c\u66f4\u662f\u8ba9\u6211\u4eec\u6536\u83b7\u5230\u4e86GitOps*Karmada\u7684\u6210\u500d\u7ea7\u7684\u6548\u7387\u63d0\u5347\u3002 \u6536\u76ca \u4ee5\u5e94\u7528\u4e3a\u7ef4\u5ea6\u6765\u7ba1\u7406K8s\u8d44\u6e90\uff0c\u964d\u4f4e\u4e86\u5e73\u53f0\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4f7f\u7528\u6548\u7387\u3002\u4e0b\u9762\u4ee5\u6211\u4eecPaaS\u5e73\u53f0\u7279\u6027\u5165\u624b\uff0c\u6765\u63cf\u8ff0\u5f15\u5165Karmada\u540e\u7684\u6539\u53d8\u3002 1\uff09\u591a\u96c6\u7fa4\u5e94\u7528\u7684\u90e8\u7f72\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff1a \u5148\u524d\u5728\u90e8\u7f72\u65f6\u9700\u8981\u5411\u6bcf\u4e2a\u96c6\u7fa4\u53d1\u9001\u90e8\u7f72\u6307\u4ee4\uff0c\u968f\u4e4b\u76d1\u6d4b\u90e8\u7f72\u72b6\u6001\u662f\u5426\u5f02\u5e38\u3002\u5982\u6b64\u5c31\u9700\u8981\u4e0d\u65ad\u7684\u68c0\u67e5\u591a\u4e2a\u96c6\u7fa4\u4e2d\u7684\u8d44\u6e90\u72b6\u6001\uff0c\u7136\u540e\u518d\u6839\u636e\u5f02\u5e38\u60c5\u51b5\u505a\u4e0d\u540c\u7684\u5904\u7406\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u903b\u8f91\u7e41\u7410\u5e76\u4e14\u7f13\u6162\u3002\u5f15\u5165Karmada\u540e\uff0cKarmada\u4f1a\u81ea\u52a8\u6536\u96c6\u548c\u6c47\u805a\u5e94\u7528\u5728\u5404\u4e2a\u96c6\u7fa4\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Karmada\u6765\u611f\u77e5\u5e94\u7528\u72b6\u6001\u3002 2\uff09\u5e94\u7528\u7684\u5dee\u5f02\u63a7\u5236\u53ef\u5f00\u653e\u7ed9\u5f00\u53d1\u8005\uff1a DevOps\u6587\u5316\u6700\u91cd\u8981\u7684\u4e00\u70b9\u5c31\u662f\u5f00\u53d1\u8005\u8981\u80fd\u591f\u5b8c\u5168\u53c2\u4e0e\u8fdb\u6765\uff0c\u80fd\u591f\u4fbf\u6377\u5730\u5bf9\u5e94\u7528\u5168\u751f\u547d\u5468\u671f\u8fdb\u884c\u7ba1\u7406\u3002\u6211\u4eec\u5145\u5206\u5229\u7528\u4e86Karmada\u7684Override\u7b56\u7565\uff0c\u76f4\u63a5\u4e0e\u5e94\u7528\u753b\u50cf\u5bf9\u63a5\uff0c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u6e05\u6670\u7684\u4e86\u89e3\u548c\u63a7\u5236\u5e94\u7528\u5728\u4e0d\u540c\u96c6\u7fa4\u7684\u5dee\u5f02\uff0c\u73b0\u5df2\u652f\u6301\u73af\u5883\u53d8\u91cf\uff0c\u542f\u52a8\u53c2\u6570\uff0c\u955c\u50cf\u4ed3\u5e93\u3002 3\uff09\u96c6\u7fa4\u7684\u5feb\u901f\u62c9\u8d77&\u5bf9GitOps\u9002\u914d\uff1a \u6211\u4eec\u5c06\u57fa\u7840\u670d\u52a1\uff08\u7cfb\u7edf\u7ea7\u548c\u901a\u7528\u7c7b\u578b\u670d\u52a1\uff09\u5728Karmada\u7684Propagation\u8bbe\u5b9a\u4e3a\u5168\u91cf\u96c6\u7fa4\uff0c\u5728\u65b0\u96c6\u7fa4\u521b\u5efa\u597d\u4ee5\u540e\uff0c\u76f4\u63a5\u52a0\u5165\u5230Karmada\u4e2d\u8fdb\u884c\u7eb3\u7ba1\uff0c\u8fd9\u4e9b\u57fa\u7840\u670d\u52a1\u53ef\u4ee5\u4f34\u968f\u96c6\u7fa4\u4ea4\u4ed8\u4e00\u5e76\u4ea4\u4ed8\uff0c\u8282\u7701\u4e86\u6211\u4eec\u5bf9\u96c6\u7fa4\u505a\u57fa\u7840\u670d\u52a1\u521d\u59cb\u5316\u7684\u64cd\u4f5c\uff0c\u5927\u5927\u7f29\u77ed\u4e86\u4ea4\u4ed8\u73af\u8282\u548c\u65f6\u95f4\u3002\u5e76\u4e14\u5927\u90e8\u5206\u57fa\u7840\u670d\u52a1\u90fd\u662f\u7531\u6211\u4eec\u7684GitOps\u4f53\u7cfb\u7ba1\u7406\u7684\uff0c\u76f8\u6bd4\u8fc7\u53bb\u4e00\u4e2a\u4e2a\u96c6\u7fa4\u7684\u914d\u7f6e\u6765\u8bb2\uff0c\u65e2\u65b9\u4fbf\u53c8\u76f4\u89c2\u3002 4\uff09\u5e73\u53f0\u6539\u9020\u5468\u671f\u77ed\uff0c\u4e1a\u52a1\u65e0\u611f\u77e5\uff1a \u5f97\u76ca\u4e8eKarmada\u7684\u539f\u751fK8s API\uff0c\u6211\u4eec\u82b1\u4e86\u5f88\u5c11\u7684\u65f6\u95f4\u5728\u63a5\u5165Karmada\u4e0a\u3002Karmada\u771f\u6b63\u505a\u5230\u4e86\u539f\u6765\u600e\u4e48\u7528K8s\u73b0\u5728\u7ee7\u7eed\u600e\u4e48\u7528\u5c31\u53ef\u4ee5\u4e86\u3002\u552f\u4e00\u9700\u8981\u8003\u8651\u7684\u662fPropagation\u7b56\u7565\u7684\u5b9a\u5236\uff0c\u53ef\u4ee5\u6309\u7167\u8d44\u6e90\u540d\u5b57\u7684\u7ef4\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u6309\u7167\u8d44\u6e90\u7c7b\u578b\u6216LabelSelector\u7684\u7ef4\u5ea6\u6765\u58f0\u660e\uff0c\u6781\u5176\u65b9\u4fbf\u3002 \u53c2\u4e0e\u5f00\u6e90\u9879\u76ee\u7684\u6536\u83b7 \u4ece2021\u5e74\u76842\u6708\u4efd\u63a5\u89e6\u5230Karmada\u9879\u76ee\u4ee5\u6765\uff0c\u6211\u4eec\u56e2\u961f\u5148\u540e\u67093\u4eba\u6210\u4e3a\u4e86Karmada\u793e\u533a\u7684Contributor\uff0c\u4ece0.5.0\u52301.0.0\u7248\u672c\uff0c\u53c2\u4e0e\u548c\u89c1\u8bc1\u4e86\u591a\u4e2afeature\u7684\u53d1\u5e03\u3002\u540c\u65f6Karmada\u4e5f\u89c1\u8bc1\u4e86\u6211\u4eec\u56e2\u961f\u7684\u6210\u957f\u3002 \u628a\u81ea\u5df1\u7684\u9700\u6c42\u5199\u6210\u4ee3\u7801\u5f88\u7b80\u5355\uff0c\u628a\u81ea\u5df1\u7684\u9700\u6c42\u548c\u5176\u4ed6\u4eba\u8ba8\u8bba\uff0c\u5bf9\u6240\u6709\u4eba\u7684\u9700\u6c42\u8fdb\u884c\u5708\u5b9a\u548c\u53d6\u820d\uff0c\u9009\u62e9\u4e00\u4e2a\u7b26\u5408\u5927\u5bb6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u518d\u8f6c\u6362\u6210\u4ee3\u7801\uff0c\u96be\u5ea6\u5219\u4f1a\u5347\u7ea7\u3002\u6211\u4eec\u56e2\u961f\u5728\u6b64\u671f\u95f4\u6536\u83b7\u4e86\u5f88\u591a\uff0c\u4e5f\u6210\u957f\u4e86\u8bb8\u591a\uff0c\u5e76\u4e14\u4e3a\u80fd\u591f\u53c2\u4e0eKarmada\u9879\u76ee\u5efa\u8bbe\u800c\u611f\u5230\u81ea\u8c6a\uff0c\u5e0c\u671b\u66f4\u591a\u7684\u5f00\u53d1\u8005\u80fd\u52a0\u5165Karmada\u793e\u533a\uff0c\u4e00\u8d77\u8ba9\u793e\u533a\u751f\u6001\u66f4\u52a0\u7e41\u8363\uff01","title":"VIPKID\u57fa\u4e8eKarmada\u7684\u5bb9\u5668PaaS\u5e73\u53f0\u843d\u5730\u5b9e\u8df5"},{"location":"adoptions/vipkid-zh/#vipkidkarmadapaas","text":"\u672c\u7bc7\u6587\u7ae0\u6765\u81ea\u5728\u7ebf\u6559\u80b2\u5e73\u53f0VIPKID\u5728\u5bb9\u5668\u4f53\u7cfb\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u843d\u5730\u5b9e\u8df5\uff0c\u4eceVIPKID\u7684\u4e1a\u52a1\u80cc\u666f\u3001\u4e1a\u52a1\u6311\u6218\u3001\u9009\u578b\u8fc7\u7a0b\u3001\u5f15\u5165Karmada\u524d\u540e\u7684\u5bf9\u6bd4\u4ee5\u53ca\u6536\u76ca\u7b49\u65b9\u9762\u6df1\u5165\u5256\u6790\u4e86VIPKID\u5bb9\u5668\u5316\u6539\u9020\u8fc7\u7a0b\u3002","title":"VIPKID\u57fa\u4e8eKarmada\u7684\u5bb9\u5668PaaS\u5e73\u53f0\u843d\u5730\u5b9e\u8df5"},{"location":"adoptions/vipkid-zh/#_1","text":"VIPKID\u7684\u4e1a\u52a1\u8986\u76d6\u6570\u5341\u4e2a\u56fd\u5bb6\u548c\u5730\u533a\uff0c\u7b7e\u7ea6\u6559\u5e08\u6570\u91cf\u8d85\u8fc78\u4e07\u540d\uff0c\u4e3a\u5168\u7403100\u4e07\u5b66\u5458\u63d0\u4f9b\u6559\u80b2\u670d\u52a1\uff0c\u7d2f\u8ba1\u6388\u8bfe1.5\u4ebf\u8282\u3002\u4e3a\u4e86\u66f4\u597d\u7684\u4e3a\u6559\u5e08\u548c\u5b66\u5458\u63d0\u4f9b\u670d\u52a1\uff0c\u76f8\u5173\u5e94\u7528\u4f1a\u6309\u5730\u57df\u5c31\u8fd1\u90e8\u7f72\uff0c\u6bd4\u5982\u9762\u5411\u6559\u5e08\u7684\u670d\u52a1\u4f1a\u8d34\u8fd1\u6559\u5e08\u6240\u5728\u7684\u5730\u57df\u90e8\u7f72\uff0c\u9762\u5411\u5b66\u5458\u7684\u670d\u52a1\u76f4\u63a5\u90e8\u7f72\u5728\u5b66\u5458\u4fa7\u3002\u4e3a\u6b64\uff0cVIPKID\u4ece\u5168\u7403\u591a\u4e2a\u4e91\u4f9b\u5e94\u5546\u91c7\u8d2d\u4e86\u6570\u5341\u4e2a\u96c6\u7fa4\u7528\u4e8e\u6784\u5efaVIPKID\u5185\u90e8\u57fa\u7840\u8bbe\u65bd\u3002","title":"\u4e1a\u52a1\u80cc\u666f"},{"location":"adoptions/vipkid-zh/#region","text":"VIPKID\u7684\u4e1a\u52a1\u5f62\u6001\u662f\u5c06\u56fd\u5185\u5916\u7684\u6559\u80b2\u8d44\u6e90\u8fdb\u884c\u4e92\u6362\u4e92\u8865\uff0c\u5305\u62ec\u8fc7\u53bb\u7684\u5317\u7f8e\u5916\u6559\u8d44\u6e90\u5f15\u5165\u5230\u56fd\u5185\u548c\u5c06\u73b0\u5728\u56fd\u5185\u7684\u6559\u80b2\u670d\u52a1\u63d0\u4f9b\u5230\u6d77\u5916\u3002 \u4e3a\u4e86\u8ffd\u6c42\u4f18\u8d28\u7684\u4e0a\u8bfe\u4f53\u9a8c\uff0c\u9664\u4e86\u7ec4\u5efa\u4e86\u4e00\u4e2a\u7a33\u5b9a\u4f4e\u5ef6\u8fdf\u7684\u4e92\u901a\u94fe\u8def\u5916\uff0c\u4e0d\u540c\u5f62\u6001\u7684\u8ba1\u7b97\u670d\u52a1\u90fd\u8981\u5c31\u8fd1\u90e8\u7f72\uff0c\u6bd4\u5982\u6559\u5e08\u5ba2\u6237\u7aef\u7684\u4f9d\u8d56\u670d\u52a1\uff0c\u8981\u4f18\u5148\u90e8\u7f72\u5728\u6d77\u5916\uff0c\u800c\u5bb6\u957f\u5ba2\u6237\u7aef\u7684\u4f9d\u8d56\u670d\u52a1\u5219\u9996\u9009\u56fd\u5185\u3002 \u56e0\u6b64VIPKID\u4f7f\u7528\u4e86\u56fd\u5185\u5916\u591a\u4e2a\u516c\u6709\u4e91\u5382\u5546\u7684\u7f51\u7edc\u8d44\u6e90\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u5bf9\u591a\u4e91\u7684\u8d44\u6e90\u7ba1\u7406\uff0c\u5f88\u65e9\u5c31\u6210\u4e3a\u4e86VIPKID\u7684IaaS\u7ba1\u7406\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u3002","title":"\u65e0\u6cd5\u8eb2\u5f00\u7684\u591a\u4e91\uff0c\u591aRegion"},{"location":"adoptions/vipkid-zh/#k8s","text":"\u5728VIPKID\u5bb9\u5668\u4f53\u7cfb\u8bbe\u8ba1\u4e4b\u521d\uff0c\u6211\u4eec\u9996\u9009\u7684\u65b9\u6848\u662f\u5355\u96c6\u7fa4\u6a21\u5f0f\uff0c\u8be5\u65b9\u6848\u7684\u4f18\u52bf\u662f\u7ed3\u6784\u7b80\u5355\u4e14\u7ba1\u7406\u6210\u672c\u4f4e\u3002\u4f46\u7efc\u5408\u8bc4\u4f30\u591a\u4e91\u4e4b\u95f4\u548c\u591aRegion\u7684\u7f51\u7edc\u8d28\u91cf\u4e0e\u57fa\u7840\u8bbe\u65bd\uff08\u7f51\u7edc\u548c\u5b58\u50a8\uff09\u65b9\u6848\uff0c\u5e76\u7efc\u5408\u6211\u4eec\u7684\u9879\u76ee\u5468\u671f\uff0c\u53ea\u80fd\u653e\u5f03\u8fd9\u4e2a\u60f3\u6cd5\u3002\u4e3b\u8981\u539f\u56e0\u6709\u4e24\u70b9\uff1a 1\uff09\u4e0d\u540c\u4e91\u4e4b\u95f4\u7684\u7f51\u7edc\u5ef6\u8fdf\u548c\u7a33\u5b9a\u6027\u65e0\u6cd5\u4fdd\u8bc1 2\uff09\u5404\u5bb6\u4e91\u5382\u5546\u7684\u5bb9\u5668\u7f51\u7edc\u548c\u5b58\u50a8\u65b9\u6848\u5747\u6709\u5dee\u5f02 \u82e5\u8981\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\uff0c\u9700\u8981\u8017\u8d39\u8f83\u9ad8\u7684\u6210\u672c\u3002\u6700\u540e\uff0c\u6211\u4eec\u6309\u7167\u4e91\u5382\u5546\u548cRegion\u7684\u7ef4\u5ea6\u53bb\u914d\u7f6eK8s\u96c6\u7fa4\uff0c\u5982\u6b64\u5c31\u62e5\u6709\u5f88\u591aK8s\u96c6\u7fa4\u3002","title":"K8s\u591a\u96c6\u7fa4\u7b56\u7565"},{"location":"adoptions/vipkid-zh/#_2","text":"\u5bb9\u707e\u5bf9\u4e8e\u5bb9\u5668\u800c\u8a00\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684VM\u8d44\u6e90\u5df2\u7ecf\u53cb\u597d\u5f97\u591a\u3002K8s\u89e3\u51b3\u4e86\u5927\u90e8\u5206Pod\u548cNode\u7ea7\u522b\u7684Case\uff0c\u4f46\u5355\u4e2a\u96c6\u7fa4\u7684\u707e\u96be\u5904\u7406\uff0c\u8fd8\u9700\u8981\u6211\u4eec\u81ea\u884c\u89e3\u51b3\uff0c\u7531\u4e8eVIPKID\u65e9\u671f\u5df2\u7ecf\u5b8c\u6210\u4e86\u5fae\u670d\u52a1\u5316\u7684\u6539\u9020\uff0c\u56e0\u6b64\u53ef\u4ee5\u5229\u7528\u5feb\u901f\u521b\u5efa\u65b0\u96c6\u7fa4\u6216\u8005\u6269\u5bb9\u73b0\u6709\u7279\u5b9a\u96c6\u7fa4\u7684\u65b9\u5f0f\u6765\u5feb\u901f\u8fdb\u884c\u8ba1\u7b97\u670d\u52a1\u7684\u8f6c\u79fb\u3002","title":"\u96c6\u7fa4\u7684\u5bb9\u707e"},{"location":"adoptions/vipkid-zh/#_3","text":"","title":"\u4e1a\u52a1\u6311\u6218\u53ca\u75db\u70b9"},{"location":"adoptions/vipkid-zh/#_4","text":"\u5728\u591a\u4e91\u90e8\u7f72\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u9047\u5230\u4e86\u4e00\u4e2a\u5f88\u590d\u6742\u7684\u95ee\u9898\uff1a\u540c\u4e00\u4e2a\u5e94\u7528\u5728\u4e0d\u540c\u96c6\u7fa4\u4e2d\u7684workload\u51e0\u4e4e\u90fd\u662f\u4e0d\u540c\u7684\uff0c\u6bd4\u5982\u4f7f\u7528\u7684\u955c\u50cf\u3001\u542f\u52a8\u53c2\u6570\uff08\u914d\u7f6e\uff09\u751a\u81f3\u6709\u65f6\u5019\u8fde\u53d1\u5e03\u7248\u672c\u90fd\u4e0d\u4e00\u6837\u3002\u524d\u671f\u6211\u4eec\u662f\u4f9d\u8d56\u81ea\u5df1\u5bb9\u5668PaaS\u5e73\u53f0\u6765\u7ba1\u7406\u8fd9\u4e9b\u5dee\u5f02\uff0c\u4f46\u968f\u7740\u5dee\u5f02\u9700\u6c42\u589e\u591a\uff0c\u573a\u666f\u4e5f\u8d8a\u6765\u8d8a\u591a\uff0c\u5dee\u5f02\u62bd\u8c61\u8d8a\u53d1\u56f0\u96be\u3002 \u6211\u4eec\u7684\u521d\u8877\u662f\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u76f4\u63a5\u5728\u6211\u4eec\u7684PaaS\u5e73\u53f0\u4e0a\u7ba1\u7406\u5176\u5e94\u7528\uff0c\u4f46\u56e0\u4e3a\u590d\u6742\u7684\u5dee\u5f02\u8d8a\u6765\u8d8a\u96be\u4ee5\u7ba1\u7406\uff0c\u6700\u7ec8\u53ea\u80fd\u4f9d\u8d56\u8fd0\u7ef4\u540c\u5b66\u534f\u52a9\u64cd\u4f5c\u3002\u53e6\u5916\uff0c\u5728\u67d0\u4e9b\u590d\u6742\u573a\u666f\u4e0b\uff0c\u8fd0\u7ef4\u540c\u5b66\u4e5f\u96be\u4ee5\u5feb\u901f\u3001\u51c6\u786e\u7684\u8fdb\u884c\u7ba1\u7406\uff0c\u5982\u6b64\u504f\u79bb\u4e86DevOps\u7406\u5ff5\uff0c\u4e0d\u4ec5\u589e\u52a0\u4e86\u7ba1\u7406\u6210\u672c\uff0c\u800c\u4e14\u964d\u4f4e\u4e86\u4f7f\u7528\u6548\u7387\u3002","title":"\u5982\u4f55\u5bf9\u5f85\u4e0d\u540c\u96c6\u7fa4\u4e2d\u7684\u540c\u4e00\u4e2a\u5e94\u7528\uff1f"},{"location":"adoptions/vipkid-zh/#_5","text":"\u5bf9\u4e8e\u6545\u969c\u8fc1\u79fb\uff0c\u8fd9\u91cc\u6211\u4ece\u5e94\u7528\u548c\u96c6\u7fa4\u4e24\u4e2a\u4e0d\u540c\u89c6\u89d2\u6765\u63cf\u8ff0\uff0c\u5e94\u7528\u89c6\u89d2\u770b\u91cd\u5173\u952e\u5e94\u7528\u7684\u81ea\u6108\u80fd\u529b\u4ee5\u53ca\u80fd\u5426\u5728\u591a\u96c6\u7fa4\u72b6\u6001\u4e0b\u4fdd\u969c\u6574\u4f53\u7684\u8d1f\u8f7d\u80fd\u529b\u3002\u800c\u96c6\u7fa4\u7ef4\u5ea6\u5219\u66f4\u770b\u91cd\u96c6\u7fa4\u6574\u4f53\u7ea7\u522b\u7684\u707e\u96be\u6216\u5bf9\u65b0\u96c6\u7fa4\u7684\u4ea4\u4ed8\u9700\u6c42\uff0c\u6bd4\u5982\u7f51\u7edc\u6545\u969c\uff0c\u6b64\u65f6\u5e94\u5bf9\u7b56\u7565\u4f1a\u6709\u4e0d\u540c\u3002 \u5e94\u7528\u89c6\u89d2\uff1a\u5e94\u7528\u7684\u52a8\u6001\u8fc1\u79fb \u4ece\u5e94\u7528\u51fa\u53d1\uff0c\u4fdd\u969c\u5173\u952e\u5e94\u7528\u7684\u7a33\u5b9a\u6027\uff0c\u53ef\u4ee5\u7075\u6d3b\u7684\u8c03\u6574\u5e94\u7528\u5728\u591a\u96c6\u7fa4\u4e2d\u7684\u90e8\u7f72\u60c5\u51b5\u3002\u4f8b\u5982\u67d0\u5173\u952e\u5e94\u7528\u5728A\u96c6\u7fa4\u7684\u5b9e\u4f8b\u51fa\u73b0\u6545\u969c\u4e14\u65e0\u6cd5\u5feb\u901f\u6062\u590d\uff0c\u90a3\u5c31\u9700\u8981\u6839\u636e\u4e8b\u5148\u5236\u5b9a\u7684\u7b56\u7565\uff0c\u5728\u540c\u5382\u5546\u6216\u540cRegion\u4e0b\u7684\u96c6\u7fa4\u4e2d\u521b\u5efa\u5b9e\u4f8b\uff0c\u5e76\u4e14\u8fd9\u4e00\u5207\u5e94\u8be5\u662f\u81ea\u52a8\u7684\u3002 \u96c6\u7fa4\u89c6\u89d2\uff1a\u65b0\u96c6\u7fa4\u5982\u4f55\u5feb\u901fready \u65b0\u96c6\u7fa4\u7684\u521b\u5efa\u5728\u6211\u4eec\u7684\u4e1a\u52a1\u573a\u666f\u5f88\u5e38\u89c1\u3002\u6bd4\u5982\u5f53\u67d0\u4e2aK8s\u96c6\u7fa4\u4e0d\u53ef\u7528\u65f6\uff0c\u6211\u4eec\u7684\u671f\u671b\u662f\u901a\u8fc7\u62c9\u8d77\u65b0\u96c6\u7fa4\u7684\u65b9\u5f0f\u8fdb\u884c\u5feb\u901f\u4fee\u590d\uff0c\u518d\u5982\uff0c\u4e1a\u52a1\u5bf9\u65b0\u7684\u4e91\u5382\u5546\u6216\u8005Region\u6709\u9700\u6c42\u65f6\u5019\uff0c\u6211\u4eec\u4e5f\u9700\u8981\u80fd\u591f\u5feb\u901f\u4ea4\u4ed8\u96c6\u7fa4\u8d44\u6e90\u3002\u6211\u4eec\u5e0c\u671b\u4ed6\u80fd\u591f\u50cf\u542f\u52a8Pod\u4e00\u6837\u8fc5\u901f\u3002","title":"\u5982\u4f55\u5feb\u901f\u5b8c\u6210\u6545\u969c\u8fc1\u79fb\uff1f"},{"location":"adoptions/vipkid-zh/#why-karmada","text":"","title":"Why Karmada"},{"location":"adoptions/vipkid-zh/#_6","text":"\u4e0a\u8ff0\u5217\u4e3e\u7684\u75db\u70b9\uff0c\u82e5\u53ea\u8bd5\u56fe\u6ee1\u8db3\u6682\u65f6\u7684\u9700\u6c42\u662f\u8fdc\u8fdc\u4e0d\u591f\u7684\uff0c\u7cfb\u7edf\u5728\u5feb\u901f\u53d1\u5c55\u8fc7\u7a0b\u4e2d\u5fc5\u987b\u8981\u9002\u5f53\u7684\u8fdb\u884c\u62bd\u8c61\u548c\u89e3\u8026\uff0c\u5e76\u4e14\u968f\u7740\u7cfb\u7edf\u7ec4\u6210\u6a21\u5757\u7684\u89d2\u8272\u5206\u5de5\u9010\u6e10\u6e05\u6670\uff0c\u4e5f\u9700\u8981\u9002\u5f53\u7684\u91cd\u6784\u3002 \u5bf9\u4e8e\u6211\u4eec\u7684\u5bb9\u5668PaaS\u5e73\u53f0\u800c\u8a00\uff0c\u4e1a\u52a1\u9700\u6c42\u4e0e\u96c6\u7fa4\u8d44\u6e90\u8026\u5408\u8d8a\u53d1\u4e25\u91cd\uff0c\u6211\u4eec\u5c06\u89e3\u8026\u7684\u5207\u9762\u753b\u5728\u4e86\u591a\u96c6\u7fa4\u7684\u7ba1\u7406\u4e0a\uff0c\u7531\u6211\u4eec\u81ea\u7814\u7684\u5e73\u53f0\u7ba1\u7406\u5e94\u7528\u7684\u751f\u547d\u5468\u671f\uff0c\u53e6\u5916\u4e00\u4e2a\u7cfb\u7edf\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u7684\u64cd\u4f5c\u6307\u4ee4\u3002 \u660e\u786e\u9700\u6c42\u540e\uff0c\u6211\u4eec\u5c31\u5f00\u59cb\u5728\u5f00\u6e90\u793e\u533a\u5bfb\u627e\u4e0e\u8c03\u7814\u8fd9\u7c7b\u4ea7\u54c1\uff0c\u4f46\u627e\u5230\u7684\u5f00\u6e90\u4ea7\u54c1\u90fd\u662f\u5e73\u53f0\u5c42\u7684\uff0c\u4e5f\u5c31\u662f\u4e0e\u6211\u4eec\u81ea\u7814\u5e73\u53f0\u89e3\u51b3\u601d\u8def\u7c7b\u4f3c\uff0c\u5e76\u4e14\u5927\u591a\u662f\u4ee5\u96c6\u7fa4\u89c6\u89d2\u6765\u8fdb\u884c\u64cd\u4f5c\u7684\uff0c\u6240\u6709\u8d44\u6e90\u9996\u5148\u5728\u96c6\u7fa4\u7684\u7ef4\u5ea6\u4e0a\u5c31\u88ab\u5272\u88c2\u5f00\u4e86\uff0c\u5e76\u4e0d\u7b26\u5408\u6211\u4eec\u5bf9\u5e94\u7528\u89c6\u89d2\u7684\u8bc9\u6c42\u3002 \u4ee5\u5e94\u7528\u4e3a\u89c6\u89d2\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u5c06\u591a\u4e2aK8s\u96c6\u7fa4\u4f5c\u4e3a\u4e00\u4e2a\u5927\u578b\u96c6\u7fa4\u6765\u7ba1\u7406\uff0c\u8fd9\u6837\u4e00\u4e2aworkload\u5c31\u53ef\u4ee5\u770b\u505a\u662f\u4e00\u4e2a\u5e94\u7528\uff08\u6216\u4e00\u4e2a\u5e94\u7528\u7684\u67d0\u4e2a\u7248\u672c\uff09\u800c\u4e0d\u662f\u6563\u843d\u5728\u591a\u4e2a\u96c6\u7fa4\u4e2d\u540c\u4e00\u4e2a\u5e94\u7528\u7684\u591a\u4e2aworkload\u3002 \u53e6\u5916\u4e00\u4e2a\u539f\u5219\u662f\u5c3d\u91cf\u4f4e\u7684\u63a5\u5165\u6210\u672c\u3002\u6211\u4eec\u8c03\u7814\u4e86\u5f00\u6e90\u793e\u533a\u7684\u591a\u79cd\u65b9\u6848\uff0c\u7efc\u5408\u8bc4\u4f30\u540e\uff0c\u53d1\u73b0Karmada\u6bd4\u8f83\u7b26\u5408\u6211\u4eec\u7684\u9700\u6c42\u3002","title":"\u4e0d\u81ea\u5df1\u9020\u8f6e\u5b50\uff0c\u7740\u773c\u5f00\u6e90\u793e\u533a"},{"location":"adoptions/vipkid-zh/#karmada","text":"\u8bd5\u7528Karmada\u540e\uff0c\u53d1\u73b0\u6709\u4ee5\u4e0b\u51e0\u65b9\u9762\u4f18\u52bf\uff1a 1\uff09Karmada\u771f\u6b63\u610f\u4e49\u7684\u5b9e\u73b0\u4e86\u4ee5\u4e00\u4e2aK8s\u96c6\u7fa4\u89c6\u89d2\u6765\u7ba1\u7406\u591a\u96c6\u7fa4\u7684\u80fd\u529b \uff0c\u8ba9\u6211\u4eec\u80fd\u591f\u4ee5\u5e94\u7528\u89c6\u89d2\u7ba1\u7406\u591a\u96c6\u7fa4\u4e2d\u7684\u8d44\u6e90\u3002\u53e6\u5916\uff0cKarmada\u7684OverridePolicy\u8bbe\u8ba1\u51e0\u4e4e\u6240\u6709\u5dee\u5f02\u90fd\u53ef\u4ee5\u5355\u72ec\u58f0\u660e\u51fa\u6765\uff0c\u7b80\u5355\u76f4\u89c2\u4e14\u4fbf\u4e8e\u7ba1\u7406\uff0c\u8fd9\u4e0e\u6211\u4eec\u5185\u90e8\u5bf9\u5e94\u7528\u753b\u50cf\u5728\u4e0d\u540c\u96c6\u7fa4\u4e4b\u95f4\u7684\u5e94\u7528\u5dee\u5f02\u4e0d\u8c0b\u800c\u5408\u3002 2\uff09Karmada\u5b8c\u5168\u4f7f\u7528\u4e86K8s\u539f\u751fAPI \uff0c\u4f7f\u5f97\u6211\u4eec\u53ef\u4ee5\u50cf\u539f\u6765\u4e00\u6837\u4f7f\u7528\uff0c\u540c\u65f6\u4e5f\u8868\u660e\u6211\u4eec\u5728\u540e\u7eed\u7684\u63a5\u5165\u6210\u672c\u4f1a\u5f88\u4f4e\u3002\u5e76\u4e14Karmada\u7684CRD\u76f8\u5bf9\u6765\u8bb2\u4e5f\u66f4\u5bb9\u6613\u7406\u89e3\uff0c\u6211\u4eec\u5e73\u53f0\u7684\u670d\u52a1\u753b\u50cf\u6a21\u5757\u53ef\u4ee5\u5f88\u5bb9\u6613\u7684\u5c06\u5206\u53d1\u548c\u5dee\u5f02\u914d\u7f6e\u52a8\u6001\u6e32\u67d3\u6210Propagation\u548cOverride\u7b56\u7565\uff0c\u4e0b\u53d1\u7ed9Karmada\u63a7\u5236\u9762\u3002 3\uff09\u5f00\u6e90\u5f00\u653e\u7684\u793e\u533a\u6cbb\u7406\u6a21\u5f0f \uff0c\u4e5f\u662f\u6211\u4eec\u56e2\u961f\u6700\u770b\u91cd\u7684\u4e00\u70b9\u3002\u5728\u8bd5\u7528Karmada\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u8bba\u662f\u6211\u4eec\u81ea\u5df1\u8fd8\u662f\u793e\u533a\u65b9\u9762\u5bf9\u9700\u6c42\u7684\u7406\u89e3\u548c\u8bbe\u60f3\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u90fd\u53ef\u4ee5\u5728\u793e\u533a\u4e2d\u5f00\u653e\u8ba8\u8bba\u3002\u540c\u65f6\uff0c\u5728\u53c2\u4e0e\u4ee3\u7801\u8d21\u732e\u8fc7\u7a0b\u4e2d\uff0c\u6211\u4eec\u56e2\u961f\u6574\u4f53\u6280\u672f\u80fd\u529b\u4e5f\u663e\u8457\u63d0\u5347\u3002","title":"\u5c31\u5b83\u4e86\uff0c Karmada\uff01"},{"location":"adoptions/vipkid-zh/#karmada-at-vipkid","text":"\u6211\u4eec\u7684\u5bb9\u5668\u5e73\u53f0\uff0c\u627f\u8f7d\u4e86\u6574\u4e2a\u516c\u53f8\u6240\u6709\u7684\u5bb9\u5668\u5316\u90e8\u7f72\u8bc9\u6c42\uff0c\u5305\u62ec\u6709\u65e0\u72b6\u6001\u3001\u5728\u79bb\u7ebf\u4e1a\u52a1\u548cAI\u5927\u6570\u636e\u7b49\u3002\u5e76\u4e14\u8981\u6c42PaaS\u5e73\u53f0\u7684\u8bbe\u8ba1\u548c\u5b9e\u65bd\u4e0d\u4f1a\u5bf9\u67d0\u4e00\u5bb6\u516c\u6709\u4e91\u4ea7\u751f\u4efb\u4f55\u4f9d\u8d56\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u4f7f\u7528\u4e91\u5382\u5546\u5c01\u88c5\u8fc7\u7684\u4e00\u4e9b\u4ea7\u54c1\u3002 \u6211\u4eec\u4f1a\u4f9d\u8d56\u5185\u90e8\u7684IaaS\u5e73\u53f0\u53bb\u7ba1\u7406\u591a\u5bb6\u4e91\u5382\u5546\u7684\u5404\u7c7b\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u62ecK8s\u96c6\u7fa4\u7684\u521b\u5efa\u3001\u6269\u5bb9\u3001VPC\uff0c\u5b50\u7f51\u4ee5\u53ca\u5b89\u5168\u7ec4\u7684\u914d\u7f6e\u3002\u8fd9\u4e2a\u5f88\u91cd\u8981\uff0c\u56e0\u4e3a\u8fd9\u8ba9\u6211\u4eec\u53ef\u4ee5\u6807\u51c6\u5316\u591a\u4e2a\u4e91\u5382\u5546\u7684K8s\u96c6\u7fa4\uff0c\u8ba9\u4e0a\u5c42PaaS\u5e73\u53f0\u51e0\u4e4e\u65e0\u9700\u5173\u5fc3\u5382\u5546\u7ea7\u522b\u7684\u5dee\u5f02\u3002 \u53e6\u5916\uff0c\u5bf9\u4e8e\u7cfb\u7edf\u7ea7\u522b\u7684\u5e94\u7528\u548c\u7ec4\u4ef6\uff0c\u6211\u4eec\u4e3a\u5f00\u53d1\u8005\u521b\u5efa\u4e86\u53e6\u5916\u4e00\u6761\u7ba1\u7406\u6e20\u9053\uff0c\u90a3\u5c31\u662f\u4f7f\u7528GitOps\u3002\u8fd9\u5bf9\u9ad8\u9636\u5f00\u53d1\u8005\u6765\u8bf4\u8981\u66f4\u52a0\u53cb\u597d\uff0c\u5bf9\u7cfb\u7edf\u5e94\u7528\u7684\u7ec4\u4ef6\u5b89\u88c5\u90e8\u7f72\u66f4\u4e3a\u9ad8\u6548\u3002","title":"Karmada at VIPKID"},{"location":"adoptions/vipkid-zh/#karmada_1","text":"\u5728\u5e73\u53f0\u843d\u5730\u4e4b\u521d\uff0c\u6211\u4eec\u5355\u72ec\u5265\u79bb\u4e86\u4e00\u4e2a\u7ec4\u4ef6\uff08\u4e0a\u56fe\u5de6\u4fa7\u7684\u201c\u96c6\u7fa4\u6c47\u805aAPI\u201d\uff09\uff0c\u4e13\u95e8\u548cK8s\u96c6\u7fa4\u8fdb\u884c\u4ea4\u4e92\uff0c\u5e76\u4e14\u5411\u4e0a\u4fdd\u7559K8s\u539f\u751fAPI\uff0c\u4e5f\u4f1a\u9644\u52a0\u4e00\u4e9b\u548c\u96c6\u7fa4\u76f8\u5173\u4fe1\u606f\u3002 \u4f46\u5728\u843d\u5730\u8fc7\u7a0b\u4e2d\uff0c\u201c\u5bb9\u5668\u5e94\u7528\u7ba1\u7406\u201d\u7cfb\u7edf\u9700\u8981\u8fdb\u884c\u8bb8\u591a\u64cd\u4f5c\u9002\u914d\u591a\u96c6\u7fa4\u4e0b\u7684\u590d\u6742\u60c5\u51b5\u3002\u6bd4\u5982\u867d\u7136PaaS\u7cfb\u7edf\u770b\u8d77\u6765\u662f\u4e00\u4e2a\u5e94\u7528\uff0c\u4f46\u7cfb\u7edf\u9700\u8981\u6e32\u67d3\u4e0d\u540c\u7684\u5b8c\u6574\u8d44\u6e90\u58f0\u660e\u5230\u4e0d\u540c\u7684\u96c6\u7fa4\uff0c\u4f7f\u5f97\u6211\u4eec\u5728\u771f\u6b63\u7ef4\u62a4\u591a\u96c6\u7fa4\u5e94\u7528\u65f6\u4ecd\u7136\u662f\u4e0d\u76f8\u5173\u7684\u3001\u5272\u88c2\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u529e\u6cd5\u5728\u5e95\u5c42\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\u8bf8\u5982\u6b64\u7c7b\u95ee\u9898\u7684\u89e3\u51b3\u8fd8\u662f\u5360\u7528\u4e86\u56e2\u961f\u4e0d\u5c11\u7684\u8d44\u6e90\uff0c\u5c24\u5176\u662f\u5f15\u5165CRD\u8d44\u6e90\u540e\uff0c\u8fd8\u9700\u8981\u91cd\u590d\u7684\u89e3\u51b3\u8fd9\u65b9\u9762\u7684\u95ee\u9898\u3002\u5e76\u4e14\u8fd9\u4e2a\u7cfb\u7edf\u65e0\u6cd5\u4e0d\u53bb\u5173\u5fc3\u6bcf\u4e2a\u96c6\u7fa4\u91cc\u9762\u7684\u7ec6\u8282\u72b6\u51b5\uff0c\u5982\u6b64\u80cc\u79bb\u4e86\u6211\u4eec\u8bbe\u8ba1\u201c\u96c6\u7fa4\u6c47\u805aAPI\u201d\u7ec4\u4ef6\u7684\u521d\u8877\u3002 \u53e6\u5916\uff0c\u7531\u4e8eGitOps\u4e5f\u9700\u8981\u4e0e\u96c6\u7fa4\u5f3a\u76f8\u5173\uff0c\u5728\u96c6\u7fa4\u6570\u91cf\u8f83\u5927\uff0c\u5e76\u4e14\u7ecf\u5e38\u4f34\u968f\u96c6\u7fa4\u4e0a\u4e0b\u7ebf\u7684\u60c5\u51b5\u4e0b\uff0c\u6b64\u65f6\uff0c\u82e5\u8981\u6b63\u5e38\u8fd0\u8f6c\u5c31\u9700\u8981\u5bf9GitOps\u7684\u5e94\u7528\u914d\u7f6e\u8fdb\u884c\u6279\u91cf\u53d8\u66f4\uff0c\u968f\u4e4b\u589e\u52a0\u7684\u590d\u6742\u5ea6\uff0c\u8ba9\u6574\u4f53\u6548\u679c\u5e76\u672a\u8fbe\u5230\u9884\u671f\u3002 \u4e0b\u56fe\u662fVIPKID\u5f15\u5165Karmada\u4e4b\u524d\u548c\u4e4b\u540e\u7684\u67b6\u6784\u5bf9\u6bd4\uff1a \u5f15\u5165Karmada\u540e\uff0c\u591a\u96c6\u7fa4\u805a\u5408\u5c42\u5f97\u4ee5\u771f\u6b63\u7684\u7edf\u4e00 \uff0c\u6211\u4eec\u53ef\u4ee5\u5728Karmada\u63a7\u5236\u5e73\u9762\u4ee5\u5e94\u7528\u7ef4\u5ea6\u53bb\u7ba1\u7406\u8d44\u6e90\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u90fd\u4e0d\u9700\u8981\u6df1\u5165\u5230\u53d7\u63a7\u96c6\u7fa4\u4e2d\uff0c\u53ea\u9700\u8981\u4e0eKarmada\u4ea4\u4e92\u5373\u53ef\u3002\u5982\u6b64\u6781\u5927\u7684\u7b80\u5316\u4e86\u6211\u4eec\u7684\u201c\u5bb9\u5668\u5e94\u7528\u7ba1\u7406\u201d\u7cfb\u7edf\u3002\u73b0\u5728\uff0c\u6211\u4eec\u7684PaaS\u5e73\u53f0\u53ef\u4ee5\u5b8c\u5168\u503e\u6ce8\u4e8e\u4e1a\u52a1\u9700\u6c42\uff0cKarmada\u5f3a\u5927\u7684\u80fd\u529b\u5df2\u7ecf\u6ee1\u8db3\u4e86\u5f53\u524d\u6211\u4eec\u7684\u5404\u7c7b\u9700\u6c42\u3002 \u800cGitOps\u4f53\u7cfb\u4f7f\u7528Karmada\u540e\uff0c\u7cfb\u7edf\u7ea7\u7ec4\u4ef6\u4e5f\u53ef\u4ee5\u7b80\u5355\u7684\u5728\u5404\u4e2a\u96c6\u7fa4\u4e2d\u8fdb\u884c\u53d1\u5e03\u548c\u5347\u7ea7\uff0c\u4e0d\u4ec5\u8ba9\u6211\u4eec\u4f53\u9a8c\u5230\u4e86GitOps\u672c\u8eab\u7684\u4fbf\u5229\uff0c\u66f4\u662f\u8ba9\u6211\u4eec\u6536\u83b7\u5230\u4e86GitOps*Karmada\u7684\u6210\u500d\u7ea7\u7684\u6548\u7387\u63d0\u5347\u3002","title":"\u57fa\u4e8eKarmada\u7684\u5bb9\u5668\u5316\u6539\u9020\u65b9\u6848"},{"location":"adoptions/vipkid-zh/#_7","text":"\u4ee5\u5e94\u7528\u4e3a\u7ef4\u5ea6\u6765\u7ba1\u7406K8s\u8d44\u6e90\uff0c\u964d\u4f4e\u4e86\u5e73\u53f0\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4f7f\u7528\u6548\u7387\u3002\u4e0b\u9762\u4ee5\u6211\u4eecPaaS\u5e73\u53f0\u7279\u6027\u5165\u624b\uff0c\u6765\u63cf\u8ff0\u5f15\u5165Karmada\u540e\u7684\u6539\u53d8\u3002 1\uff09\u591a\u96c6\u7fa4\u5e94\u7528\u7684\u90e8\u7f72\u901f\u5ea6\u663e\u8457\u63d0\u5347\uff1a \u5148\u524d\u5728\u90e8\u7f72\u65f6\u9700\u8981\u5411\u6bcf\u4e2a\u96c6\u7fa4\u53d1\u9001\u90e8\u7f72\u6307\u4ee4\uff0c\u968f\u4e4b\u76d1\u6d4b\u90e8\u7f72\u72b6\u6001\u662f\u5426\u5f02\u5e38\u3002\u5982\u6b64\u5c31\u9700\u8981\u4e0d\u65ad\u7684\u68c0\u67e5\u591a\u4e2a\u96c6\u7fa4\u4e2d\u7684\u8d44\u6e90\u72b6\u6001\uff0c\u7136\u540e\u518d\u6839\u636e\u5f02\u5e38\u60c5\u51b5\u505a\u4e0d\u540c\u7684\u5904\u7406\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u903b\u8f91\u7e41\u7410\u5e76\u4e14\u7f13\u6162\u3002\u5f15\u5165Karmada\u540e\uff0cKarmada\u4f1a\u81ea\u52a8\u6536\u96c6\u548c\u6c47\u805a\u5e94\u7528\u5728\u5404\u4e2a\u96c6\u7fa4\u7684\u72b6\u6001\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7Karmada\u6765\u611f\u77e5\u5e94\u7528\u72b6\u6001\u3002 2\uff09\u5e94\u7528\u7684\u5dee\u5f02\u63a7\u5236\u53ef\u5f00\u653e\u7ed9\u5f00\u53d1\u8005\uff1a DevOps\u6587\u5316\u6700\u91cd\u8981\u7684\u4e00\u70b9\u5c31\u662f\u5f00\u53d1\u8005\u8981\u80fd\u591f\u5b8c\u5168\u53c2\u4e0e\u8fdb\u6765\uff0c\u80fd\u591f\u4fbf\u6377\u5730\u5bf9\u5e94\u7528\u5168\u751f\u547d\u5468\u671f\u8fdb\u884c\u7ba1\u7406\u3002\u6211\u4eec\u5145\u5206\u5229\u7528\u4e86Karmada\u7684Override\u7b56\u7565\uff0c\u76f4\u63a5\u4e0e\u5e94\u7528\u753b\u50cf\u5bf9\u63a5\uff0c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u6e05\u6670\u7684\u4e86\u89e3\u548c\u63a7\u5236\u5e94\u7528\u5728\u4e0d\u540c\u96c6\u7fa4\u7684\u5dee\u5f02\uff0c\u73b0\u5df2\u652f\u6301\u73af\u5883\u53d8\u91cf\uff0c\u542f\u52a8\u53c2\u6570\uff0c\u955c\u50cf\u4ed3\u5e93\u3002 3\uff09\u96c6\u7fa4\u7684\u5feb\u901f\u62c9\u8d77&\u5bf9GitOps\u9002\u914d\uff1a \u6211\u4eec\u5c06\u57fa\u7840\u670d\u52a1\uff08\u7cfb\u7edf\u7ea7\u548c\u901a\u7528\u7c7b\u578b\u670d\u52a1\uff09\u5728Karmada\u7684Propagation\u8bbe\u5b9a\u4e3a\u5168\u91cf\u96c6\u7fa4\uff0c\u5728\u65b0\u96c6\u7fa4\u521b\u5efa\u597d\u4ee5\u540e\uff0c\u76f4\u63a5\u52a0\u5165\u5230Karmada\u4e2d\u8fdb\u884c\u7eb3\u7ba1\uff0c\u8fd9\u4e9b\u57fa\u7840\u670d\u52a1\u53ef\u4ee5\u4f34\u968f\u96c6\u7fa4\u4ea4\u4ed8\u4e00\u5e76\u4ea4\u4ed8\uff0c\u8282\u7701\u4e86\u6211\u4eec\u5bf9\u96c6\u7fa4\u505a\u57fa\u7840\u670d\u52a1\u521d\u59cb\u5316\u7684\u64cd\u4f5c\uff0c\u5927\u5927\u7f29\u77ed\u4e86\u4ea4\u4ed8\u73af\u8282\u548c\u65f6\u95f4\u3002\u5e76\u4e14\u5927\u90e8\u5206\u57fa\u7840\u670d\u52a1\u90fd\u662f\u7531\u6211\u4eec\u7684GitOps\u4f53\u7cfb\u7ba1\u7406\u7684\uff0c\u76f8\u6bd4\u8fc7\u53bb\u4e00\u4e2a\u4e2a\u96c6\u7fa4\u7684\u914d\u7f6e\u6765\u8bb2\uff0c\u65e2\u65b9\u4fbf\u53c8\u76f4\u89c2\u3002 4\uff09\u5e73\u53f0\u6539\u9020\u5468\u671f\u77ed\uff0c\u4e1a\u52a1\u65e0\u611f\u77e5\uff1a \u5f97\u76ca\u4e8eKarmada\u7684\u539f\u751fK8s API\uff0c\u6211\u4eec\u82b1\u4e86\u5f88\u5c11\u7684\u65f6\u95f4\u5728\u63a5\u5165Karmada\u4e0a\u3002Karmada\u771f\u6b63\u505a\u5230\u4e86\u539f\u6765\u600e\u4e48\u7528K8s\u73b0\u5728\u7ee7\u7eed\u600e\u4e48\u7528\u5c31\u53ef\u4ee5\u4e86\u3002\u552f\u4e00\u9700\u8981\u8003\u8651\u7684\u662fPropagation\u7b56\u7565\u7684\u5b9a\u5236\uff0c\u53ef\u4ee5\u6309\u7167\u8d44\u6e90\u540d\u5b57\u7684\u7ef4\u5ea6\uff0c\u4e5f\u53ef\u4ee5\u6309\u7167\u8d44\u6e90\u7c7b\u578b\u6216LabelSelector\u7684\u7ef4\u5ea6\u6765\u58f0\u660e\uff0c\u6781\u5176\u65b9\u4fbf\u3002","title":"\u6536\u76ca"},{"location":"adoptions/vipkid-zh/#_8","text":"\u4ece2021\u5e74\u76842\u6708\u4efd\u63a5\u89e6\u5230Karmada\u9879\u76ee\u4ee5\u6765\uff0c\u6211\u4eec\u56e2\u961f\u5148\u540e\u67093\u4eba\u6210\u4e3a\u4e86Karmada\u793e\u533a\u7684Contributor\uff0c\u4ece0.5.0\u52301.0.0\u7248\u672c\uff0c\u53c2\u4e0e\u548c\u89c1\u8bc1\u4e86\u591a\u4e2afeature\u7684\u53d1\u5e03\u3002\u540c\u65f6Karmada\u4e5f\u89c1\u8bc1\u4e86\u6211\u4eec\u56e2\u961f\u7684\u6210\u957f\u3002 \u628a\u81ea\u5df1\u7684\u9700\u6c42\u5199\u6210\u4ee3\u7801\u5f88\u7b80\u5355\uff0c\u628a\u81ea\u5df1\u7684\u9700\u6c42\u548c\u5176\u4ed6\u4eba\u8ba8\u8bba\uff0c\u5bf9\u6240\u6709\u4eba\u7684\u9700\u6c42\u8fdb\u884c\u5708\u5b9a\u548c\u53d6\u820d\uff0c\u9009\u62e9\u4e00\u4e2a\u7b26\u5408\u5927\u5bb6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u518d\u8f6c\u6362\u6210\u4ee3\u7801\uff0c\u96be\u5ea6\u5219\u4f1a\u5347\u7ea7\u3002\u6211\u4eec\u56e2\u961f\u5728\u6b64\u671f\u95f4\u6536\u83b7\u4e86\u5f88\u591a\uff0c\u4e5f\u6210\u957f\u4e86\u8bb8\u591a\uff0c\u5e76\u4e14\u4e3a\u80fd\u591f\u53c2\u4e0eKarmada\u9879\u76ee\u5efa\u8bbe\u800c\u611f\u5230\u81ea\u8c6a\uff0c\u5e0c\u671b\u66f4\u591a\u7684\u5f00\u53d1\u8005\u80fd\u52a0\u5165Karmada\u793e\u533a\uff0c\u4e00\u8d77\u8ba9\u793e\u533a\u751f\u6001\u66f4\u52a0\u7e41\u8363\uff01","title":"\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee\u7684\u6536\u83b7"},{"location":"contributors/devel/cherry-picks/","text":"Overview This document explains how cherry picks are managed on release branches within the karmada-io/karmada repository. A common use case for this task is backporting PRs from master to release branches. This doc is lifted from Kubernetes cherry-pick . Prerequisites What Kind of PRs are Good for Cherry Picks Initiate a Cherry Pick Cherry Pick Review Troubleshooting Cherry Picks Cherry Picks for Unsupported Releases Prerequisites A pull request merged against the master branch. The release branch exists (example: release-1.0 ) The normal git and GitHub configured shell environment for pushing to your karmada origin fork on GitHub and making a pull request against a configured remote upstream that tracks https://github.com/karmada-io/karmada.git , including GITHUB_USER . Have GitHub CLI ( gh ) installed following installation instructions . A github personal access token which has permissions \"repo\" and \"read:org\". Permissions are required for gh auth login and not used for anything unrelated to cherry-pick creation process (creating a branch and initiating PR). What Kind of PRs are Good for Cherry Picks Compared to the normal master branch's merge volume across time, the release branches see one or two orders of magnitude less PRs. This is because there is an order or two of magnitude higher scrutiny. Again, the emphasis is on critical bug fixes, e.g., Loss of data Memory corruption Panic, crash, hang Security A bugfix for a functional issue (not a data loss or security issue) that only affects an alpha feature does not qualify as a critical bug fix. If you are proposing a cherry pick and it is not a clear and obvious critical bug fix, please reconsider. If upon reflection you wish to continue, bolster your case by supplementing your PR with e.g., A GitHub issue detailing the problem Scope of the change Risks of adding a change Risks of associated regression Testing performed, test cases added Key stakeholder reviewers/approvers attesting to their confidence in the change being a required backport It is critical that our full community is actively engaged on enhancements in the project. If a released feature was not enabled on a particular provider's platform, this is a community miss that needs to be resolved in the master branch for subsequent releases. Such enabling will not be backported to the patch release branches. Initiate a Cherry Pick Run the cherry pick script This example applies a master branch PR #1206 to the remote branch upstream/release-1.0 : shell hack/cherry_pick_pull.sh upstream/release-1.0 1206 Be aware the cherry pick script assumes you have a git remote called upstream that points at the Karmada github org. You will need to run the cherry pick script separately for each patch release you want to cherry pick to. Cherry picks should be applied to all active release branches where the fix is applicable. If GITHUB_TOKEN is not set you will be asked for your github password: provide the github personal access token rather than your actual github password. If you can securely set the environment variable GITHUB_TOKEN to your personal access token then you can avoid an interactive prompt. Refer https://github.com/github/hub/issues/2655#issuecomment-735836048 Cherry Pick Review As with any other PR, code OWNERS review ( /lgtm ) and approve ( /approve ) on cherry pick PRs as they deem appropriate. The same release note requirements apply as normal pull requests, except the release note stanza will auto-populate from the master branch pull request from which the cherry pick originated. Troubleshooting Cherry Picks Contributors may encounter some of the following difficulties when initiating a cherry pick. A cherry pick PR does not apply cleanly against an old release branch. In that case, you will need to manually fix conflicts. The cherry pick PR includes code that does not pass CI tests. In such a case you will have to fetch the auto-generated branch from your fork, amend the problematic commit and force push to the auto-generated branch. Alternatively, you can create a new PR, which is noisier. Cherry Picks for Unsupported Releases The community supports & patches releases need to be discussed.","title":"Overview"},{"location":"contributors/devel/cherry-picks/#overview","text":"This document explains how cherry picks are managed on release branches within the karmada-io/karmada repository. A common use case for this task is backporting PRs from master to release branches. This doc is lifted from Kubernetes cherry-pick . Prerequisites What Kind of PRs are Good for Cherry Picks Initiate a Cherry Pick Cherry Pick Review Troubleshooting Cherry Picks Cherry Picks for Unsupported Releases","title":"Overview"},{"location":"contributors/devel/cherry-picks/#prerequisites","text":"A pull request merged against the master branch. The release branch exists (example: release-1.0 ) The normal git and GitHub configured shell environment for pushing to your karmada origin fork on GitHub and making a pull request against a configured remote upstream that tracks https://github.com/karmada-io/karmada.git , including GITHUB_USER . Have GitHub CLI ( gh ) installed following installation instructions . A github personal access token which has permissions \"repo\" and \"read:org\". Permissions are required for gh auth login and not used for anything unrelated to cherry-pick creation process (creating a branch and initiating PR).","title":"Prerequisites"},{"location":"contributors/devel/cherry-picks/#what-kind-of-prs-are-good-for-cherry-picks","text":"Compared to the normal master branch's merge volume across time, the release branches see one or two orders of magnitude less PRs. This is because there is an order or two of magnitude higher scrutiny. Again, the emphasis is on critical bug fixes, e.g., Loss of data Memory corruption Panic, crash, hang Security A bugfix for a functional issue (not a data loss or security issue) that only affects an alpha feature does not qualify as a critical bug fix. If you are proposing a cherry pick and it is not a clear and obvious critical bug fix, please reconsider. If upon reflection you wish to continue, bolster your case by supplementing your PR with e.g., A GitHub issue detailing the problem Scope of the change Risks of adding a change Risks of associated regression Testing performed, test cases added Key stakeholder reviewers/approvers attesting to their confidence in the change being a required backport It is critical that our full community is actively engaged on enhancements in the project. If a released feature was not enabled on a particular provider's platform, this is a community miss that needs to be resolved in the master branch for subsequent releases. Such enabling will not be backported to the patch release branches.","title":"What Kind of PRs are Good for Cherry Picks"},{"location":"contributors/devel/cherry-picks/#initiate-a-cherry-pick","text":"Run the cherry pick script This example applies a master branch PR #1206 to the remote branch upstream/release-1.0 : shell hack/cherry_pick_pull.sh upstream/release-1.0 1206 Be aware the cherry pick script assumes you have a git remote called upstream that points at the Karmada github org. You will need to run the cherry pick script separately for each patch release you want to cherry pick to. Cherry picks should be applied to all active release branches where the fix is applicable. If GITHUB_TOKEN is not set you will be asked for your github password: provide the github personal access token rather than your actual github password. If you can securely set the environment variable GITHUB_TOKEN to your personal access token then you can avoid an interactive prompt. Refer https://github.com/github/hub/issues/2655#issuecomment-735836048","title":"Initiate a Cherry Pick"},{"location":"contributors/devel/cherry-picks/#cherry-pick-review","text":"As with any other PR, code OWNERS review ( /lgtm ) and approve ( /approve ) on cherry pick PRs as they deem appropriate. The same release note requirements apply as normal pull requests, except the release note stanza will auto-populate from the master branch pull request from which the cherry pick originated.","title":"Cherry Pick Review"},{"location":"contributors/devel/cherry-picks/#troubleshooting-cherry-picks","text":"Contributors may encounter some of the following difficulties when initiating a cherry pick. A cherry pick PR does not apply cleanly against an old release branch. In that case, you will need to manually fix conflicts. The cherry pick PR includes code that does not pass CI tests. In such a case you will have to fetch the auto-generated branch from your fork, amend the problematic commit and force push to the auto-generated branch. Alternatively, you can create a new PR, which is noisier.","title":"Troubleshooting Cherry Picks"},{"location":"contributors/devel/cherry-picks/#cherry-picks-for-unsupported-releases","text":"The community supports & patches releases need to be discussed.","title":"Cherry Picks for Unsupported Releases"},{"location":"contributors/guide/github-workflow/","text":"This doc is lifted from Kubernetes github-workflow . 1 Fork in the cloud Visit https://github.com/karmada-io/karmada Click Fork button (top right) to establish a cloud-based fork. 2 Clone fork to local storage Per Go's workspace instructions , place Karmada' code on your GOPATH using the following cloning procedure. Define a local working directory: # If your GOPATH has multiple paths, pick # just one and use it instead of $GOPATH here. # You must follow exactly this pattern, # neither `$GOPATH/src/github.com/${your github profile name/` # nor any other pattern will work. export working_dir=\"$(go env GOPATH)/src/github.com/karmada-io\" Set user to match your github profile name: export user={your github profile name} Both $working_dir and $user are mentioned in the figure above. Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/$user/karmada.git # or: git clone git@github.com:$user/karmada.git cd $working_dir/karmada git remote add upstream https://github.com/karmada-io/karmada.git # or: git remote add upstream git@github.com:karmada-io/karmada.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v 3 Branch Get your local master up to date: # Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. cd $working_dir/karmada git fetch upstream git checkout master git rebase upstream/master Branch from it: git checkout -b myfeature Then edit code on the myfeature branch. 4 Keep your branch in sync # Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. # While on your myfeature branch git fetch upstream git rebase upstream/master Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful (see below). You can also consider changing your .git/config file via git config branch.autoSetupRebase always to change the behavior of git pull , or another non-merge option such as git pull --rebase . 5 Commit Commit your changes. git commit --signoff Likely you go back and edit/build/test some more then commit --amend in a few cycles. 6 Push When ready to review (or just to establish an offsite backup of your work), push your branch to your fork on github.com : git push -f ${your_remote_name} myfeature 7 Create a pull request Visit your fork at https://github.com/$user/karmada Click the Compare & Pull Request button next to your myfeature branch. If you have upstream write access , please refrain from using the GitHub UI for creating PRs, because GitHub will create the PR branch inside the main repository rather than inside your fork. Get a code review Once your pull request has been opened it will be assigned to one or more reviewers. Those reviewers will do a thorough code review, looking for correctness, bugs, opportunities for improvement, documentation and comments, and style. Commit changes made in response to review comments to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review. Squash commits After a review, prepare your PR for merging by squashing your commits. All commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Before merging a PR, squash the following kinds of commits: Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it's not a requirement. In particular, merge commits must be removed, as they will not pass tests. To squash your commits, perform an interactive rebase : Check your git branch: git status Output is similar to: On branch your-contribution Your branch is up to date with 'origin/your-contribution'. Start an interactive rebase using a specific commit hash, or count backwards from your last commit using HEAD~<n> , where <n> represents the number of commits to include in the rebase. git rebase -i HEAD~3 Output is similar to: ``` pick 2ebe926 Original commit pick 31f33e9 Address feedback pick b0315fe Second unit of work # Rebase 7c34fc9..b0315ff onto 7c34fc9 (3 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message ... ``` Use a command line text editor to change the word pick to squash for the commits you want to squash, then save your changes and continue the rebase: ``` pick 2ebe926 Original commit squash 31f33e9 Address feedback pick b0315fe Second unit of work ... ``` Output (after saving changes) is similar to: ``` [detached HEAD 61fdded] Second unit of work Date: Thu Mar 5 19:01:32 2020 +0100 2 files changed, 15 insertions(+), 1 deletion(-) ... Successfully rebased and updated refs/heads/master. ``` 4. Force push your changes to your remote branch: git push --force For mass automated fixups (e.g. automated doc formatting), use one or more commits for the changes to tooling and a final commit to apply the fixup en masse. This makes reviews easier. Merging a commit Once you've received review and approval, your commits are squashed, your PR is ready for merging. Merging happens automatically after both a Reviewer and Approver have approved the PR. If you haven't squashed your commits, they may ask you to do so before approving a PR. Reverting a commit In case you wish to revert a commit, use the following instructions. If you have upstream write access , please refrain from using the Revert button in the GitHub UI for creating the PR, because GitHub will create the PR branch inside the main repository rather than inside your fork. Create a branch and sync it with upstream. ```sh # Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. # create a branch git checkout -b myrevert # sync the branch with upstream git fetch upstream git rebase upstream/master ``` - If the commit you wish to revert is a: - merge commit: ```sh # SHA is the hash of the merge commit you wish to revert git revert -m 1 SHA ``` single commit: ```sh SHA is the hash of the single commit you wish to revert git revert SHA ``` This will create a new commit reverting the changes. Push this new commit to your remote. git push ${your_remote_name} myrevert Create a Pull Request using this branch.","title":"GitHub Workflow"},{"location":"contributors/guide/github-workflow/#1-fork-in-the-cloud","text":"Visit https://github.com/karmada-io/karmada Click Fork button (top right) to establish a cloud-based fork.","title":"1 Fork in the cloud"},{"location":"contributors/guide/github-workflow/#2-clone-fork-to-local-storage","text":"Per Go's workspace instructions , place Karmada' code on your GOPATH using the following cloning procedure. Define a local working directory: # If your GOPATH has multiple paths, pick # just one and use it instead of $GOPATH here. # You must follow exactly this pattern, # neither `$GOPATH/src/github.com/${your github profile name/` # nor any other pattern will work. export working_dir=\"$(go env GOPATH)/src/github.com/karmada-io\" Set user to match your github profile name: export user={your github profile name} Both $working_dir and $user are mentioned in the figure above. Create your clone: mkdir -p $working_dir cd $working_dir git clone https://github.com/$user/karmada.git # or: git clone git@github.com:$user/karmada.git cd $working_dir/karmada git remote add upstream https://github.com/karmada-io/karmada.git # or: git remote add upstream git@github.com:karmada-io/karmada.git # Never push to upstream master git remote set-url --push upstream no_push # Confirm that your remotes make sense: git remote -v","title":"2 Clone fork to local storage"},{"location":"contributors/guide/github-workflow/#3-branch","text":"Get your local master up to date: # Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. cd $working_dir/karmada git fetch upstream git checkout master git rebase upstream/master Branch from it: git checkout -b myfeature Then edit code on the myfeature branch.","title":"3 Branch"},{"location":"contributors/guide/github-workflow/#4-keep-your-branch-in-sync","text":"# Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. # While on your myfeature branch git fetch upstream git rebase upstream/master Please don't use git pull instead of the above fetch / rebase . git pull does a merge, which leaves merge commits. These make the commit history messy and violate the principle that commits ought to be individually understandable and useful (see below). You can also consider changing your .git/config file via git config branch.autoSetupRebase always to change the behavior of git pull , or another non-merge option such as git pull --rebase .","title":"4 Keep your branch in sync"},{"location":"contributors/guide/github-workflow/#5-commit","text":"Commit your changes. git commit --signoff Likely you go back and edit/build/test some more then commit --amend in a few cycles.","title":"5 Commit"},{"location":"contributors/guide/github-workflow/#6-push","text":"When ready to review (or just to establish an offsite backup of your work), push your branch to your fork on github.com : git push -f ${your_remote_name} myfeature","title":"6 Push"},{"location":"contributors/guide/github-workflow/#7-create-a-pull-request","text":"Visit your fork at https://github.com/$user/karmada Click the Compare & Pull Request button next to your myfeature branch. If you have upstream write access , please refrain from using the GitHub UI for creating PRs, because GitHub will create the PR branch inside the main repository rather than inside your fork.","title":"7 Create a pull request"},{"location":"contributors/guide/github-workflow/#get-a-code-review","text":"Once your pull request has been opened it will be assigned to one or more reviewers. Those reviewers will do a thorough code review, looking for correctness, bugs, opportunities for improvement, documentation and comments, and style. Commit changes made in response to review comments to the same branch on your fork. Very small PRs are easy to review. Very large PRs are very difficult to review.","title":"Get a code review"},{"location":"contributors/guide/github-workflow/#squash-commits","text":"After a review, prepare your PR for merging by squashing your commits. All commits left on your branch after a review should represent meaningful milestones or units of work. Use commits to add clarity to the development and review process. Before merging a PR, squash the following kinds of commits: Fixes/review feedback Typos Merges and rebases Work in progress Aim to have every commit in a PR compile and pass tests independently if you can, but it's not a requirement. In particular, merge commits must be removed, as they will not pass tests. To squash your commits, perform an interactive rebase : Check your git branch: git status Output is similar to: On branch your-contribution Your branch is up to date with 'origin/your-contribution'. Start an interactive rebase using a specific commit hash, or count backwards from your last commit using HEAD~<n> , where <n> represents the number of commits to include in the rebase. git rebase -i HEAD~3 Output is similar to: ``` pick 2ebe926 Original commit pick 31f33e9 Address feedback pick b0315fe Second unit of work # Rebase 7c34fc9..b0315ff onto 7c34fc9 (3 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message ... ``` Use a command line text editor to change the word pick to squash for the commits you want to squash, then save your changes and continue the rebase: ``` pick 2ebe926 Original commit squash 31f33e9 Address feedback pick b0315fe Second unit of work ... ``` Output (after saving changes) is similar to: ``` [detached HEAD 61fdded] Second unit of work Date: Thu Mar 5 19:01:32 2020 +0100 2 files changed, 15 insertions(+), 1 deletion(-) ... Successfully rebased and updated refs/heads/master. ``` 4. Force push your changes to your remote branch: git push --force For mass automated fixups (e.g. automated doc formatting), use one or more commits for the changes to tooling and a final commit to apply the fixup en masse. This makes reviews easier.","title":"Squash commits"},{"location":"contributors/guide/github-workflow/#merging-a-commit","text":"Once you've received review and approval, your commits are squashed, your PR is ready for merging. Merging happens automatically after both a Reviewer and Approver have approved the PR. If you haven't squashed your commits, they may ask you to do so before approving a PR.","title":"Merging a commit"},{"location":"contributors/guide/github-workflow/#reverting-a-commit","text":"In case you wish to revert a commit, use the following instructions. If you have upstream write access , please refrain from using the Revert button in the GitHub UI for creating the PR, because GitHub will create the PR branch inside the main repository rather than inside your fork. Create a branch and sync it with upstream. ```sh # Depending on which repository you are working from, # the default branch may be called 'main' instead of 'master'. # create a branch git checkout -b myrevert # sync the branch with upstream git fetch upstream git rebase upstream/master ``` - If the commit you wish to revert is a: - merge commit: ```sh # SHA is the hash of the merge commit you wish to revert git revert -m 1 SHA ``` single commit: ```sh","title":"Reverting a commit"},{"location":"contributors/guide/github-workflow/#sha-is-the-hash-of-the-single-commit-you-wish-to-revert","text":"git revert SHA ``` This will create a new commit reverting the changes. Push this new commit to your remote. git push ${your_remote_name} myrevert Create a Pull Request using this branch.","title":"SHA is the hash of the single commit you wish to revert"},{"location":"installation/fromsource/","text":"Table of Contents generated with DocToc Installing Karmada on Cluster from Source Select a way to expose karmada-apiserver 1. expose by service with LoadBalancer type 2. expose by service with ClusterIP type Install Installing Karmada on Cluster from Source This document describes how you can use the hack/remote-up-karmada.sh script to install Karmada on your clusters based on the codebase. Select a way to expose karmada-apiserver The hack/remote-up-karmada.sh will install karmada-apiserver and provide two ways to expose the server: 1. expose by HostNetwork type By default, the hack/remote-up-karmada.sh will expose karmada-apiserver by HostNetwork . No extra operations needed with this type. 2. expose by service with LoadBalancer type If you don't want to use the HostNetwork , you can ask hack/remote-up-karmada.sh to expose karmada-apiserver by a service with LoadBalancer type that requires your cluster have deployed the Load Balancer . All you need to do is set an environment: export LOAD_BALANCER=true Install From the root directory the karmada repo, install Karmada by command: hack/remote-up-karmada.sh <kubeconfig> <context_name> kubeconfig is your cluster's kubeconfig that you want to install to context_name is the name of context in 'kubeconfig' For example: hack/remote-up-karmada.sh $HOME/.kube/config mycluster If everything goes well, at the end of the script output, you will see similar messages as follows: ------------------------------------------------------------------------------------------------------ \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2591 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 ------------------------------------------------------------------------------------------------------ Karmada is installed successfully. Kubeconfig for karmada in file: /root/.kube/karmada.config, so you can run: export KUBECONFIG=\"/root/.kube/karmada.config\" Or use kubectl with --kubeconfig=/root/.kube/karmada.config Please use 'kubectl config use-context karmada-apiserver' to switch the cluster of karmada control plane And use 'kubectl config use-context your-host' for debugging karmada installation","title":"Fromsource"},{"location":"installation/fromsource/#installing-karmada-on-cluster-from-source","text":"This document describes how you can use the hack/remote-up-karmada.sh script to install Karmada on your clusters based on the codebase.","title":"Installing Karmada on Cluster from Source"},{"location":"installation/fromsource/#select-a-way-to-expose-karmada-apiserver","text":"The hack/remote-up-karmada.sh will install karmada-apiserver and provide two ways to expose the server:","title":"Select a way to expose karmada-apiserver"},{"location":"installation/fromsource/#1-expose-by-hostnetwork-type","text":"By default, the hack/remote-up-karmada.sh will expose karmada-apiserver by HostNetwork . No extra operations needed with this type.","title":"1. expose by HostNetwork type"},{"location":"installation/fromsource/#2-expose-by-service-with-loadbalancer-type","text":"If you don't want to use the HostNetwork , you can ask hack/remote-up-karmada.sh to expose karmada-apiserver by a service with LoadBalancer type that requires your cluster have deployed the Load Balancer . All you need to do is set an environment: export LOAD_BALANCER=true","title":"2. expose by service with LoadBalancer type"},{"location":"installation/fromsource/#install","text":"From the root directory the karmada repo, install Karmada by command: hack/remote-up-karmada.sh <kubeconfig> <context_name> kubeconfig is your cluster's kubeconfig that you want to install to context_name is the name of context in 'kubeconfig' For example: hack/remote-up-karmada.sh $HOME/.kube/config mycluster If everything goes well, at the end of the script output, you will see similar messages as follows: ------------------------------------------------------------------------------------------------------ \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2591 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 ------------------------------------------------------------------------------------------------------ Karmada is installed successfully. Kubeconfig for karmada in file: /root/.kube/karmada.config, so you can run: export KUBECONFIG=\"/root/.kube/karmada.config\" Or use kubectl with --kubeconfig=/root/.kube/karmada.config Please use 'kubectl config use-context karmada-apiserver' to switch the cluster of karmada control plane And use 'kubectl config use-context your-host' for debugging karmada installation","title":"Install"},{"location":"installation/install-kubectl-karmada/","text":"kubectl-karmada Installation You can install kubectl-karmada plug-in in any of the following ways: Download from the release. Install using Krew. Build from source code. Prerequisites kubectl kubectl is the Kubernetes command line tool lets you control Kubernetes clusters. For installation instructions see installing kubectl . Download from the release Karmada provides kubectl-karmada plug-in download service since v0.9.0. You can choose proper plug-in version which fits your operator system form karmada release . Take v0.9.0 that working with linux-amd64 os as an example: wget https://github.com/karmada-io/karmada/releases/download/v0.9.0/kubectl-karmada-linux-amd64.tar.gz tar -zxf kubectl-karmada-linux-amd64.tar.gz Next, move kubectl-karmada executable file to PATH path, reference from Installing kubectl plugins . Install using Krew Krew is the plugin manager for kubectl command-line tool. Install and set up Krew on your machine. Then install kubectl-karmada plug-in: kubectl krew install karmada You can refer to Quickstart of Krew for more information. Build from source code Clone karmada repo and run make cmd from the repository: make kubectl-karmada Next, move the kubectl-karmada executable file under the _output folder in the project root directory to the PATH path.","title":"Install kubectl Karmada"},{"location":"installation/install-kubectl-karmada/#kubectl-karmada-installation","text":"You can install kubectl-karmada plug-in in any of the following ways: Download from the release. Install using Krew. Build from source code.","title":"kubectl-karmada Installation"},{"location":"installation/install-kubectl-karmada/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/install-kubectl-karmada/#kubectl","text":"kubectl is the Kubernetes command line tool lets you control Kubernetes clusters. For installation instructions see installing kubectl .","title":"kubectl"},{"location":"installation/install-kubectl-karmada/#download-from-the-release","text":"Karmada provides kubectl-karmada plug-in download service since v0.9.0. You can choose proper plug-in version which fits your operator system form karmada release . Take v0.9.0 that working with linux-amd64 os as an example: wget https://github.com/karmada-io/karmada/releases/download/v0.9.0/kubectl-karmada-linux-amd64.tar.gz tar -zxf kubectl-karmada-linux-amd64.tar.gz Next, move kubectl-karmada executable file to PATH path, reference from Installing kubectl plugins .","title":"Download from the release"},{"location":"installation/install-kubectl-karmada/#install-using-krew","text":"Krew is the plugin manager for kubectl command-line tool. Install and set up Krew on your machine. Then install kubectl-karmada plug-in: kubectl krew install karmada You can refer to Quickstart of Krew for more information.","title":"Install using Krew"},{"location":"installation/install-kubectl-karmada/#build-from-source-code","text":"Clone karmada repo and run make cmd from the repository: make kubectl-karmada Next, move the kubectl-karmada executable file under the _output folder in the project root directory to the PATH path.","title":"Build from source code"},{"location":"installation/installation/","text":"Table of Contents generated with DocToc Installing Karmada Prerequisites Karmada kubectl plugin Install Karmada by Karmada command-line tool Install Karmada on your own cluster Offline installation Deploy HA Install Karmada in Kind cluster Install Karmada by Helm Chart Deployment Install Karmada from source Install Karmada for development environment Installing Karmada Prerequisites Karmada kubectl plugin kubectl-karmada is the Karmada command-line tool that lets you control the Karmada control plane, it presents as the kubectl plugin . For installation instructions see installing kubectl-karmada . Install Karmada by Karmada command-line tool Install Karmada on your own cluster Assume you have put your cluster's kubeconfig file to $HOME/.kube/config or specify the path with KUBECONFIG environment variable. Otherwise, you should specify the configuration file by setting --kubeconfig flag to the following commands. Note: The init command is available from v1.0. Run the following command to install: kubectl karmada init It might take about 5 minutes and if everything goes well, you will see outputs similar to: I1216 07:37:45.862959 4256 cert.go:230] Generate ca certificate success. I1216 07:37:46.000798 4256 cert.go:230] Generate etcd-server certificate success. ... ... ------------------------------------------------------------------------------------------------------ \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2591 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 ------------------------------------------------------------------------------------------------------ Karmada is installed successfully. Register Kubernetes cluster to Karmada control plane. Register cluster with 'Push' mode Step 1: Use karmadactl join to register the cluster to Karmada control panel. --cluster-kubeconfig is members kubeconfig. (In karmada)~# MEMBER_CLUSTER_NAME=`cat ~/.kube/config | grep current-context | sed 's/: /\\n/g'| sed '1d'` (In karmada)~# karmadactl --kubeconfig /etc/karmada/karmada-apiserver.config join ${MEMBER_CLUSTER_NAME} --cluster-kubeconfig=$HOME/.kube/config Step 2: Show members of karmada (In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters Register cluster with 'Pull' mode Step 1: Send karmada kubeconfig and karmada-agent.yaml to member kubernetes (In karmada)~# scp /etc/karmada/karmada-apiserver.config /etc/karmada/karmada-agent.yaml {member kubernetes}:~ Step 2: Create karmada kubeconfig secret Notice: Cross-network, need to change the config server address. (In member kubernetes)~# kubectl create ns karmada-system (In member kubernetes)~# kubectl create secret generic karmada-kubeconfig --from-file=karmada-kubeconfig=/root/karmada-apiserver.config -n karmada-system Step 3: Create karmada agent (In member kubernetes)~# MEMBER_CLUSTER_NAME=\"demo\" (In member kubernetes)~# sed -i \"s/{member_cluster_name}/${MEMBER_CLUSTER_NAME}/g\" karmada-agent.yaml (In member kubernetes)~# kubectl apply -f karmada-agent.yaml Step 4: Show members of karmada (In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters The components of Karmada are installed in karmada-system namespace by default, you can get them by: kubectl get deployments -n karmada-system NAME READY UP-TO-DATE AVAILABLE AGE karmada-aggregated-apiserver 1/1 1 1 102s karmada-apiserver 1/1 1 1 2m34s karmada-controller-manager 1/1 1 1 116s karmada-scheduler 1/1 1 1 119s karmada-webhook 1/1 1 1 113s kube-controller-manager 1/1 1 1 2m3s And the karmada-etcd is installed as the StatefulSet , get it by: kubectl get statefulsets -n karmada-system NAME READY AGE etcd 1/1 28m The configuration file of Karmada will be created to /etc/karmada/karmada-apiserver.config by default. Offline installation When installing Karmada, the kubectl karmada init will download the APIs(CRD) from the Karmada official release page (e.g. https://github.com/karmada-io/karmada/releases/tag/v0.10.1 ) and load images from the official registry by default. If you want to install Karmada offline, maybe you have to specify the APIs tar file as well as the image. Use --crds flag to specify the CRD file. e.g. kubectl karmada init --crds /$HOME/crds.tar.gz The images of Karmada components could be specified, take karmada-controller-manager as an example: kubectl karmada init --karmada-controller-manager-image=example.registry.com/library/karmada-controller-manager:1.0 Deploy HA Use --karmada-apiserver-replicas and --etcd-replicas flags to specify the number of the replicas (defaults to 1 ). kubectl karmada init --karmada-apiserver-replicas 3 --etcd-replicas 3 Install Karmada in Kind cluster kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". It was primarily designed for testing Kubernetes itself, not for production. Create a cluster named host by hack/create-cluster.sh : hack/create-cluster.sh host $HOME/.kube/host.config Install Karmada v1.1.0 by command kubectl karmada init : kubectl karmada init --crds https://github.com/karmada-io/karmada/releases/download/v1.1.0/crds.tar.gz --kubeconfig=$HOME/.kube/host.config Check installed components: kubectl get pods -n karmada-system --kubeconfig=$HOME/.kube/host.config NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 2m55s karmada-aggregated-apiserver-84b45bf9b-n5gnk 1/1 Running 0 109s karmada-apiserver-6dc4cf6964-cz4jh 1/1 Running 0 2m40s karmada-controller-manager-556cf896bc-79sxz 1/1 Running 0 2m3s karmada-scheduler-7b9d8b5764-6n48j 1/1 Running 0 2m6s karmada-webhook-7cf7986866-m75jw 1/1 Running 0 2m kube-controller-manager-85c789dcfc-k89f8 1/1 Running 0 2m10s Install Karmada by Helm Chart Deployment Please refer to installing by Helm . Install Karmada from source Please refer to installing from source . Install Karmada for development environment If you want to try Karmada, we recommend that build a development environment by hack/local-up-karmada.sh which will do following tasks for you: - Start a Kubernetes cluster by kind to run the Karmada control plane, aka. the host cluster . - Build Karmada control plane components based on a current codebase. - Deploy Karmada control plane components on the host cluster . - Create member clusters and join Karmada. 1. Clone Karmada repo to your machine: git clone https://github.com/karmada-io/karmada or use your fork repo by replacing your GitHub ID : git clone https://github.com/<GitHub ID>/karmada 2. Change to the karmada directory: cd karmada 3. Deploy and run Karmada control plane: run the following script: hack/local-up-karmada.sh This script will do following tasks for you: - Start a Kubernetes cluster to run the Karmada control plane, aka. the host cluster . - Build Karmada control plane components based on a current codebase. - Deploy Karmada control plane components on the host cluster . - Create member clusters and join Karmada. If everything goes well, at the end of the script output, you will see similar messages as follows: Local Karmada is running. To start using your Karmada environment, run: export KUBECONFIG=\"$HOME/.kube/karmada.config\" Please use 'kubectl config use-context karmada-host/karmada-apiserver' to switch the host and control plane cluster. To manage your member clusters, run: export KUBECONFIG=\"$HOME/.kube/members.config\" Please use 'kubectl config use-context member1/member2/member3' to switch to the different member cluster. 4. Check registered cluster kubectl get clusters --kubeconfig=/$HOME/.kube/karmada.config You will get similar output as follows: NAME VERSION MODE READY AGE member1 v1.23.4 Push True 7m38s member2 v1.23.4 Push True 7m35s member3 v1.23.4 Pull True 7m27s There are 3 clusters named member1 , member2 and member3 have registered with Push or Pull mode.","title":"Installation"},{"location":"installation/installation/#installing-karmada","text":"","title":"Installing Karmada"},{"location":"installation/installation/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/installation/#karmada-kubectl-plugin","text":"kubectl-karmada is the Karmada command-line tool that lets you control the Karmada control plane, it presents as the kubectl plugin . For installation instructions see installing kubectl-karmada .","title":"Karmada kubectl plugin"},{"location":"installation/installation/#install-karmada-by-karmada-command-line-tool","text":"","title":"Install Karmada by Karmada command-line tool"},{"location":"installation/installation/#install-karmada-on-your-own-cluster","text":"Assume you have put your cluster's kubeconfig file to $HOME/.kube/config or specify the path with KUBECONFIG environment variable. Otherwise, you should specify the configuration file by setting --kubeconfig flag to the following commands. Note: The init command is available from v1.0. Run the following command to install: kubectl karmada init It might take about 5 minutes and if everything goes well, you will see outputs similar to: I1216 07:37:45.862959 4256 cert.go:230] Generate ca certificate success. I1216 07:37:46.000798 4256 cert.go:230] Generate etcd-server certificate success. ... ... ------------------------------------------------------------------------------------------------------ \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2588\u2588\u2588\u2588\u2588\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2591 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2591\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 \u2591\u2591\u2591\u2591\u2591 ------------------------------------------------------------------------------------------------------ Karmada is installed successfully. Register Kubernetes cluster to Karmada control plane. Register cluster with 'Push' mode Step 1: Use karmadactl join to register the cluster to Karmada control panel. --cluster-kubeconfig is members kubeconfig. (In karmada)~# MEMBER_CLUSTER_NAME=`cat ~/.kube/config | grep current-context | sed 's/: /\\n/g'| sed '1d'` (In karmada)~# karmadactl --kubeconfig /etc/karmada/karmada-apiserver.config join ${MEMBER_CLUSTER_NAME} --cluster-kubeconfig=$HOME/.kube/config Step 2: Show members of karmada (In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters Register cluster with 'Pull' mode Step 1: Send karmada kubeconfig and karmada-agent.yaml to member kubernetes (In karmada)~# scp /etc/karmada/karmada-apiserver.config /etc/karmada/karmada-agent.yaml {member kubernetes}:~ Step 2: Create karmada kubeconfig secret Notice: Cross-network, need to change the config server address. (In member kubernetes)~# kubectl create ns karmada-system (In member kubernetes)~# kubectl create secret generic karmada-kubeconfig --from-file=karmada-kubeconfig=/root/karmada-apiserver.config -n karmada-system Step 3: Create karmada agent (In member kubernetes)~# MEMBER_CLUSTER_NAME=\"demo\" (In member kubernetes)~# sed -i \"s/{member_cluster_name}/${MEMBER_CLUSTER_NAME}/g\" karmada-agent.yaml (In member kubernetes)~# kubectl apply -f karmada-agent.yaml Step 4: Show members of karmada (In karmada)~# kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get clusters The components of Karmada are installed in karmada-system namespace by default, you can get them by: kubectl get deployments -n karmada-system NAME READY UP-TO-DATE AVAILABLE AGE karmada-aggregated-apiserver 1/1 1 1 102s karmada-apiserver 1/1 1 1 2m34s karmada-controller-manager 1/1 1 1 116s karmada-scheduler 1/1 1 1 119s karmada-webhook 1/1 1 1 113s kube-controller-manager 1/1 1 1 2m3s And the karmada-etcd is installed as the StatefulSet , get it by: kubectl get statefulsets -n karmada-system NAME READY AGE etcd 1/1 28m The configuration file of Karmada will be created to /etc/karmada/karmada-apiserver.config by default.","title":"Install Karmada on your own cluster"},{"location":"installation/installation/#offline-installation","text":"When installing Karmada, the kubectl karmada init will download the APIs(CRD) from the Karmada official release page (e.g. https://github.com/karmada-io/karmada/releases/tag/v0.10.1 ) and load images from the official registry by default. If you want to install Karmada offline, maybe you have to specify the APIs tar file as well as the image. Use --crds flag to specify the CRD file. e.g. kubectl karmada init --crds /$HOME/crds.tar.gz The images of Karmada components could be specified, take karmada-controller-manager as an example: kubectl karmada init --karmada-controller-manager-image=example.registry.com/library/karmada-controller-manager:1.0","title":"Offline installation"},{"location":"installation/installation/#deploy-ha","text":"Use --karmada-apiserver-replicas and --etcd-replicas flags to specify the number of the replicas (defaults to 1 ). kubectl karmada init --karmada-apiserver-replicas 3 --etcd-replicas 3","title":"Deploy HA"},{"location":"installation/installation/#install-karmada-in-kind-cluster","text":"kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". It was primarily designed for testing Kubernetes itself, not for production. Create a cluster named host by hack/create-cluster.sh : hack/create-cluster.sh host $HOME/.kube/host.config Install Karmada v1.1.0 by command kubectl karmada init : kubectl karmada init --crds https://github.com/karmada-io/karmada/releases/download/v1.1.0/crds.tar.gz --kubeconfig=$HOME/.kube/host.config Check installed components: kubectl get pods -n karmada-system --kubeconfig=$HOME/.kube/host.config NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 2m55s karmada-aggregated-apiserver-84b45bf9b-n5gnk 1/1 Running 0 109s karmada-apiserver-6dc4cf6964-cz4jh 1/1 Running 0 2m40s karmada-controller-manager-556cf896bc-79sxz 1/1 Running 0 2m3s karmada-scheduler-7b9d8b5764-6n48j 1/1 Running 0 2m6s karmada-webhook-7cf7986866-m75jw 1/1 Running 0 2m kube-controller-manager-85c789dcfc-k89f8 1/1 Running 0 2m10s","title":"Install Karmada in Kind cluster"},{"location":"installation/installation/#install-karmada-by-helm-chart-deployment","text":"Please refer to installing by Helm .","title":"Install Karmada by Helm Chart Deployment"},{"location":"installation/installation/#install-karmada-from-source","text":"Please refer to installing from source .","title":"Install Karmada from source"},{"location":"installation/installation/#install-karmada-for-development-environment","text":"If you want to try Karmada, we recommend that build a development environment by hack/local-up-karmada.sh which will do following tasks for you: - Start a Kubernetes cluster by kind to run the Karmada control plane, aka. the host cluster . - Build Karmada control plane components based on a current codebase. - Deploy Karmada control plane components on the host cluster . - Create member clusters and join Karmada. 1. Clone Karmada repo to your machine: git clone https://github.com/karmada-io/karmada or use your fork repo by replacing your GitHub ID : git clone https://github.com/<GitHub ID>/karmada 2. Change to the karmada directory: cd karmada 3. Deploy and run Karmada control plane: run the following script: hack/local-up-karmada.sh This script will do following tasks for you: - Start a Kubernetes cluster to run the Karmada control plane, aka. the host cluster . - Build Karmada control plane components based on a current codebase. - Deploy Karmada control plane components on the host cluster . - Create member clusters and join Karmada. If everything goes well, at the end of the script output, you will see similar messages as follows: Local Karmada is running. To start using your Karmada environment, run: export KUBECONFIG=\"$HOME/.kube/karmada.config\" Please use 'kubectl config use-context karmada-host/karmada-apiserver' to switch the host and control plane cluster. To manage your member clusters, run: export KUBECONFIG=\"$HOME/.kube/members.config\" Please use 'kubectl config use-context member1/member2/member3' to switch to the different member cluster. 4. Check registered cluster kubectl get clusters --kubeconfig=/$HOME/.kube/karmada.config You will get similar output as follows: NAME VERSION MODE READY AGE member1 v1.23.4 Push True 7m38s member2 v1.23.4 Push True 7m35s member3 v1.23.4 Pull True 7m27s There are 3 clusters named member1 , member2 and member3 have registered with Push or Pull mode.","title":"Install Karmada for development environment"},{"location":"keps/cleanup-propagated-resources/","text":"Cleanup propagated resources Table of Contents Cleanup propagated resources Table of Contents Motivation Goals Proposals Implementation Details Best Effort Strategy Required Strategy Addition ClusterConditionTypes (optional) Unjoining ClusterConfigurationType Failed ClusterConfigurationType This document proposes a mechanism to specify that a member cluster resource should be removed from a managed cluster when leaving. Motivation When a cluster is unjoined, karmada should provide a mechanism to cleanup the resources propagated by karmada. Currently, when unjoin a cluster, Karamada first try to remove propagated resource, and will skip remove if the cluster not ready. Goals Define how users can indicate that propagated resource should be removed when a cluster leaves Define cluster removal strategies Proposals The Cluster struct should be updated to contain a RemoveStrategy member. RemoveStrategy will initially support two values, Needless and Required . The Needless strategy will not cleanup any propagated resources. The Required strategy will halt the unjoin process and set the Cluster resource in a failed state when encountering errors. Unjoining is blocked until all propagated resources have been removed successfully. By default, RemoveStrategy will be needless on all cluster. A user must explicitly set a removal strategy on the join cluster. Implementation details Update cluster to add a new attribute: RemoveStrategy *RemoveStrategy `json:\"removeStrategy,omitempty\"` ``` type RemoveStrategy string const ( RemoveStrategyNeedless RemoveStrategy = \"Needless\" RemoveStrategyRequired RemoveStrategy = \"Required\" ) ``` Add a flag remove-strategy to karmadactl join , and set the value to RemoveStrategy attribute During unjoin cluster, karmada should consider remove strategy In kubefed's propose, the author suggest add a BestEffort strategy, reviewers say we need a certain value, so we should not use it too. Karamada use the BestEffort strategy, currently. Needless Strategy Not need cleanup propagated resources when unjoining cluster. Karamada should use this strategy as default value, condsider the business risk. Required Strategy Clusters with the \"required\" removal strategy must remove execution namespaces successfully on unjoin. After removing the work resource, the client must verify that the propagated resource is no longer present. If a resource cannot be removed: UnjoinCluster should return a formatted error containing which work could not be removed A ClusterCondition should be added to the cluster status with a message indicating which resources could not be removed (optional) Set the ConditionType to Failed Execution Controller When cluster is unjoined, karmada should process by cluster remove strategy: Needless strategy ignore work delete event Required strategy return true if the work delete success, and set clustercondition if delete fail Additional ClusterConditionTypes (optional) Presently, there are only two ClusterConditionTypes, Unjoining and UnjoinFailed . Given the required remove strategy, a Cluster may end up in a state where the cluster is Ready but is in the process of \"Unjoining\". Adding additional conditions will make it easier to understand life cycle of cluster operator. status: conditions: - lastTransitionTime: \"2021-05-08T12:34:15Z\" lastUpdateTime: \"2021-05-08T12:34:30Z\" message: Unjoining resources. reason: UnjoinCluster status: \"True\" type: Unjoining Unjoining ClusterConditionType The Unjoining condition should be set on a cluster when exec karmadactl unjoin command. Failed ClusterConditionType In the event of an error, such as failure to remove a resource with a required remove strategy, the cluster should enter a UnjoinFailed condition.","title":"Cleanup Member Cluster Resources"},{"location":"keps/cleanup-propagated-resources/#cleanup-propagated-resources","text":"","title":"Cleanup propagated resources"},{"location":"keps/cleanup-propagated-resources/#table-of-contents","text":"Cleanup propagated resources Table of Contents Motivation Goals Proposals Implementation Details Best Effort Strategy Required Strategy Addition ClusterConditionTypes (optional) Unjoining ClusterConfigurationType Failed ClusterConfigurationType This document proposes a mechanism to specify that a member cluster resource should be removed from a managed cluster when leaving.","title":"Table of Contents"},{"location":"keps/cleanup-propagated-resources/#motivation","text":"When a cluster is unjoined, karmada should provide a mechanism to cleanup the resources propagated by karmada. Currently, when unjoin a cluster, Karamada first try to remove propagated resource, and will skip remove if the cluster not ready.","title":"Motivation"},{"location":"keps/cleanup-propagated-resources/#goals","text":"Define how users can indicate that propagated resource should be removed when a cluster leaves Define cluster removal strategies","title":"Goals"},{"location":"keps/cleanup-propagated-resources/#proposals","text":"The Cluster struct should be updated to contain a RemoveStrategy member. RemoveStrategy will initially support two values, Needless and Required . The Needless strategy will not cleanup any propagated resources. The Required strategy will halt the unjoin process and set the Cluster resource in a failed state when encountering errors. Unjoining is blocked until all propagated resources have been removed successfully. By default, RemoveStrategy will be needless on all cluster. A user must explicitly set a removal strategy on the join cluster.","title":"Proposals"},{"location":"keps/cleanup-propagated-resources/#implementation-details","text":"Update cluster to add a new attribute: RemoveStrategy *RemoveStrategy `json:\"removeStrategy,omitempty\"` ``` type RemoveStrategy string const ( RemoveStrategyNeedless RemoveStrategy = \"Needless\" RemoveStrategyRequired RemoveStrategy = \"Required\" ) ``` Add a flag remove-strategy to karmadactl join , and set the value to RemoveStrategy attribute During unjoin cluster, karmada should consider remove strategy In kubefed's propose, the author suggest add a BestEffort strategy, reviewers say we need a certain value, so we should not use it too. Karamada use the BestEffort strategy, currently.","title":"Implementation details"},{"location":"keps/cleanup-propagated-resources/#needless-strategy","text":"Not need cleanup propagated resources when unjoining cluster. Karamada should use this strategy as default value, condsider the business risk.","title":"Needless Strategy"},{"location":"keps/cleanup-propagated-resources/#required-strategy","text":"Clusters with the \"required\" removal strategy must remove execution namespaces successfully on unjoin. After removing the work resource, the client must verify that the propagated resource is no longer present. If a resource cannot be removed: UnjoinCluster should return a formatted error containing which work could not be removed A ClusterCondition should be added to the cluster status with a message indicating which resources could not be removed (optional) Set the ConditionType to Failed","title":"Required Strategy"},{"location":"keps/cleanup-propagated-resources/#execution-controller","text":"When cluster is unjoined, karmada should process by cluster remove strategy: Needless strategy ignore work delete event Required strategy return true if the work delete success, and set clustercondition if delete fail","title":"Execution Controller"},{"location":"keps/cleanup-propagated-resources/#additional-clusterconditiontypes-optional","text":"Presently, there are only two ClusterConditionTypes, Unjoining and UnjoinFailed . Given the required remove strategy, a Cluster may end up in a state where the cluster is Ready but is in the process of \"Unjoining\". Adding additional conditions will make it easier to understand life cycle of cluster operator. status: conditions: - lastTransitionTime: \"2021-05-08T12:34:15Z\" lastUpdateTime: \"2021-05-08T12:34:30Z\" message: Unjoining resources. reason: UnjoinCluster status: \"True\" type: Unjoining","title":"Additional ClusterConditionTypes (optional)"},{"location":"keps/cleanup-propagated-resources/#unjoining-clusterconditiontype","text":"The Unjoining condition should be set on a cluster when exec karmadactl unjoin command.","title":"Unjoining ClusterConditionType"},{"location":"keps/cleanup-propagated-resources/#failed-clusterconditiontype","text":"In the event of an error, such as failure to remove a resource with a required remove strategy, the cluster should enter a UnjoinFailed condition.","title":"Failed ClusterConditionType"},{"location":"proposals/caching/","text":"Caching member cluster resources for Karmada Summary Provides a caching layer for Karmada to cache (member clusters) Kubernetes resources. Motivation Goals Accelerates resource requests processing speed across regions Provides a cross-cluster resource view Compatible with (multi-cluster) multiple kubernetes resource versions Unified resource requests entries Reduces API server pressure of member clusters Non-Goals Proposal User Stories (Optional) Story 1 Imagine that we have a cross-region application that needs to run in multiple regions in order to provide service capabilities nearby. We achieve this by deploying a Kubernetes cluster in each region. Goals: Get the distribution of the application in different clusters. Get resource information across clusters through a unified endpoint. Accelerate processing speed of resource requests across regions. Get resources in multiple clusters by labels. Design Details Define the scope of the cached resource New ClusterCache API We propose a new CR in cluster.karmada.io group. package v1alpha1 import ( corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) //+kubebuilder:object:root=true //+kubebuilder:subresource:status // ClusterCache is the Schema for the cluster cache API type ClusterCache struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ClusterCacheSpec `json:\"spec,omitempty\"` Status ClusterCacheStatus `json:\"status,omitempty\"` } // ClusterCacheSpec defines the desired state of ClusterCache type ClusterCacheSpec struct { // ClusterSelectors represents the filter to select clusters. // +required ClusterSelectors []ClusterSelector `json:\"clusterSelectors\"` // ResourceSelectors used to select resources. // +required ResourceSelectors []ResourceSelector `json:\"resourceSelectors\"` // StatusUpdatePeriodSeconds is the period to update the status of the resource. // default is 10s. // +optional StatusUpdatePeriodSeconds uint32 `json:\"statusUpdatePeriodSeconds,omitempty\"` } // ClusterSelector represents the filter to select clusters. type ClusterSelector struct { // LabelSelector is a filter to select member clusters by labels. // If non-nil and non-empty, only the clusters match this filter will be selected. // +optional LabelSelector *metav1.LabelSelector `json:\"labelSelector,omitempty\"` // FieldSelector is a filter to select member clusters by fields. // If non-nil and non-empty, only the clusters match this filter will be selected. // +optional FieldSelector *FieldSelector `json:\"fieldSelector,omitempty\"` // ClusterNames is the list of clusters to be selected. // +optional ClusterNames []string `json:\"clusterNames,omitempty\"` // ExcludedClusters is the list of clusters to be ignored. // +optional ExcludeClusters []string `json:\"exclude,omitempty\"` } // FieldSelector is a field filter. type FieldSelector struct { // A list of field selector requirements. MatchExpressions []corev1.NodeSelectorRequirement `json:\"matchExpressions,omitempty\"` } // ResourceSelector the resources will be selected. type ResourceSelector struct { // APIVersion represents the API version of the target resources. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the target resources. // +required Kind string `json:\"kind\"` // Namespace of the target resource. // Default is empty, which means all namespaces. // +optional Namespace string `json:\"namespace,omitempty\"` } // ClusterCacheStatus defines the observed state of ClusterCache type ClusterCacheStatus struct { // +optional Resources []ResourceStatusRef `json:\"resources,omitempty\"` // +optional StartTime *metav1.Time `json:\"startTime,omitempty\"` } type ResourceStatusRef struct { // +required Cluster string `json:\"cluster\"` // +optional APIVersion string `json:\"apiVersion,omitempty\"` // +required Kind string `json:\"kind\"` // +optional Namespace string `json:\"namespace,omitempty\"` // +required State CachePhase `json:\"state\"` // +required TotalNum int32 `json:\"totalNum\"` // +required UpdateTime *metav1.Time `json:\"updateTime\"` } // CachePhase is the current state of the cache // +enum type CachePhase string // These are the valid statuses of cache. const ( CacheRunning CachePhase = \"Running\" CacheFailed CachePhase = \"Failed\" CacheUnknown CachePhase = \"Unknown\" ) type ResourceStateRef struct { // +required Phase CachePhase `json:\"phase\"` // +optional Reason string `json:\"reason\"` } //+kubebuilder:object:root=true // ClusterCacheList contains a list of ClusterCache type ClusterCacheList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []ClusterCache `json:\"items\"` } func init() { SchemeBuilder.Register(&ClusterCache{}, &ClusterCacheList{}) } Example The following example shows how to create a ClusterCache CRD. apiVersion: clustercaches.karmada.io/v1alpha1 kind: ClusterCache metadata: name: clustercache-sample spec: clusterSelectors: - clusterNames: - member1 - member2 - member3 resourceSelectors: - kind: Pod - kind: Ingress apiVersion: networking.k8s.io/v1 - kind: DaemonSet namespace: kube-system - kind: Deployment status: startTime: \"2020-05-01T00:00:00Z\" resources: - cluster: member1 kind: Pod totalNum: 700 state: phase: Running updateTime: \"2022-01-01T00:00:00Z\" - cluster: member1 kind: Ingress totalNum: 0 state: phase: Failed reason: the server doesn't have a resource type ingresses updateTime: \"2022-01-01T00:00:00Z\" Test Plan","title":"Caching member cluster resources for Karmada"},{"location":"proposals/caching/#caching-member-cluster-resources-for-karmada","text":"","title":"Caching member cluster resources for Karmada"},{"location":"proposals/caching/#summary","text":"Provides a caching layer for Karmada to cache (member clusters) Kubernetes resources.","title":"Summary"},{"location":"proposals/caching/#motivation","text":"","title":"Motivation"},{"location":"proposals/caching/#goals","text":"Accelerates resource requests processing speed across regions Provides a cross-cluster resource view Compatible with (multi-cluster) multiple kubernetes resource versions Unified resource requests entries Reduces API server pressure of member clusters","title":"Goals"},{"location":"proposals/caching/#non-goals","text":"","title":"Non-Goals"},{"location":"proposals/caching/#proposal","text":"","title":"Proposal"},{"location":"proposals/caching/#user-stories-optional","text":"","title":"User Stories (Optional)"},{"location":"proposals/caching/#story-1","text":"Imagine that we have a cross-region application that needs to run in multiple regions in order to provide service capabilities nearby. We achieve this by deploying a Kubernetes cluster in each region. Goals: Get the distribution of the application in different clusters. Get resource information across clusters through a unified endpoint. Accelerate processing speed of resource requests across regions. Get resources in multiple clusters by labels.","title":"Story 1"},{"location":"proposals/caching/#design-details","text":"","title":"Design Details"},{"location":"proposals/caching/#define-the-scope-of-the-cached-resource","text":"","title":"Define the scope of the cached resource"},{"location":"proposals/caching/#new-clustercache-api","text":"We propose a new CR in cluster.karmada.io group. package v1alpha1 import ( corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) //+kubebuilder:object:root=true //+kubebuilder:subresource:status // ClusterCache is the Schema for the cluster cache API type ClusterCache struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Spec ClusterCacheSpec `json:\"spec,omitempty\"` Status ClusterCacheStatus `json:\"status,omitempty\"` } // ClusterCacheSpec defines the desired state of ClusterCache type ClusterCacheSpec struct { // ClusterSelectors represents the filter to select clusters. // +required ClusterSelectors []ClusterSelector `json:\"clusterSelectors\"` // ResourceSelectors used to select resources. // +required ResourceSelectors []ResourceSelector `json:\"resourceSelectors\"` // StatusUpdatePeriodSeconds is the period to update the status of the resource. // default is 10s. // +optional StatusUpdatePeriodSeconds uint32 `json:\"statusUpdatePeriodSeconds,omitempty\"` } // ClusterSelector represents the filter to select clusters. type ClusterSelector struct { // LabelSelector is a filter to select member clusters by labels. // If non-nil and non-empty, only the clusters match this filter will be selected. // +optional LabelSelector *metav1.LabelSelector `json:\"labelSelector,omitempty\"` // FieldSelector is a filter to select member clusters by fields. // If non-nil and non-empty, only the clusters match this filter will be selected. // +optional FieldSelector *FieldSelector `json:\"fieldSelector,omitempty\"` // ClusterNames is the list of clusters to be selected. // +optional ClusterNames []string `json:\"clusterNames,omitempty\"` // ExcludedClusters is the list of clusters to be ignored. // +optional ExcludeClusters []string `json:\"exclude,omitempty\"` } // FieldSelector is a field filter. type FieldSelector struct { // A list of field selector requirements. MatchExpressions []corev1.NodeSelectorRequirement `json:\"matchExpressions,omitempty\"` } // ResourceSelector the resources will be selected. type ResourceSelector struct { // APIVersion represents the API version of the target resources. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the target resources. // +required Kind string `json:\"kind\"` // Namespace of the target resource. // Default is empty, which means all namespaces. // +optional Namespace string `json:\"namespace,omitempty\"` } // ClusterCacheStatus defines the observed state of ClusterCache type ClusterCacheStatus struct { // +optional Resources []ResourceStatusRef `json:\"resources,omitempty\"` // +optional StartTime *metav1.Time `json:\"startTime,omitempty\"` } type ResourceStatusRef struct { // +required Cluster string `json:\"cluster\"` // +optional APIVersion string `json:\"apiVersion,omitempty\"` // +required Kind string `json:\"kind\"` // +optional Namespace string `json:\"namespace,omitempty\"` // +required State CachePhase `json:\"state\"` // +required TotalNum int32 `json:\"totalNum\"` // +required UpdateTime *metav1.Time `json:\"updateTime\"` } // CachePhase is the current state of the cache // +enum type CachePhase string // These are the valid statuses of cache. const ( CacheRunning CachePhase = \"Running\" CacheFailed CachePhase = \"Failed\" CacheUnknown CachePhase = \"Unknown\" ) type ResourceStateRef struct { // +required Phase CachePhase `json:\"phase\"` // +optional Reason string `json:\"reason\"` } //+kubebuilder:object:root=true // ClusterCacheList contains a list of ClusterCache type ClusterCacheList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\"` Items []ClusterCache `json:\"items\"` } func init() { SchemeBuilder.Register(&ClusterCache{}, &ClusterCacheList{}) }","title":"New ClusterCache API"},{"location":"proposals/caching/#example","text":"The following example shows how to create a ClusterCache CRD. apiVersion: clustercaches.karmada.io/v1alpha1 kind: ClusterCache metadata: name: clustercache-sample spec: clusterSelectors: - clusterNames: - member1 - member2 - member3 resourceSelectors: - kind: Pod - kind: Ingress apiVersion: networking.k8s.io/v1 - kind: DaemonSet namespace: kube-system - kind: Deployment status: startTime: \"2020-05-01T00:00:00Z\" resources: - cluster: member1 kind: Pod totalNum: 700 state: phase: Running updateTime: \"2022-01-01T00:00:00Z\" - cluster: member1 kind: Ingress totalNum: 0 state: phase: Failed reason: the server doesn't have a resource type ingresses updateTime: \"2022-01-01T00:00:00Z\"","title":"Example"},{"location":"proposals/caching/#test-plan","text":"","title":"Test Plan"},{"location":"proposals/configurable-local-value-retention/","text":"Configurable Local Value Retention Summary For now, Karmada keeps watching the propagated resources in member clusters to ensure the resource is always in desire state. In many cases, the controllers running in member clusters will make changes to the resource, such as: - The Kubernetes will assign a clusterIP for Service with type ClusterIP . - The Kubernetes will assign a nodeName for Pod in the scheduling phase. When Karmada users make changes to the resource template, Karmada will update the resource against the member cluster, but before the update, Karmada should retain the changes made by member cluster controllers. Karmada has implemented different retain methods for common resources, such as Service , Pod , ServiceAccount , and so on. The retain feature works for most cases, but still has disadvantages: - The retain methods are built-in and users can't customize them. - No retain method for custom resources(declared by CustomResourceDefinition ). This proposal aims to provide a strategy to customize the retain methods for any kind of resource. Motivation Nowadays, Kubernetes has provided dozens of resources, even though we don't need to implement the retain method for each of them, but it is still a heavy burden to maintain, it's hard to meet different kinds of expectations. On the other hand, we can't define the retain method for custom resources as you even don't know the fields in it. Goals Provide a strategy to support customize retain methods for any kind of resources. Support overrides built-in retain methods of Kubernetes resources. Support customize retain method for custom resources(declared by CustomResourceDefinition). Provide a general mechanism to customize karmada controller behaviors. The mechanism can be reused by other customized requirements. Non-Goals Define specific retain methods for Kubernetes resources. Deprecate the built-in retain methods. We should maintain built-in retain for the well-known resources to simplify user configuration. Proposal User Stories As a user, I want to customize the built-in retain method because it can't fulfill my requirement. The built-in retain methods aim to provide a default retain method for well-known resources, they are suitable for most situations, but can't guarantee to meet all user's needs. On the other hand, we usually maintain built-in retain methods for a preferred version of the resource, such as Deployment , The apps/v1 retain method might not suitable for apps/v1beta1 . As a user, I want to customize the retain method for my CRD resources. Nowadays, it's getting pretty common that people extend Kubernetes by CRD, at the meanwhile people might implement their controllers and the controllers probably make changes to these CRs when reconciling the changes. In this case, people should define the retain method for the CR, otherwise, it will be a conflict when syncing changes make on the Karmada control plane.(Karmada update--> controller update them back --> Karmada update ... endless update loop) Notes/Constraints/Caveats (Optional) Risks and Mitigations Design Details New Config API We propose a new CR in config.karmada.io group. // Config represents the configuration of Karmada. type Config struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Spec represents the specification of the desired behavior of Karmada configuration. // +required Spec ConfigSpec `json:\"spec\"` } type ConfigSpec struct { // Retentions represents a group of customized retention methods. // +optional Retentions []LocalValueRetention `json:\"retentions,omitempty\"` } // LocalValueRetention represents a customized retention method for specific API resource. type LocalValueRetention struct { // APIVersion represents the API version of the target resources. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the target resources. // +required Kind string `json:\"kind\"` // Fields indicates the fields that should be retained. // Each field describes a field in JsonPath format. // +optional Fields []string `json:\"fields,omitempty\"` // RetentionLua holds the Lua script that is used to retain runtime values to the desired specification. // The script should implement a function just like: // // function Retain(desiredObj, runtimeObj) // desiredObj.spec.fieldFoo = runtimeObj.spec.fieldFoo // return desiredObj // end // // RetentionLua only holds the function body part, take the `desiredObj` and `runtimeObj` as the function parameters or // global parameters, and finally retains a retained specification. // +optional RetentionLua string `json:\"retentionLua,omitempty\"` } Each LocalValueRetention customizes one API resource referencing by APIVersion and Kind . The Fields holds a collection of fields that should be retained. The RetentionLua holds a fragment of Lua script which is used to implement the retention logic. Note: The Lua might not be the best choice, another considered approach is cue as mentioned by @pigletfly at this discussion , we have to do some investigation. But we also reserve the possibility that uses more than one script, people can select by themselves. Bundle well-known custom resources There are a lot of famous projects that defined CRD in the Kubernetes ecosystem, for these widely adopted CRDs, we can bundle them into Karmada, just like the built-in retain methods. This part of the design will continue later. Test Plan Propose E2E test cases according to user stories above: Test we can customize the built-in method Test we can customize custom resource Propose a tool that people can test the script. Alternatives","title":"Configurable Local Value Retention"},{"location":"proposals/configurable-local-value-retention/#configurable-local-value-retention","text":"","title":"Configurable Local Value Retention"},{"location":"proposals/configurable-local-value-retention/#summary","text":"For now, Karmada keeps watching the propagated resources in member clusters to ensure the resource is always in desire state. In many cases, the controllers running in member clusters will make changes to the resource, such as: - The Kubernetes will assign a clusterIP for Service with type ClusterIP . - The Kubernetes will assign a nodeName for Pod in the scheduling phase. When Karmada users make changes to the resource template, Karmada will update the resource against the member cluster, but before the update, Karmada should retain the changes made by member cluster controllers. Karmada has implemented different retain methods for common resources, such as Service , Pod , ServiceAccount , and so on. The retain feature works for most cases, but still has disadvantages: - The retain methods are built-in and users can't customize them. - No retain method for custom resources(declared by CustomResourceDefinition ). This proposal aims to provide a strategy to customize the retain methods for any kind of resource.","title":"Summary"},{"location":"proposals/configurable-local-value-retention/#motivation","text":"Nowadays, Kubernetes has provided dozens of resources, even though we don't need to implement the retain method for each of them, but it is still a heavy burden to maintain, it's hard to meet different kinds of expectations. On the other hand, we can't define the retain method for custom resources as you even don't know the fields in it.","title":"Motivation"},{"location":"proposals/configurable-local-value-retention/#goals","text":"Provide a strategy to support customize retain methods for any kind of resources. Support overrides built-in retain methods of Kubernetes resources. Support customize retain method for custom resources(declared by CustomResourceDefinition). Provide a general mechanism to customize karmada controller behaviors. The mechanism can be reused by other customized requirements.","title":"Goals"},{"location":"proposals/configurable-local-value-retention/#non-goals","text":"Define specific retain methods for Kubernetes resources. Deprecate the built-in retain methods. We should maintain built-in retain for the well-known resources to simplify user configuration.","title":"Non-Goals"},{"location":"proposals/configurable-local-value-retention/#proposal","text":"","title":"Proposal"},{"location":"proposals/configurable-local-value-retention/#user-stories","text":"","title":"User Stories"},{"location":"proposals/configurable-local-value-retention/#as-a-user-i-want-to-customize-the-built-in-retain-method-because-it-cant-fulfill-my-requirement","text":"The built-in retain methods aim to provide a default retain method for well-known resources, they are suitable for most situations, but can't guarantee to meet all user's needs. On the other hand, we usually maintain built-in retain methods for a preferred version of the resource, such as Deployment , The apps/v1 retain method might not suitable for apps/v1beta1 .","title":"As a user, I want to customize the built-in retain method because it can't fulfill my requirement."},{"location":"proposals/configurable-local-value-retention/#as-a-user-i-want-to-customize-the-retain-method-for-my-crd-resources","text":"Nowadays, it's getting pretty common that people extend Kubernetes by CRD, at the meanwhile people might implement their controllers and the controllers probably make changes to these CRs when reconciling the changes. In this case, people should define the retain method for the CR, otherwise, it will be a conflict when syncing changes make on the Karmada control plane.(Karmada update--> controller update them back --> Karmada update ... endless update loop)","title":"As a user, I want to customize the retain method for my CRD resources."},{"location":"proposals/configurable-local-value-retention/#notesconstraintscaveats-optional","text":"","title":"Notes/Constraints/Caveats (Optional)"},{"location":"proposals/configurable-local-value-retention/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"proposals/configurable-local-value-retention/#design-details","text":"","title":"Design Details"},{"location":"proposals/configurable-local-value-retention/#new-config-api","text":"We propose a new CR in config.karmada.io group. // Config represents the configuration of Karmada. type Config struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Spec represents the specification of the desired behavior of Karmada configuration. // +required Spec ConfigSpec `json:\"spec\"` } type ConfigSpec struct { // Retentions represents a group of customized retention methods. // +optional Retentions []LocalValueRetention `json:\"retentions,omitempty\"` } // LocalValueRetention represents a customized retention method for specific API resource. type LocalValueRetention struct { // APIVersion represents the API version of the target resources. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the target resources. // +required Kind string `json:\"kind\"` // Fields indicates the fields that should be retained. // Each field describes a field in JsonPath format. // +optional Fields []string `json:\"fields,omitempty\"` // RetentionLua holds the Lua script that is used to retain runtime values to the desired specification. // The script should implement a function just like: // // function Retain(desiredObj, runtimeObj) // desiredObj.spec.fieldFoo = runtimeObj.spec.fieldFoo // return desiredObj // end // // RetentionLua only holds the function body part, take the `desiredObj` and `runtimeObj` as the function parameters or // global parameters, and finally retains a retained specification. // +optional RetentionLua string `json:\"retentionLua,omitempty\"` } Each LocalValueRetention customizes one API resource referencing by APIVersion and Kind . The Fields holds a collection of fields that should be retained. The RetentionLua holds a fragment of Lua script which is used to implement the retention logic. Note: The Lua might not be the best choice, another considered approach is cue as mentioned by @pigletfly at this discussion , we have to do some investigation. But we also reserve the possibility that uses more than one script, people can select by themselves.","title":"New Config API"},{"location":"proposals/configurable-local-value-retention/#bundle-well-known-custom-resources","text":"There are a lot of famous projects that defined CRD in the Kubernetes ecosystem, for these widely adopted CRDs, we can bundle them into Karmada, just like the built-in retain methods. This part of the design will continue later.","title":"Bundle well-known custom resources"},{"location":"proposals/configurable-local-value-retention/#test-plan","text":"Propose E2E test cases according to user stories above: Test we can customize the built-in method Test we can customize custom resource Propose a tool that people can test the script.","title":"Test Plan"},{"location":"proposals/configurable-local-value-retention/#alternatives","text":"","title":"Alternatives"},{"location":"proposals/dependencies-automatically-propagation/","text":"Dependencies Automatically Propagation Summary Secrets or ConfigMaps can be mounted as data volumes or exposed as environment variables to be used by containers in a Pod. Meanwhile, users can expose the application running on a set of Pods by Service. Thus, it is necessary to propagate the relevant resource into the same member clusters when deploying a Pod by Karmada, i.e, Pod dependencies(ConfigMaps and Secrets) and dependents(Services and Ingresses) should \"follow\" the delegate pods. Usually, users don't create Pods directly. Instead, create them using workload resources such as Deployment, StatefulSet, or Job. So I take Deployment as an example. The dependencies of deployment include ConfigMaps, Secrets, PVCs, Services, etc. Till now, Karmada provides two ways to propagate dependent resources for a Deployment: - All dependent resources share the same propagation policy with the Deployment. - Create individual propagation policy for each dependent resource, but users need to make sure the resource scheduled to needed clusters. These methods work for most cases, but still has disadvantages: - For the first method, the dependencies(like configmaps and secrets) will be bound with one Deployment which means it can't be mounted by other Deployments. - For the second method, users should keep watching the scheduling result of the Deployment. This proposal aims to provide a strategy to intelligently propagate dependencies for workload resources such as Deployment. Motivation Users need to deploy dependent resources manually when deploying Pods or workload resources by Karmada. It is a heavy burden to maintain target clusters for dependent resources. Goals Provide a strategy to automatically propagate dependent resources of workload resources to needed clusters. Non-Goals Deprecate the default propagate process for dependent resources. Relevant resources like ConfigMap, Secret, and Service can be propagated by propagation policy as usual. Proposal This proposal introduces a new controller to intelligently propagate dependent resources. When the user propagates a deployment, the corresponding Karmada Controllers will propagate the dependencies. This proposal is divided into several steps, see below: PropagationPolicy API changes to add PropagateDeps . ResourceBinding API changes to add PropagateDeps and RequiredBy . New controllers to intelligently propagate dependent resources(named dependencies distributor). New ResourceInterpreter to parse the dependent resources of the given object. Associated docs and architecture diagram as a supplement. User Stories Story 1 Imagine that user creates a deployment and a PropagationPolicy, and the PropagateDeps is True . - Condition: - The deployment references a ConfigMap and a Secret. - Result: - When the referred ConfigMap and Secret are created, they will be propagated to the same clusters as the deployment. Design Details For better illustration, I classify resource binding into two categories, independent resource binding, and attached resource binding. An independent resource binding will be created when a resource is matched by a propagation policy. Attached resource bindings will be created when the controller propagates dependent resources. Here's the architecture design diagram. Note: - To enable auto-propagating dependencies, users need to turn on the PropagateDeps feature gate in karmada-controller-manager by --feature-gates=PropagateDeps=true - Each dependent resource has an individual attached resource binding. - The controller parses dependencies from the raw resource template, which means the dependencies introduced by the override policy can't be identified. API Changes We propose to extend the PropagationPolicy and ResourceBinding API. (The ClusterPropagationPolicy and ClusterResourceBining should be extended accordingly.) // PropagationSpec represents the desired behavior of PropagationPolicy. type PropagationSpec struct { // PropagateDeps tells if relevant resources should be propagated automatically. // Take 'Deployment' which referencing 'ConfigMap' and 'Secret' as an example, when 'propagateDeps' is 'true', // the referencing resources could be omitted(for saving config effort) from 'resourceSelectors' as they will be // propagated along with the Deployment. In addition to the propagating process, the referencing resources will be // migrated along with the Deployment in the fail-over scenario. // // Defaults to false. // +optional PropagateDeps bool `json:\"propagateDeps,omitempty\"` } // ResourceBindingSpec represents the expectation of ResourceBinding. type ResourceBindingSpec struct { // PropagateDeps tells if relevant resources should be propagated automatically. // It is inherited from PropagationPolicy or ClusterPropagationPolicy. // default false. // +optional PropagateDeps bool `json:\"propagateDeps,omitempty\"` // RequiredBy represents the list of Bindings that depend on the referencing resource. // +optional RequiredBy []BindingSnapshot `json:\"requiredBy,omitempty\"` } // BindingSnapshot is a snapshot of a ResourceBinding or ClusterResourceBinding. type BindingSnapshot struct { // Namespace represents the namespace of the Binding. // It is required for ResourceBinding. // If Namespace is not specified, means the referencing is ClusterResourceBinding. // +optional Namespace string `json:\"namespace,omitempty\"` // Name represents the name of the Binding. // +required Name string `json:\"name\"` // Clusters represents the scheduled result. // +optional Clusters []TargetCluster `json:\"clusters,omitempty\"` } Example Suppose we create a deployment named myapp which references configmap. apiVersion: apps/v1 kind: Deployment metadata: name: myapp labels: app: myapp spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - image: nginx name: nginx volumeMounts: - name: configmap mountPath: \"/configmap\" volumes: - name: configmap configMap: name: my-config Creating a propagation policy to propagate the deployment to specific clusters. To enable auto-propagating dependencies, we need to set propagateDeps as ture . apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: myapp-propagation spec: propagateDeps: true resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: myapp placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaSchedulingType: Duplicated Then creating the reference configmap. apiVersion: v1 kind: ConfigMap metadata: name: my-config data: nginx.properties: | proxy-connect-timeout: \"10s\" proxy-read-timeout: \"10s\" client-max-body-size: \"2m\" The resource binding of my-config will be created by dependencies distributor . And the spec field will be: spec: requiredBy: - clusters: - name: member1 replicas: 1 - name: member2 replicas: 1 name: myapp-deployment namespace: default resource: apiVersion: v1 kind: ConfigMap name: my-config namespace: default resourceVersion: \"757297\" Test Plan Propose E2E test cases according to user stories above: Test if the dependent resources propagated to needed clusters. Alternatives","title":"Dependencies Automatically Propagation"},{"location":"proposals/dependencies-automatically-propagation/#dependencies-automatically-propagation","text":"","title":"Dependencies Automatically Propagation"},{"location":"proposals/dependencies-automatically-propagation/#summary","text":"Secrets or ConfigMaps can be mounted as data volumes or exposed as environment variables to be used by containers in a Pod. Meanwhile, users can expose the application running on a set of Pods by Service. Thus, it is necessary to propagate the relevant resource into the same member clusters when deploying a Pod by Karmada, i.e, Pod dependencies(ConfigMaps and Secrets) and dependents(Services and Ingresses) should \"follow\" the delegate pods. Usually, users don't create Pods directly. Instead, create them using workload resources such as Deployment, StatefulSet, or Job. So I take Deployment as an example. The dependencies of deployment include ConfigMaps, Secrets, PVCs, Services, etc. Till now, Karmada provides two ways to propagate dependent resources for a Deployment: - All dependent resources share the same propagation policy with the Deployment. - Create individual propagation policy for each dependent resource, but users need to make sure the resource scheduled to needed clusters. These methods work for most cases, but still has disadvantages: - For the first method, the dependencies(like configmaps and secrets) will be bound with one Deployment which means it can't be mounted by other Deployments. - For the second method, users should keep watching the scheduling result of the Deployment. This proposal aims to provide a strategy to intelligently propagate dependencies for workload resources such as Deployment.","title":"Summary"},{"location":"proposals/dependencies-automatically-propagation/#motivation","text":"Users need to deploy dependent resources manually when deploying Pods or workload resources by Karmada. It is a heavy burden to maintain target clusters for dependent resources.","title":"Motivation"},{"location":"proposals/dependencies-automatically-propagation/#goals","text":"Provide a strategy to automatically propagate dependent resources of workload resources to needed clusters.","title":"Goals"},{"location":"proposals/dependencies-automatically-propagation/#non-goals","text":"Deprecate the default propagate process for dependent resources. Relevant resources like ConfigMap, Secret, and Service can be propagated by propagation policy as usual.","title":"Non-Goals"},{"location":"proposals/dependencies-automatically-propagation/#proposal","text":"This proposal introduces a new controller to intelligently propagate dependent resources. When the user propagates a deployment, the corresponding Karmada Controllers will propagate the dependencies. This proposal is divided into several steps, see below: PropagationPolicy API changes to add PropagateDeps . ResourceBinding API changes to add PropagateDeps and RequiredBy . New controllers to intelligently propagate dependent resources(named dependencies distributor). New ResourceInterpreter to parse the dependent resources of the given object. Associated docs and architecture diagram as a supplement.","title":"Proposal"},{"location":"proposals/dependencies-automatically-propagation/#user-stories","text":"","title":"User Stories"},{"location":"proposals/dependencies-automatically-propagation/#story-1","text":"Imagine that user creates a deployment and a PropagationPolicy, and the PropagateDeps is True . - Condition: - The deployment references a ConfigMap and a Secret. - Result: - When the referred ConfigMap and Secret are created, they will be propagated to the same clusters as the deployment.","title":"Story 1"},{"location":"proposals/dependencies-automatically-propagation/#design-details","text":"For better illustration, I classify resource binding into two categories, independent resource binding, and attached resource binding. An independent resource binding will be created when a resource is matched by a propagation policy. Attached resource bindings will be created when the controller propagates dependent resources. Here's the architecture design diagram. Note: - To enable auto-propagating dependencies, users need to turn on the PropagateDeps feature gate in karmada-controller-manager by --feature-gates=PropagateDeps=true - Each dependent resource has an individual attached resource binding. - The controller parses dependencies from the raw resource template, which means the dependencies introduced by the override policy can't be identified.","title":"Design Details"},{"location":"proposals/dependencies-automatically-propagation/#api-changes","text":"We propose to extend the PropagationPolicy and ResourceBinding API. (The ClusterPropagationPolicy and ClusterResourceBining should be extended accordingly.) // PropagationSpec represents the desired behavior of PropagationPolicy. type PropagationSpec struct { // PropagateDeps tells if relevant resources should be propagated automatically. // Take 'Deployment' which referencing 'ConfigMap' and 'Secret' as an example, when 'propagateDeps' is 'true', // the referencing resources could be omitted(for saving config effort) from 'resourceSelectors' as they will be // propagated along with the Deployment. In addition to the propagating process, the referencing resources will be // migrated along with the Deployment in the fail-over scenario. // // Defaults to false. // +optional PropagateDeps bool `json:\"propagateDeps,omitempty\"` } // ResourceBindingSpec represents the expectation of ResourceBinding. type ResourceBindingSpec struct { // PropagateDeps tells if relevant resources should be propagated automatically. // It is inherited from PropagationPolicy or ClusterPropagationPolicy. // default false. // +optional PropagateDeps bool `json:\"propagateDeps,omitempty\"` // RequiredBy represents the list of Bindings that depend on the referencing resource. // +optional RequiredBy []BindingSnapshot `json:\"requiredBy,omitempty\"` } // BindingSnapshot is a snapshot of a ResourceBinding or ClusterResourceBinding. type BindingSnapshot struct { // Namespace represents the namespace of the Binding. // It is required for ResourceBinding. // If Namespace is not specified, means the referencing is ClusterResourceBinding. // +optional Namespace string `json:\"namespace,omitempty\"` // Name represents the name of the Binding. // +required Name string `json:\"name\"` // Clusters represents the scheduled result. // +optional Clusters []TargetCluster `json:\"clusters,omitempty\"` }","title":"API Changes"},{"location":"proposals/dependencies-automatically-propagation/#example","text":"Suppose we create a deployment named myapp which references configmap. apiVersion: apps/v1 kind: Deployment metadata: name: myapp labels: app: myapp spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - image: nginx name: nginx volumeMounts: - name: configmap mountPath: \"/configmap\" volumes: - name: configmap configMap: name: my-config Creating a propagation policy to propagate the deployment to specific clusters. To enable auto-propagating dependencies, we need to set propagateDeps as ture . apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: myapp-propagation spec: propagateDeps: true resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: myapp placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaSchedulingType: Duplicated Then creating the reference configmap. apiVersion: v1 kind: ConfigMap metadata: name: my-config data: nginx.properties: | proxy-connect-timeout: \"10s\" proxy-read-timeout: \"10s\" client-max-body-size: \"2m\" The resource binding of my-config will be created by dependencies distributor . And the spec field will be: spec: requiredBy: - clusters: - name: member1 replicas: 1 - name: member2 replicas: 1 name: myapp-deployment namespace: default resource: apiVersion: v1 kind: ConfigMap name: my-config namespace: default resourceVersion: \"757297\"","title":"Example"},{"location":"proposals/dependencies-automatically-propagation/#test-plan","text":"Propose E2E test cases according to user stories above: Test if the dependent resources propagated to needed clusters.","title":"Test Plan"},{"location":"proposals/dependencies-automatically-propagation/#alternatives","text":"","title":"Alternatives"},{"location":"proposals/hpa/federated-hpa/","text":"Federated HPA Summary HPA is a widely used approach to improve the stability of apps dealing with burst of request and resource utility. We can run our apps on multiple clusters as easy as a single cluster with Karmada, but Karmada lacks the HPA support now. This proposal introduces a new component FederatedHPA to Karmada and enables Karmada to autoscale workloads across clusters. It also tries to minimize the differences of the user experience between HPA on a single cluster. Motivation Goals Bring HPA from single cluster to multiple clusters. Compatible with the HPA related resources in the single cluster. Tolerate the disaster of member cluster or karmada control plane. It is better to integrate well with the scenarios such as workloads shifting and cloud burst. It is better to support both Kubernetes HPA and customized HPA. Non-Goals Deal the workloads with different subset of services. Proposal User Stories (Optional) Story 1 For a platform developer using Kubernetes, now I want to use Karmada to run apps on multiclusters. But the CD ecosystem is built based on the single cluster and the original HPA is heavilly used. So I want to migrate the HPA resources to multiclusters without too much efforts. It is better to be compatible with the schema of HPA used in single cluster. Story 2 For an application developer, I create a HPA CR for the application running on Karmada with FederatedHPA enabled. target cpu util 30% min replica 3 max replica 100 Suddenly, one of the member clusters which my application running on stops working and can't scale up new pods. Unfortunately, a request burst is comming into the application. The CPU util of pods becomes higher than 30%. It will need 100 Pods totally to take the request burst. I hope the Karmada FederatedHPA can scale up new pods in other healthy clusters. Story 3 As an administrator of the Karmada&Kubernetes platform, I receive an alert that the Karmada control plane stops working and any requests to the Karmada control plane are failed. There are many applications running on the platform heavilly depend on the HPA to handle the unpredictable burst of requests. The chance of RCA occurred becomes really high if the system can't tolerate the failure of federation control plane. So I hope the Karmada FederatedHPA can scale in the member clusters even if the Karmada control plane is down. Notes/Constraints/Caveats (Optional) The workloads/pods in different member clusters selected by the same HPA CR/resource share the load of the application equally. For example, 10 pods of the application are spread into two member clusters with distribution cluster1: 3 pods, cluster2: 7 pods , so the 3 pods in cluster1 take 3/10 of total requests and 7 pods in cluster2 take 7/10 of total requests. Scenarios don't meet the restriction are not considered in this proposal. Risks and Mitigations Design Details Architecture There are no new CRDs or resources introduced in this design. All the core functions are implemented in the FederatedHPAController . 1. The Kubernetes HPA components are still used in the member cluster and can work standalone. 1. The FederatedHPAController is responsible for the purposes 1. Watch the HPA resource and PropagationPolicy/ResourceBinding corresponding to the Workload , to learn which clusters the HPA resource should propagated to and what weight the workloads should be spread between clusters. 1. Create Work corresponding to HPA resource to spread the HPA to clusters. Distribute min/max fields of the HPA resources between member clusters based on the weight learned. 1. Redistribute 'some' fields of the HPA resources after PropagationPolicy/ResourceBinding corresponding to Workload is changed. 1. There will be ResourceInterpreterWebhook s provided for different types of Workload . They are responsible for retaining the replicas in the member clusters and aggregate statuses. How FederatedHPAController learn the propagation information corresponding to the Workload When a new HPA resource created or changed, the FederatedHPAController should know the propagation and weight information of the corresponding Workload . How does the FederatedHPAController know it? The FederatedHPAController will easily find the corresponding Workload based on the field ScaleTargetRef and then will find the PropagationPolicy resource based on the matching of ResourceSelectors . For the weight information, because the karmada scheduler already plays the role to schedule the replicas, the FederatedHPAController can simplely reuse the scheduling result to learn the weight. The HPAController in the member cluster scale the Workload in the member cluster directly, it will conflict between the karmada scheduler. We can retain replicas in the member cluster by using feature resource interpreter webhook . How to deal with the spec.replicas of the Workload in the control plane In the original Kubernetes HPA and Workload(Deployment, for example), HPA scale the workload through the scale subresource of Workload. Then, the field replicas will be modified to the desired number. But in this design, HPAController in the member cluster work standalone and don't scale workloads through control plane so that the actual number of pods in the member clusters don't match the spec.replicas of Workload in the control plane. This mismatch would cause incident when spec.replicas in control plane is much smaller than in member clusters and user delete the HPA resource. To solve this problem, FederatedHPAController can collect the sum of spec.replicas values from member clusters and set it to the scale subresource of Workload in the control plane. Even the spec.replicas of Workload in the control plane matches the actual total replicas in the member clusers, every time the spec.replicas of Workload in the control plane is modified, the replicas distribution in the Work re-calculated by Karmada scheduler most probably don't match the actual distribution in the member clusters. The mismatch also would cause incident mentioned above. To solve this problem, we can split it into two sub-problems * How to gracefully shift workload in member clusters when desired distribution calculated by the karmada scheduler and actual distribution among member clusters differ substaintially. This may caused by modification of the PropagationPolicy or the remove of HPA resources controlled by the FederatedHPAController. Without the graceful shifting progress, the service may get out of capacity. In the future, features such as Federated Pod Disruption Budget may are needed to solve the problem here. * How to control the difference between the actual distribution in member clusters and the desired state the karmada scheduler calculated even the FederatedHPA is enabled. But this problem will not be too critical if the first problem is solved. It is better to solve the first sub-problem in another proposal. So we will leave this problem until the first one is solved. How to deal with the situation that spec.replicas is 1 The workload can be spread into multiple member clusters when the spec.replicas in the control plane is greater than 1 . The disaster of one member cluster and control plane can be tolerated because the workload can be scaled in other member clusters. But if the spec.replicas is 1 , the workload and HPA resource would only be spread into one member cluster. If the member cluster and control plane are out of service in the same time, the workload can't be scaled. How to integrate with and migrate from existing HPA resources For some scenarios, people may want a friendly mechanism to control what HPA resources can be controled by FederatedHPAController 1. There are already many HPA resources in the control plane managed by PropagationPolicy and OverridePolicy before the Karmada support FederatedHPA natively. For some risk concerns, the administrator of the platform wants to migrate these HPA resources to be managed by the FederatedHPAController step by step. 1. There are already many HPA resources in the control plane managed by PropagationPolicy and OverridePolicy before the Karmada support FederatedHPA natively. But in the same Karmada control plane, some users want to use the native FederatedHPA but others want to remain the old ways. The FederatedHPA should not conflict with the HPA resources managed by the old ways. To meet the requirements of above scenarios mentioned, a label federatedhpa.karmada.io/enabled=true/false for HPA resources will be introduced. How user stories are addressed Story1 Suppose platform administrator create a ClusterPropagationPolicy as a global default propagation for the Workload resources inside namespace apps in advance. According to the default ClusterPropagationPolicy , the weight between clustera and clusterb should be 4:1 . unfold me to see the yaml apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: default spec: placement: clusterAffinity: clusterNames: - clustera - clusterb replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - clustera weight: 4 - targetCluster: clusterNames: - clusterb weight: 1 resourceSelectors: - apiVersion: workload.example.io/v1alpha1 kind: Workload In the term of user story 1, suppose a user create a Workload and an HPA in the apps namespace unfold me to see the yaml apiVersion: workload.example.io/v1alpha1 kind: Workload metadata: name: nginx namespace: apps labels: app: nginx spec: replicas: 5 paused: false template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ------------------ apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nginx namespace: apps spec: scaleTargetRef: apiVersion: workload.example.io/v1alpha1 kind: Workload name: nginx minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 Then, ClusterResourceBinding resources will be created and scheduled by the Karmada scheduler unfold me to see the yaml apiVersion: work.karmada.io/v1alpha2 kind: ClusterResourceBinding metadata: name: xxx spec: resource: apiVersion: workload.example.io/v1alpha1 kind: Workload name: nginx ... clusters: - name: clustera replicas: 4 - name: clusterb replicas: 1 The FederatedHPAController continuously watchs the events of HPA and Karmada relevant resources( ClusterPropagationPolicy/PropagationPolicy or ClusterResourceBinding/ResourceBinding ) to learn * Which clusters the HPA resources should be propagated to * What weight the workload should be spreaded to clusters. The weight will be used to spread the min/max of HPA to clusters Then, FederatedHPAController create/update Work resources for HPA resource. Finally, the HPA resource will be spread to clustera and clusterb . The min/max of HPA resource in clustera and clusterb will be 1/8 and 1/2 . High Availability Integration Test Plan Alternatives","title":"Federated HPA"},{"location":"proposals/hpa/federated-hpa/#federated-hpa","text":"","title":"Federated HPA"},{"location":"proposals/hpa/federated-hpa/#summary","text":"HPA is a widely used approach to improve the stability of apps dealing with burst of request and resource utility. We can run our apps on multiple clusters as easy as a single cluster with Karmada, but Karmada lacks the HPA support now. This proposal introduces a new component FederatedHPA to Karmada and enables Karmada to autoscale workloads across clusters. It also tries to minimize the differences of the user experience between HPA on a single cluster.","title":"Summary"},{"location":"proposals/hpa/federated-hpa/#motivation","text":"","title":"Motivation"},{"location":"proposals/hpa/federated-hpa/#goals","text":"Bring HPA from single cluster to multiple clusters. Compatible with the HPA related resources in the single cluster. Tolerate the disaster of member cluster or karmada control plane. It is better to integrate well with the scenarios such as workloads shifting and cloud burst. It is better to support both Kubernetes HPA and customized HPA.","title":"Goals"},{"location":"proposals/hpa/federated-hpa/#non-goals","text":"Deal the workloads with different subset of services.","title":"Non-Goals"},{"location":"proposals/hpa/federated-hpa/#proposal","text":"","title":"Proposal"},{"location":"proposals/hpa/federated-hpa/#user-stories-optional","text":"","title":"User Stories (Optional)"},{"location":"proposals/hpa/federated-hpa/#story-1","text":"For a platform developer using Kubernetes, now I want to use Karmada to run apps on multiclusters. But the CD ecosystem is built based on the single cluster and the original HPA is heavilly used. So I want to migrate the HPA resources to multiclusters without too much efforts. It is better to be compatible with the schema of HPA used in single cluster.","title":"Story 1"},{"location":"proposals/hpa/federated-hpa/#story-2","text":"For an application developer, I create a HPA CR for the application running on Karmada with FederatedHPA enabled. target cpu util 30% min replica 3 max replica 100 Suddenly, one of the member clusters which my application running on stops working and can't scale up new pods. Unfortunately, a request burst is comming into the application. The CPU util of pods becomes higher than 30%. It will need 100 Pods totally to take the request burst. I hope the Karmada FederatedHPA can scale up new pods in other healthy clusters.","title":"Story 2"},{"location":"proposals/hpa/federated-hpa/#story-3","text":"As an administrator of the Karmada&Kubernetes platform, I receive an alert that the Karmada control plane stops working and any requests to the Karmada control plane are failed. There are many applications running on the platform heavilly depend on the HPA to handle the unpredictable burst of requests. The chance of RCA occurred becomes really high if the system can't tolerate the failure of federation control plane. So I hope the Karmada FederatedHPA can scale in the member clusters even if the Karmada control plane is down.","title":"Story 3"},{"location":"proposals/hpa/federated-hpa/#notesconstraintscaveats-optional","text":"The workloads/pods in different member clusters selected by the same HPA CR/resource share the load of the application equally. For example, 10 pods of the application are spread into two member clusters with distribution cluster1: 3 pods, cluster2: 7 pods , so the 3 pods in cluster1 take 3/10 of total requests and 7 pods in cluster2 take 7/10 of total requests. Scenarios don't meet the restriction are not considered in this proposal.","title":"Notes/Constraints/Caveats (Optional)"},{"location":"proposals/hpa/federated-hpa/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"proposals/hpa/federated-hpa/#design-details","text":"","title":"Design Details"},{"location":"proposals/hpa/federated-hpa/#architecture","text":"There are no new CRDs or resources introduced in this design. All the core functions are implemented in the FederatedHPAController . 1. The Kubernetes HPA components are still used in the member cluster and can work standalone. 1. The FederatedHPAController is responsible for the purposes 1. Watch the HPA resource and PropagationPolicy/ResourceBinding corresponding to the Workload , to learn which clusters the HPA resource should propagated to and what weight the workloads should be spread between clusters. 1. Create Work corresponding to HPA resource to spread the HPA to clusters. Distribute min/max fields of the HPA resources between member clusters based on the weight learned. 1. Redistribute 'some' fields of the HPA resources after PropagationPolicy/ResourceBinding corresponding to Workload is changed. 1. There will be ResourceInterpreterWebhook s provided for different types of Workload . They are responsible for retaining the replicas in the member clusters and aggregate statuses.","title":"Architecture"},{"location":"proposals/hpa/federated-hpa/#how-federatedhpacontroller-learn-the-propagation-information-corresponding-to-the-workload","text":"When a new HPA resource created or changed, the FederatedHPAController should know the propagation and weight information of the corresponding Workload . How does the FederatedHPAController know it? The FederatedHPAController will easily find the corresponding Workload based on the field ScaleTargetRef and then will find the PropagationPolicy resource based on the matching of ResourceSelectors . For the weight information, because the karmada scheduler already plays the role to schedule the replicas, the FederatedHPAController can simplely reuse the scheduling result to learn the weight. The HPAController in the member cluster scale the Workload in the member cluster directly, it will conflict between the karmada scheduler. We can retain replicas in the member cluster by using feature resource interpreter webhook .","title":"How FederatedHPAController learn the propagation information corresponding to the Workload"},{"location":"proposals/hpa/federated-hpa/#how-to-deal-with-the-specreplicas-of-the-workload-in-the-control-plane","text":"In the original Kubernetes HPA and Workload(Deployment, for example), HPA scale the workload through the scale subresource of Workload. Then, the field replicas will be modified to the desired number. But in this design, HPAController in the member cluster work standalone and don't scale workloads through control plane so that the actual number of pods in the member clusters don't match the spec.replicas of Workload in the control plane. This mismatch would cause incident when spec.replicas in control plane is much smaller than in member clusters and user delete the HPA resource. To solve this problem, FederatedHPAController can collect the sum of spec.replicas values from member clusters and set it to the scale subresource of Workload in the control plane. Even the spec.replicas of Workload in the control plane matches the actual total replicas in the member clusers, every time the spec.replicas of Workload in the control plane is modified, the replicas distribution in the Work re-calculated by Karmada scheduler most probably don't match the actual distribution in the member clusters. The mismatch also would cause incident mentioned above. To solve this problem, we can split it into two sub-problems * How to gracefully shift workload in member clusters when desired distribution calculated by the karmada scheduler and actual distribution among member clusters differ substaintially. This may caused by modification of the PropagationPolicy or the remove of HPA resources controlled by the FederatedHPAController. Without the graceful shifting progress, the service may get out of capacity. In the future, features such as Federated Pod Disruption Budget may are needed to solve the problem here. * How to control the difference between the actual distribution in member clusters and the desired state the karmada scheduler calculated even the FederatedHPA is enabled. But this problem will not be too critical if the first problem is solved. It is better to solve the first sub-problem in another proposal. So we will leave this problem until the first one is solved.","title":"How to deal with the spec.replicas of the Workload in the control plane"},{"location":"proposals/hpa/federated-hpa/#how-to-deal-with-the-situation-that-specreplicas-is-1","text":"The workload can be spread into multiple member clusters when the spec.replicas in the control plane is greater than 1 . The disaster of one member cluster and control plane can be tolerated because the workload can be scaled in other member clusters. But if the spec.replicas is 1 , the workload and HPA resource would only be spread into one member cluster. If the member cluster and control plane are out of service in the same time, the workload can't be scaled.","title":"How to deal with the situation that spec.replicas is 1"},{"location":"proposals/hpa/federated-hpa/#how-to-integrate-with-and-migrate-from-existing-hpa-resources","text":"For some scenarios, people may want a friendly mechanism to control what HPA resources can be controled by FederatedHPAController 1. There are already many HPA resources in the control plane managed by PropagationPolicy and OverridePolicy before the Karmada support FederatedHPA natively. For some risk concerns, the administrator of the platform wants to migrate these HPA resources to be managed by the FederatedHPAController step by step. 1. There are already many HPA resources in the control plane managed by PropagationPolicy and OverridePolicy before the Karmada support FederatedHPA natively. But in the same Karmada control plane, some users want to use the native FederatedHPA but others want to remain the old ways. The FederatedHPA should not conflict with the HPA resources managed by the old ways. To meet the requirements of above scenarios mentioned, a label federatedhpa.karmada.io/enabled=true/false for HPA resources will be introduced.","title":"How to integrate with and migrate from existing HPA resources"},{"location":"proposals/hpa/federated-hpa/#how-user-stories-are-addressed","text":"","title":"How user stories are addressed"},{"location":"proposals/hpa/federated-hpa/#story1","text":"Suppose platform administrator create a ClusterPropagationPolicy as a global default propagation for the Workload resources inside namespace apps in advance. According to the default ClusterPropagationPolicy , the weight between clustera and clusterb should be 4:1 . unfold me to see the yaml apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: default spec: placement: clusterAffinity: clusterNames: - clustera - clusterb replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - clustera weight: 4 - targetCluster: clusterNames: - clusterb weight: 1 resourceSelectors: - apiVersion: workload.example.io/v1alpha1 kind: Workload In the term of user story 1, suppose a user create a Workload and an HPA in the apps namespace unfold me to see the yaml apiVersion: workload.example.io/v1alpha1 kind: Workload metadata: name: nginx namespace: apps labels: app: nginx spec: replicas: 5 paused: false template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx ------------------ apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: nginx namespace: apps spec: scaleTargetRef: apiVersion: workload.example.io/v1alpha1 kind: Workload name: nginx minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 Then, ClusterResourceBinding resources will be created and scheduled by the Karmada scheduler unfold me to see the yaml apiVersion: work.karmada.io/v1alpha2 kind: ClusterResourceBinding metadata: name: xxx spec: resource: apiVersion: workload.example.io/v1alpha1 kind: Workload name: nginx ... clusters: - name: clustera replicas: 4 - name: clusterb replicas: 1 The FederatedHPAController continuously watchs the events of HPA and Karmada relevant resources( ClusterPropagationPolicy/PropagationPolicy or ClusterResourceBinding/ResourceBinding ) to learn * Which clusters the HPA resources should be propagated to * What weight the workload should be spreaded to clusters. The weight will be used to spread the min/max of HPA to clusters Then, FederatedHPAController create/update Work resources for HPA resource. Finally, the HPA resource will be spread to clustera and clusterb . The min/max of HPA resource in clustera and clusterb will be 1/8 and 1/2 .","title":"Story1"},{"location":"proposals/hpa/federated-hpa/#high-availability","text":"","title":"High Availability"},{"location":"proposals/hpa/federated-hpa/#integration","text":"","title":"Integration"},{"location":"proposals/hpa/federated-hpa/#test-plan","text":"","title":"Test Plan"},{"location":"proposals/hpa/federated-hpa/#alternatives","text":"","title":"Alternatives"},{"location":"proposals/proposal-template/proposal-template/","text":"Your short, descriptive title Summary Motivation Goals Non-Goals Proposal User Stories (Optional) Story 1 Story 2 Notes/Constraints/Caveats (Optional) Risks and Mitigations Design Details Test Plan Alternatives","title":"Your short, descriptive title"},{"location":"proposals/proposal-template/proposal-template/#your-short-descriptive-title","text":"","title":"Your short, descriptive title"},{"location":"proposals/proposal-template/proposal-template/#summary","text":"","title":"Summary"},{"location":"proposals/proposal-template/proposal-template/#motivation","text":"","title":"Motivation"},{"location":"proposals/proposal-template/proposal-template/#goals","text":"","title":"Goals"},{"location":"proposals/proposal-template/proposal-template/#non-goals","text":"","title":"Non-Goals"},{"location":"proposals/proposal-template/proposal-template/#proposal","text":"","title":"Proposal"},{"location":"proposals/proposal-template/proposal-template/#user-stories-optional","text":"","title":"User Stories (Optional)"},{"location":"proposals/proposal-template/proposal-template/#story-1","text":"","title":"Story 1"},{"location":"proposals/proposal-template/proposal-template/#story-2","text":"","title":"Story 2"},{"location":"proposals/proposal-template/proposal-template/#notesconstraintscaveats-optional","text":"","title":"Notes/Constraints/Caveats (Optional)"},{"location":"proposals/proposal-template/proposal-template/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"proposals/proposal-template/proposal-template/#design-details","text":"","title":"Design Details"},{"location":"proposals/proposal-template/proposal-template/#test-plan","text":"","title":"Test Plan"},{"location":"proposals/proposal-template/proposal-template/#alternatives","text":"","title":"Alternatives"},{"location":"proposals/resource-interpreter-webhook/","text":"Resource Interpreter Webhook Summary In the progress of a resource(as known as resource template ) propagating to cluster, Karmada take actions according to the resource definition. For example, at the phase of building ResourceBinding , the karmada-controller will parse the replicas from resource templates like deployments but do nothing for resources that don't have replicas . For the Kubernetes native resources, Karmada knows how to parse them. But for custom resource type, as lack of the knowledge of the structure, Karmada treat the custom resource type as a general resource. This proposal aims to provide a solution for users to teach Karmada to learn their custom resources. Motivation Nowadays, lots of people or projects extend Kubernetes by Custom Resource Defination . In order to propagate the custom resources, Karmada has to learn the structure of the custom resource. Goals Provide a solution to support custom resources by teaching Karmada the resource structure. Non-Goals Proposal User Stories As a user, I want to propagate my custom resource(workload type with replicas) to leverage the Karmada replica scheduling capabilities. I have a custom resource which extremely similar with deployments , it has a replica field as well, I want to divide the replicas to multiple clusters by declaring a ReplicaScheduling rule. Without this framework, as lack of knowledge of the custom resource, Karmada can't grab it's replica . As a user, I want to customize the retain method for my CRD resources. I have a custom resource which reconciling by a controller running in member clusters. The controllers would make changes to the resource(such as update some fields in .spec), I wish Karmada could retain the changes made by my controller. Without this framework, as lack of knowledge of the custom resource, Karmada might can't retain the custom resource correctly. Thus, the resource might be changed back and forth by Karmada and it's controller. Notes/Constraints/Caveats (Optional) Risks and Mitigations Design Details Inspire of the Kubernetes Admission webhook , we propose a webhook called ResourceInterpreterWebhook which contains: - A configuration API ResourceInterpreterWebhookConfiguration to declare the enabled webhooks. - A message API ResourceInterpreterContext to declare the request and response between Karmada and webhooks. In the ResourceInterpreterWebhookConfiguration API, the InterpreterOperation represents the request that Karmada might call the webhooks in the whole propagating process. New ResourceInterpreterWebhookConfiguration API We propose a new CR in config.karmada.io group. // ResourceInterpreterWebhookConfiguration describes the configuration of webhooks which take the responsibility to // tell karmada the details of the resource object, especially for custom resources. type ResourceInterpreterWebhookConfiguration struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Webhooks is a list of webhooks and the affected resources and operations. // +required Webhooks []ResourceInterpreterWebhook `json:\"webhooks\"` } // ResourceInterpreterWebhook describes the webhook as well as the resources and operations it applies to. type ResourceInterpreterWebhook struct { // Name is the full-qualified name of the webhook. // +required Name string `json:\"name\"` // ClientConfig defines how to communicate with the hook. // +required ClientConfig admissionregistrationv1.WebhookClientConfig `json:\"clientConfig\"` // Rules describes what operations on what resources the webhook cares about. // The webhook cares about an operation if it matches any Rule. // +optional Rules []RuleWithOperations `json:\"rules,omitempty\"` // TimeoutSeconds specifies the timeout for this webhook. After the timeout passes, // the webhook call will be ignored or the API call will fail based on the // failure policy. // The timeout value must be between 1 and 30 seconds. // Default to 10 seconds. // +optional TimeoutSeconds *int32 `json:\"timeoutSeconds,omitempty\"` // InterpreterContextVersions is an ordered list of preferred `ResourceInterpreterContext` // versions the Webhook expects. Karmada will try to use first version in // the list which it supports. If none of the versions specified in this list // supported by Karmada, validation will fail for this object. // If a persisted webhook configuration specifies allowed versions and does not // include any versions known to the Karmada, calls to the webhook will fail // and be subject to the failure policy. InterpreterContextVersions []string `json:\"interpreterContextVersions\"` } // RuleWithOperations is a tuple of Operations and Resources. It is recommended to make // sure that all the tuple expansions are valid. type RuleWithOperations struct { // Operations is the operations the hook cares about. // If '*' is present, the length of the slice must be one. // +required Operations []InterpreterOperation `json:\"operations\"` // Rule is embedded, it describes other criteria of the rule, like // APIGroups, APIVersions, Kinds, etc. Rule `json:\",inline\"` } // InterpreterOperation specifies an operation for a request. type InterpreterOperation string const ( // InterpreterOperationAll indicates math all InterpreterOperation. InterpreterOperationAll InterpreterOperation = \"*\" // InterpreterOperationInterpretReplica indicates that karmada want to figure out the replica declaration of a specific object. // Only necessary for those resource types that have replica declaration, like Deployment or similar custom resources. InterpreterOperationInterpretReplica InterpreterOperation = \"InterpretReplica\" // InterpreterOperationReviseReplica indicates that karmada request webhook to modify the replica. InterpreterOperationReviseReplica InterpreterOperation = \"ReviseReplica\" // InterpreterOperationInterpretStatus indicates that karmada want to figure out how to get the status. // Only necessary for those resource types that define their status in a special path(not '.status'). InterpreterOperationInterpretStatus InterpreterOperation = \"InterpretStatus\" // InterpreterOperationPrune indicates that karmada want to figure out how to package resource template to Work. InterpreterOperationPrune InterpreterOperation = \"Prune\" // InterpreterOperationRetain indicates that karmada request webhook to retain the desired resource template. // Only necessary for those resources which specification will be updated by their controllers running in member cluster. InterpreterOperationRetain InterpreterOperation = \"Retain\" // InterpreterOperationAggregateStatus indicates that karmada want to figure out how to aggregate status to resource template. // Only necessary for those resource types that want to aggregate status to resource template. InterpreterOperationAggregateStatus InterpreterOperation = \"AggregateStatus\" // InterpreterOperationInterpretHealthy indicates that karmada want to figure out the healthy status of a specific object. // Only necessary for those resource types that have and want to reflect their healthy status. InterpreterOperationInterpretHealthy InterpreterOperation = \"InterpretHealthy\" // InterpreterOperationInterpretDependency indicates that karmada want to figure out the dependencies of a specific object. // Only necessary for those resource types that have dependencies resources and expect the dependencies be propagated // together, like Deployment depends on ConfigMap/Secret. InterpreterOperationInterpretDependency InterpreterOperation = \"InterpretDependency\" ) // Rule is a tuple of APIGroups, APIVersion, and Kinds. type Rule struct { // APIGroups is the API groups the resources belong to. '*' is all groups. // If '*' is present, the length of the slice must be one. // For example: // [\"apps\", \"batch\", \"example.io\"] means matches 3 groups. // [\"*\"] means matches all group // // Note: The group cloud be empty, e.g the 'core' group of kubernetes, in that case use [\"\"]. // +required APIGroups []string `json:\"apiGroups\"` // APIVersions is the API versions the resources belong to. '*' is all versions. // If '*' is present, the length of the slice must be one. // For example: // [\"v1alpha1\", \"v1beta1\"] means matches 2 versions. // [\"*\"] means matches all versions. // +required APIVersions []string `json:\"apiVersions\"` // Kinds is a list of resources this rule applies to. // If '*' is present, the length of the slice must be one. // For example: // [\"Deployment\", \"Pod\"] means matches Deployment and Pod. // [\"*\"] means apply to all resources. // +required Kinds []string `json:\"kinds\"` } New ResourceInterpreterContext API // ResourceInterpreterContext describes an interpreter context request and response. type ResourceInterpreterContext struct { metav1.TypeMeta `json:\",inline\"` // Request describes the attributes for the interpreter request. // +optional Request *ResourceInterpreterRequest `json:\"request,omitempty\"` // Response describes the attributes for the interpreter response. // +optional Response *ResourceInterpreterResponse `json:\"response,omitempty\"` } // ResourceInterpreterRequest describes the interpreter.Attributes for the interpreter request. type ResourceInterpreterRequest struct { // UID is an identifier for the individual request/response. // The UID is meant to track the round trip (request/response) between the karmada and the WebHook, not the user request. // It is suitable for correlating log entries between the webhook and karmada, for either auditing or debugging. // +required UID types.UID `json:\"uid\"` // Kind is the fully-qualified type of object being submitted (for example, v1.Pod or autoscaling.v1.Scale) // +required Kind metav1.GroupVersionKind `json:\"kind\"` // Name is the name of the object as presented in the request. // +required Name string `json:\"name\"` // Namespace is the namespace associated with the request (if any). // +optional Namespace string `json:\"namespace,omitempty\"` // Operation is the operation being performed. // +required Operation InterpreterOperation `json:\"operation\"` // Object is the object from the incoming request. // +optional Object runtime.RawExtension `json:\"object,omitempty\"` // ObservedObject is the object observed from the kube-apiserver of member clusters. // Not nil only when InterpreterOperation is InterpreterOperationRetain. // +optional ObservedObject *runtime.RawExtension `json:\"observedObject,omitempty\"` // DesiredReplicas represents the desired pods number which webhook should revise with. // It'll be set only if InterpreterOperation is InterpreterOperationReviseReplica. // +optional DesiredReplicas *int32 `json:\"replicas,omitempty\"` // AggregatedStatus represents status list of the resource running in each member cluster. // +optional AggregatedStatus []workv1alpha1.AggregatedStatusItem `json:\"aggregatedStatus,omitempty\"` } // ResourceInterpreterResponse describes an interpreter response. type ResourceInterpreterResponse struct { // UID is an identifier for the individual request/response. // This must be copied over from the corresponding ResourceInterpreterRequest. // +required UID types.UID `json:\"uid\"` // Successful indicates whether the request be processed successfully. // +required Successful bool `json:\"successful\"` // Status contains extra details information about why the request not successful. // This filed is not consulted in any way if \"Successful\" is \"true\". // +optional Status *RequestStatus `json:\"status,omitempty\"` // The patch body. We only support \"JSONPatch\" currently which implements RFC 6902. // +optional Patch []byte `json:\"patch,omitempty\"` // The type of Patch. We only allow \"JSONPatch\" currently. // +optional PatchType *PatchType `json:\"patchType,omitempty\" protobuf:\"bytes,5,opt,name=patchType\"` // ReplicaRequirements represents the requirements required by each replica. // Required if InterpreterOperation is InterpreterOperationInterpretReplica. // +optional ReplicaRequirements *workv1alpha2.ReplicaRequirements `json:\"replicaRequirements,omitempty\"` // Replicas represents the number of desired pods. This is a pointer to distinguish between explicit // zero and not specified. // Required if InterpreterOperation is InterpreterOperationInterpretReplica. // +optional Replicas *int32 `json:\"replicas,omitempty\"` // Dependencies represents the reference of dependencies object. // Required if InterpreterOperation is InterpreterOperationInterpretDependency. // +optional Dependencies []DependentObjectReference `json:\"dependencies,omitempty\"` // RawStatus represents the referencing object's status. // +optional RawStatus *runtime.RawExtension `json:\"rawStatus,omitempty\"` // Healthy represents the referencing object's healthy status. // +optional Healthy *bool `json:\"healthy,omitempty\"` } // RequestStatus holds the status of a request. type RequestStatus struct { // Message is human-readable description of the status of this operation. // +optional Message string `json:\"message,omitempty\"` // Code is the HTTP return code of this status. // +optional Code int32 `json:\"code,omitempty\"` } // PatchType is the type of patch being used to represent the mutated object type PatchType string const ( // PatchTypeJSONPatch represents the JSONType. PatchTypeJSONPatch PatchType = \"JSONPatch\" ) // DependentObjectReference contains enough information to locate the referenced object inside current cluster. type DependentObjectReference struct { // APIVersion represents the API version of the referent. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the referent. // +required Kind string `json:\"kind\"` // Namespace represents the namespace for the referent. // For non-namespace scoped resources(e.g. 'ClusterRole')\uff0cdo not need specify Namespace, // and for namespace scoped resources, Namespace is required. // If Namespace is not specified, means the resource is non-namespace scoped. // +optional Namespace string `json:\"namespace,omitempty\"` // Name represents the name of the referent. // +required Name string `json:\"name\"` } Example Configuration The example below show two webhooks configuration. The foo.example.com webhook serves for foos under foo.example.com group and implemented Retain and InterpretHealthy operations. The bar.example.com webhook serves for bars under bar.example.com group and implemented InterpretDependency and InterpretHealthy operations. apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterWebhookConfiguration metadata: name: example webhooks: - name: foo.example.com rules: - operations: [\"Retain\", \"InterpretHealthy\"] apiGroups: [\"foo.example.com\"] apiVersions: [\"*\"] kinds: [\"Foo\"] scope: \"Namespaced\" clientConfig: url: https://xxx:443/explore-foo caBundle: {{caBundle}} exploreReviewVersions: [\"v1alpha1\"] timeoutSeconds: 3 - name: bar.example.com rules: - operations: [\"InterpretDependency\", \"InterpretHealthy\"] apiGroups: [\"bar.example.com\"] apiVersions: [\"*\"] kinds: [\"Bar\"] scope: \"Cluster\" clientConfig: url: https://xxx:443/explore-bar caBundle: {{caBundle}} exploreReviewVersions: [\"v1alpha1\"] timeoutSeconds: 3 Request and Response Take InterpretHealthy for example, Karmada will send the request like: apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterContext request: - uid: xxx - Kind: - group: foo.example.com version: v1alpha1 Kind: Foo - name: foo - namespace: default - operation: InterpretHealthy - object: <raw data of the object> And the response like: apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterContext response: - uid: xxx(same uid in the request) - healthy: true Test Plan Propose E2E test cases according the operations described above. Alternatives The proposal Configurable Local Value Retention described a solution to retain custom resource, but the configuration would be a little complex to users.","title":"Resource Interpreter Webhook"},{"location":"proposals/resource-interpreter-webhook/#resource-interpreter-webhook","text":"","title":"Resource Interpreter Webhook"},{"location":"proposals/resource-interpreter-webhook/#summary","text":"In the progress of a resource(as known as resource template ) propagating to cluster, Karmada take actions according to the resource definition. For example, at the phase of building ResourceBinding , the karmada-controller will parse the replicas from resource templates like deployments but do nothing for resources that don't have replicas . For the Kubernetes native resources, Karmada knows how to parse them. But for custom resource type, as lack of the knowledge of the structure, Karmada treat the custom resource type as a general resource. This proposal aims to provide a solution for users to teach Karmada to learn their custom resources.","title":"Summary"},{"location":"proposals/resource-interpreter-webhook/#motivation","text":"Nowadays, lots of people or projects extend Kubernetes by Custom Resource Defination . In order to propagate the custom resources, Karmada has to learn the structure of the custom resource.","title":"Motivation"},{"location":"proposals/resource-interpreter-webhook/#goals","text":"Provide a solution to support custom resources by teaching Karmada the resource structure.","title":"Goals"},{"location":"proposals/resource-interpreter-webhook/#non-goals","text":"","title":"Non-Goals"},{"location":"proposals/resource-interpreter-webhook/#proposal","text":"","title":"Proposal"},{"location":"proposals/resource-interpreter-webhook/#user-stories","text":"","title":"User Stories"},{"location":"proposals/resource-interpreter-webhook/#as-a-user-i-want-to-propagate-my-custom-resourceworkload-type-with-replicas-to-leverage-the-karmada-replica-scheduling-capabilities","text":"I have a custom resource which extremely similar with deployments , it has a replica field as well, I want to divide the replicas to multiple clusters by declaring a ReplicaScheduling rule. Without this framework, as lack of knowledge of the custom resource, Karmada can't grab it's replica .","title":"As a user, I want to propagate my custom resource(workload type with replicas) to leverage the Karmada replica scheduling capabilities."},{"location":"proposals/resource-interpreter-webhook/#as-a-user-i-want-to-customize-the-retain-method-for-my-crd-resources","text":"I have a custom resource which reconciling by a controller running in member clusters. The controllers would make changes to the resource(such as update some fields in .spec), I wish Karmada could retain the changes made by my controller. Without this framework, as lack of knowledge of the custom resource, Karmada might can't retain the custom resource correctly. Thus, the resource might be changed back and forth by Karmada and it's controller.","title":"As a user, I want to customize the retain method for my CRD resources."},{"location":"proposals/resource-interpreter-webhook/#notesconstraintscaveats-optional","text":"","title":"Notes/Constraints/Caveats (Optional)"},{"location":"proposals/resource-interpreter-webhook/#risks-and-mitigations","text":"","title":"Risks and Mitigations"},{"location":"proposals/resource-interpreter-webhook/#design-details","text":"Inspire of the Kubernetes Admission webhook , we propose a webhook called ResourceInterpreterWebhook which contains: - A configuration API ResourceInterpreterWebhookConfiguration to declare the enabled webhooks. - A message API ResourceInterpreterContext to declare the request and response between Karmada and webhooks. In the ResourceInterpreterWebhookConfiguration API, the InterpreterOperation represents the request that Karmada might call the webhooks in the whole propagating process.","title":"Design Details"},{"location":"proposals/resource-interpreter-webhook/#new-resourceinterpreterwebhookconfiguration-api","text":"We propose a new CR in config.karmada.io group. // ResourceInterpreterWebhookConfiguration describes the configuration of webhooks which take the responsibility to // tell karmada the details of the resource object, especially for custom resources. type ResourceInterpreterWebhookConfiguration struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` // Webhooks is a list of webhooks and the affected resources and operations. // +required Webhooks []ResourceInterpreterWebhook `json:\"webhooks\"` } // ResourceInterpreterWebhook describes the webhook as well as the resources and operations it applies to. type ResourceInterpreterWebhook struct { // Name is the full-qualified name of the webhook. // +required Name string `json:\"name\"` // ClientConfig defines how to communicate with the hook. // +required ClientConfig admissionregistrationv1.WebhookClientConfig `json:\"clientConfig\"` // Rules describes what operations on what resources the webhook cares about. // The webhook cares about an operation if it matches any Rule. // +optional Rules []RuleWithOperations `json:\"rules,omitempty\"` // TimeoutSeconds specifies the timeout for this webhook. After the timeout passes, // the webhook call will be ignored or the API call will fail based on the // failure policy. // The timeout value must be between 1 and 30 seconds. // Default to 10 seconds. // +optional TimeoutSeconds *int32 `json:\"timeoutSeconds,omitempty\"` // InterpreterContextVersions is an ordered list of preferred `ResourceInterpreterContext` // versions the Webhook expects. Karmada will try to use first version in // the list which it supports. If none of the versions specified in this list // supported by Karmada, validation will fail for this object. // If a persisted webhook configuration specifies allowed versions and does not // include any versions known to the Karmada, calls to the webhook will fail // and be subject to the failure policy. InterpreterContextVersions []string `json:\"interpreterContextVersions\"` } // RuleWithOperations is a tuple of Operations and Resources. It is recommended to make // sure that all the tuple expansions are valid. type RuleWithOperations struct { // Operations is the operations the hook cares about. // If '*' is present, the length of the slice must be one. // +required Operations []InterpreterOperation `json:\"operations\"` // Rule is embedded, it describes other criteria of the rule, like // APIGroups, APIVersions, Kinds, etc. Rule `json:\",inline\"` } // InterpreterOperation specifies an operation for a request. type InterpreterOperation string const ( // InterpreterOperationAll indicates math all InterpreterOperation. InterpreterOperationAll InterpreterOperation = \"*\" // InterpreterOperationInterpretReplica indicates that karmada want to figure out the replica declaration of a specific object. // Only necessary for those resource types that have replica declaration, like Deployment or similar custom resources. InterpreterOperationInterpretReplica InterpreterOperation = \"InterpretReplica\" // InterpreterOperationReviseReplica indicates that karmada request webhook to modify the replica. InterpreterOperationReviseReplica InterpreterOperation = \"ReviseReplica\" // InterpreterOperationInterpretStatus indicates that karmada want to figure out how to get the status. // Only necessary for those resource types that define their status in a special path(not '.status'). InterpreterOperationInterpretStatus InterpreterOperation = \"InterpretStatus\" // InterpreterOperationPrune indicates that karmada want to figure out how to package resource template to Work. InterpreterOperationPrune InterpreterOperation = \"Prune\" // InterpreterOperationRetain indicates that karmada request webhook to retain the desired resource template. // Only necessary for those resources which specification will be updated by their controllers running in member cluster. InterpreterOperationRetain InterpreterOperation = \"Retain\" // InterpreterOperationAggregateStatus indicates that karmada want to figure out how to aggregate status to resource template. // Only necessary for those resource types that want to aggregate status to resource template. InterpreterOperationAggregateStatus InterpreterOperation = \"AggregateStatus\" // InterpreterOperationInterpretHealthy indicates that karmada want to figure out the healthy status of a specific object. // Only necessary for those resource types that have and want to reflect their healthy status. InterpreterOperationInterpretHealthy InterpreterOperation = \"InterpretHealthy\" // InterpreterOperationInterpretDependency indicates that karmada want to figure out the dependencies of a specific object. // Only necessary for those resource types that have dependencies resources and expect the dependencies be propagated // together, like Deployment depends on ConfigMap/Secret. InterpreterOperationInterpretDependency InterpreterOperation = \"InterpretDependency\" ) // Rule is a tuple of APIGroups, APIVersion, and Kinds. type Rule struct { // APIGroups is the API groups the resources belong to. '*' is all groups. // If '*' is present, the length of the slice must be one. // For example: // [\"apps\", \"batch\", \"example.io\"] means matches 3 groups. // [\"*\"] means matches all group // // Note: The group cloud be empty, e.g the 'core' group of kubernetes, in that case use [\"\"]. // +required APIGroups []string `json:\"apiGroups\"` // APIVersions is the API versions the resources belong to. '*' is all versions. // If '*' is present, the length of the slice must be one. // For example: // [\"v1alpha1\", \"v1beta1\"] means matches 2 versions. // [\"*\"] means matches all versions. // +required APIVersions []string `json:\"apiVersions\"` // Kinds is a list of resources this rule applies to. // If '*' is present, the length of the slice must be one. // For example: // [\"Deployment\", \"Pod\"] means matches Deployment and Pod. // [\"*\"] means apply to all resources. // +required Kinds []string `json:\"kinds\"` }","title":"New ResourceInterpreterWebhookConfiguration API"},{"location":"proposals/resource-interpreter-webhook/#new-resourceinterpretercontext-api","text":"// ResourceInterpreterContext describes an interpreter context request and response. type ResourceInterpreterContext struct { metav1.TypeMeta `json:\",inline\"` // Request describes the attributes for the interpreter request. // +optional Request *ResourceInterpreterRequest `json:\"request,omitempty\"` // Response describes the attributes for the interpreter response. // +optional Response *ResourceInterpreterResponse `json:\"response,omitempty\"` } // ResourceInterpreterRequest describes the interpreter.Attributes for the interpreter request. type ResourceInterpreterRequest struct { // UID is an identifier for the individual request/response. // The UID is meant to track the round trip (request/response) between the karmada and the WebHook, not the user request. // It is suitable for correlating log entries between the webhook and karmada, for either auditing or debugging. // +required UID types.UID `json:\"uid\"` // Kind is the fully-qualified type of object being submitted (for example, v1.Pod or autoscaling.v1.Scale) // +required Kind metav1.GroupVersionKind `json:\"kind\"` // Name is the name of the object as presented in the request. // +required Name string `json:\"name\"` // Namespace is the namespace associated with the request (if any). // +optional Namespace string `json:\"namespace,omitempty\"` // Operation is the operation being performed. // +required Operation InterpreterOperation `json:\"operation\"` // Object is the object from the incoming request. // +optional Object runtime.RawExtension `json:\"object,omitempty\"` // ObservedObject is the object observed from the kube-apiserver of member clusters. // Not nil only when InterpreterOperation is InterpreterOperationRetain. // +optional ObservedObject *runtime.RawExtension `json:\"observedObject,omitempty\"` // DesiredReplicas represents the desired pods number which webhook should revise with. // It'll be set only if InterpreterOperation is InterpreterOperationReviseReplica. // +optional DesiredReplicas *int32 `json:\"replicas,omitempty\"` // AggregatedStatus represents status list of the resource running in each member cluster. // +optional AggregatedStatus []workv1alpha1.AggregatedStatusItem `json:\"aggregatedStatus,omitempty\"` } // ResourceInterpreterResponse describes an interpreter response. type ResourceInterpreterResponse struct { // UID is an identifier for the individual request/response. // This must be copied over from the corresponding ResourceInterpreterRequest. // +required UID types.UID `json:\"uid\"` // Successful indicates whether the request be processed successfully. // +required Successful bool `json:\"successful\"` // Status contains extra details information about why the request not successful. // This filed is not consulted in any way if \"Successful\" is \"true\". // +optional Status *RequestStatus `json:\"status,omitempty\"` // The patch body. We only support \"JSONPatch\" currently which implements RFC 6902. // +optional Patch []byte `json:\"patch,omitempty\"` // The type of Patch. We only allow \"JSONPatch\" currently. // +optional PatchType *PatchType `json:\"patchType,omitempty\" protobuf:\"bytes,5,opt,name=patchType\"` // ReplicaRequirements represents the requirements required by each replica. // Required if InterpreterOperation is InterpreterOperationInterpretReplica. // +optional ReplicaRequirements *workv1alpha2.ReplicaRequirements `json:\"replicaRequirements,omitempty\"` // Replicas represents the number of desired pods. This is a pointer to distinguish between explicit // zero and not specified. // Required if InterpreterOperation is InterpreterOperationInterpretReplica. // +optional Replicas *int32 `json:\"replicas,omitempty\"` // Dependencies represents the reference of dependencies object. // Required if InterpreterOperation is InterpreterOperationInterpretDependency. // +optional Dependencies []DependentObjectReference `json:\"dependencies,omitempty\"` // RawStatus represents the referencing object's status. // +optional RawStatus *runtime.RawExtension `json:\"rawStatus,omitempty\"` // Healthy represents the referencing object's healthy status. // +optional Healthy *bool `json:\"healthy,omitempty\"` } // RequestStatus holds the status of a request. type RequestStatus struct { // Message is human-readable description of the status of this operation. // +optional Message string `json:\"message,omitempty\"` // Code is the HTTP return code of this status. // +optional Code int32 `json:\"code,omitempty\"` } // PatchType is the type of patch being used to represent the mutated object type PatchType string const ( // PatchTypeJSONPatch represents the JSONType. PatchTypeJSONPatch PatchType = \"JSONPatch\" ) // DependentObjectReference contains enough information to locate the referenced object inside current cluster. type DependentObjectReference struct { // APIVersion represents the API version of the referent. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the referent. // +required Kind string `json:\"kind\"` // Namespace represents the namespace for the referent. // For non-namespace scoped resources(e.g. 'ClusterRole')\uff0cdo not need specify Namespace, // and for namespace scoped resources, Namespace is required. // If Namespace is not specified, means the resource is non-namespace scoped. // +optional Namespace string `json:\"namespace,omitempty\"` // Name represents the name of the referent. // +required Name string `json:\"name\"` }","title":"New ResourceInterpreterContext API"},{"location":"proposals/resource-interpreter-webhook/#example","text":"","title":"Example"},{"location":"proposals/resource-interpreter-webhook/#configuration","text":"The example below show two webhooks configuration. The foo.example.com webhook serves for foos under foo.example.com group and implemented Retain and InterpretHealthy operations. The bar.example.com webhook serves for bars under bar.example.com group and implemented InterpretDependency and InterpretHealthy operations. apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterWebhookConfiguration metadata: name: example webhooks: - name: foo.example.com rules: - operations: [\"Retain\", \"InterpretHealthy\"] apiGroups: [\"foo.example.com\"] apiVersions: [\"*\"] kinds: [\"Foo\"] scope: \"Namespaced\" clientConfig: url: https://xxx:443/explore-foo caBundle: {{caBundle}} exploreReviewVersions: [\"v1alpha1\"] timeoutSeconds: 3 - name: bar.example.com rules: - operations: [\"InterpretDependency\", \"InterpretHealthy\"] apiGroups: [\"bar.example.com\"] apiVersions: [\"*\"] kinds: [\"Bar\"] scope: \"Cluster\" clientConfig: url: https://xxx:443/explore-bar caBundle: {{caBundle}} exploreReviewVersions: [\"v1alpha1\"] timeoutSeconds: 3","title":"Configuration"},{"location":"proposals/resource-interpreter-webhook/#request-and-response","text":"Take InterpretHealthy for example, Karmada will send the request like: apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterContext request: - uid: xxx - Kind: - group: foo.example.com version: v1alpha1 Kind: Foo - name: foo - namespace: default - operation: InterpretHealthy - object: <raw data of the object> And the response like: apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterContext response: - uid: xxx(same uid in the request) - healthy: true","title":"Request and Response"},{"location":"proposals/resource-interpreter-webhook/#test-plan","text":"Propose E2E test cases according the operations described above.","title":"Test Plan"},{"location":"proposals/resource-interpreter-webhook/#alternatives","text":"The proposal Configurable Local Value Retention described a solution to retain custom resource, but the configuration would be a little complex to users.","title":"Alternatives"},{"location":"proposals/resource-quota/","text":"title: ResourceQuota authors: - \"@aven-ai\" reviewers: - \"@TBD\" approvers: - \"@TBD\" creation-date: 2021-08-20 ResourceQuota Summary With the widespread used of multi-clusters, the single-cluster quota management ResourceQuota in Kubernetes can no longer meet the administrator's resource management and restriction requirements for federated clusters. Resource administrators often need to stand on the global dimension to manage and control the total consumption of resources by each business. Creating a corresponding namespace and ResourceQuotas under each Kubernetes is the usual practice, and then Kubernetes will limit the resources by ResourceQuotas . However, with the growth of the number of businesses, the expansion and contraction of sub-clusters, the number of available resources and resource types of each cluster are different, which brings many problems to the administrator. In addition, Karmada supports the propagation of ResourceQuota objects to Kubernetes clusters through PropagationPolicy, so as to achieve the purpose of creating quotas on multi-clusters by k8s native interfaces. However, it is impossible to freely adjust and limit the global resource usage of a business by PropagationPolicy, and it is quite troublesome to expand such a requirement ont it. We need a global quota for Karmada, not just ResourceQuota on sub-clusters. This document describes a quota system KarmadaQuota for Karmada. As a part of admission control, the KarmadaQuota enforcing hard resource usage limits per namespace. The following design documents partially refer to the Kubernetes ResourceQuota design. Motivation Goals Supports the global quota management for multi-clusters on Karmada. Allows administrators to perform quota management on Karmada to balance multi-clusters resources allocation, monitor usage, and control billing for different businesses. Non-Goals Provide complex quotas to meet various business scenarios. Proposal Function overview Ability to enumerate resource usage limits per namespace. Ability to monitor resource usage for tracked resources. Ability to reject resource usage exceeding hard quotas. User Stories Story 1 As a Karmada administrator, I want to create a resource limit of 100 core CPUs for business A. The available resources on clusters: - cluster1: 20C - cluster2: 50C - cluster3: 100C - ... Generally, administrators have two methods. - In each cluster, manually divide a small quota for business A. - Or use Karmada PropagationPolicy to propagate the same quota to sub-clusters. Neither of these two methods can meet the needs of administrators. Now, the administrator can directly create a 100 core CPUs Quota in Karmada for business A by Karmada's KarmadaQuota . The Quota can control all the available resources of the sub-clusters, administrators no longer need to operate ResourceQouota in the sub-clusters. Story 2 As a business user, I want to know how many quotas I have in total and how many quotas I currently used. Business users usually pay attention to the total resource usage of Quota that they have applied for, rather than how many resources are used in a specific cluster. Now we can create a total quota for business A (business representative) on karmada, and Karmada itself can monitor the resource usage of the tracked resource, not the monitor of sub-clusters quota. Notes/Constraints/Caveats (Optional) Risks and Mitigations If a customer create a pod in member cluster itself, karmada is not able to perceive them, and then KarmadaQuota will not limit them too. It reverts the usage of single cluster. If a customer use karmada to propagate a controller which can create pods in the member clusters, as the quota webhook can not check these pods's resources, these pod will be created in the member clusters lead to a customer may use resources more than the quota limit. There are currently no mitigation measures yet, please try to avoid this usage, and continue to pay attention to the karmada community. See discuss about this situation . Design Details General Idea The quota management of multi-cluster Karmada is expected to a centralized design, all quota management information and resource verification will be closed-loop within karmada. Do not have too much coupling with the outside world or sub-clusters. The sub-clusters do not need to create ResourceQuotas to manage its own resources. Quota information is stored in Karmada etcd, and cluster resources are managed by Karmada. The subordinate clusters of Karmada is equivalent to the node of Kubernetes. Data Model API In order to distinguish the functions and positioning of Karmada's multi-cluster management quota and the ResourceQuota of k8s, the quota API on Karmada is defined as KarmadaQuota . The KarmadaQuota object is scoped to a Namespace . // KarmadaQuota sets aggregate quota restrictions enforced per namespace type KarmadaQuota struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // Spec defines the desired quota. // +optional Spec corev1.ResourceQuotaSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // Status defines the actual enforced quota and its current usage. // +optional Status corev1.ResourceQuotaStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // KarmadaQuotaList is a list of KarmadaQuota resources. type KarmadaQuotaList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Items []KarmadaQuota `json:\"items\" protobuf:\"bytes,2,rep,name=items\"` } Quota Tracked Resources KarmadaQuota supports resources almost similar to k8s native ResourceQuota . But unlike k8s, KarmadaQuota only supports objects and resources that can be distributed to sub-clusters through PropagationPolicy . If an object or resource does not need to be distributed to sub-clusters, then the KarmadaQuota does not track it. The following example resources are supported by the quota system: Resource Description cpu Total requested cpu usage memory Total requested memory usage pods Total number of active pods where phase is pending or active. services Total number of services secrets Total number of secrets If a third-party wants to track additional resources, it must follow the resource naming conventions prescribed by Kubernetes. This means the resource must have a fully-qualified name (i.e. mycompany.org/shinynewresource) Resource Requirements: Requests vs. Limits The admission check and resource consumption deduction of KarmadaQuota are achieved by tracking Resourcebinding . Resourcebinding of karmada is designed to only support ResourceRequest, and the limit of pod Container does not be distributed to the sub-cluster, so KarmadaQuota only supports Request type resources The following is the API definition of ResourceBinding's replicaRequirements, the resourceRequest is the resource Request that karmada will distribute to the sub-cluster by PropagationPolicy. // ReplicaRequirements represents the requirements required by each replica. type ReplicaRequirements struct { // A node selector represents the union of the results of one or more label queries // over a set of nodes; that is, it represents the OR of the selectors represented // by the node selector terms. // +optional NodeAffinity *corev1.NodeSelector `json:\"nodeAffinity,omitempty\"` // NodeSelector is a selector which must be true for the pod to fit on a node. // Selector which must match a node's labels for the pod to be scheduled on that node. // +optional NodeSelector map[string]string `json:\"nodeSelector,omitempty\"` // If specified, the pod's tolerations. // +optional Tolerations []corev1.Toleration `json:\"tolerations,omitempty\"` // ResourceRequest represents the resources required by each replica. // +optional ResourceRequest corev1.ResourceList `json:\"resourceRequest,omitempty\"` } ```` ### Scopes in ResourceQuota Each quota can have an associated set of scopes. A quota will only measure usage for a resource if it matches the intersection of enumerated scopes. When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope. Resources specified on the quota outside of the allowed set results in a validation error. | Scope | Description | | --- | --------- | | PriorityClass | Match pods that references the specified priority class. | The scopeSelector supports the following values in the operator field: - In - NotIn - Exists - DoesNotExist If the operator is In or NotIn, the values field must have at least one value. For example: ```go scopeSelector: matchExpressions: - scopeName: PriorityClass operator: In values: - middle karmada Quota Controller A resource quota controller monitors observed usage for tracked resources in the Namespace . If there is observed difference between the current usage stats versus the current KarmadaQuota.Status , the controller posts an update of the currently observed usage metrics to the KarmadaQuota via the /status endpoint. The resource quota controller is the only component capable of monitoring and recording usage updates after a DELETE operation since admission control is incapable of guaranteeing a DELETE request actually succeeded. karmada Quota webhook The KarmadaQuota plug-in introspects all incoming admission requests. It makes decisions by evaluating the incoming object against all defined KarmadaQuota.Status.Hard resource limits in the request namespace. If acceptance of the resource would cause the total usage of a named resource to exceed its hard limit, the request is denied. If the incoming request does not cause the total usage to exceed any of the enumerated hard resource limits, the plug-in will post a KarmadaQuota.Status document to the server to atomically update the observed usage based on the previously read KarmadaQuota.ResourceVersion . This keeps incremental usage atomically consistent, but does introduce a bottleneck (intentionally) into the system. To optimize system performance, it is encouraged that all resource quotas are tracked on the same KarmadaQuota document in a Namespace . As a result, it is encouraged to impose a cap on the total number of individual quotas that are tracked in the Namespace to 1 in the KarmadaQuota document. Test Plan Propose E2E test cases according to our use cases above: Test we can enumerate resource usage limits per namespace. Test we can monitor resource usage for tracked resources. Test we can reject resource usage exceeding hard quotas. Propose a tool that people can test the script. More information See k8s resource quota document and the example of Resource Quota for more information.","title":"Index"},{"location":"proposals/resource-quota/#resourcequota","text":"","title":"ResourceQuota"},{"location":"proposals/resource-quota/#summary","text":"With the widespread used of multi-clusters, the single-cluster quota management ResourceQuota in Kubernetes can no longer meet the administrator's resource management and restriction requirements for federated clusters. Resource administrators often need to stand on the global dimension to manage and control the total consumption of resources by each business. Creating a corresponding namespace and ResourceQuotas under each Kubernetes is the usual practice, and then Kubernetes will limit the resources by ResourceQuotas . However, with the growth of the number of businesses, the expansion and contraction of sub-clusters, the number of available resources and resource types of each cluster are different, which brings many problems to the administrator. In addition, Karmada supports the propagation of ResourceQuota objects to Kubernetes clusters through PropagationPolicy, so as to achieve the purpose of creating quotas on multi-clusters by k8s native interfaces. However, it is impossible to freely adjust and limit the global resource usage of a business by PropagationPolicy, and it is quite troublesome to expand such a requirement ont it. We need a global quota for Karmada, not just ResourceQuota on sub-clusters. This document describes a quota system KarmadaQuota for Karmada. As a part of admission control, the KarmadaQuota enforcing hard resource usage limits per namespace. The following design documents partially refer to the Kubernetes ResourceQuota design.","title":"Summary"},{"location":"proposals/resource-quota/#motivation","text":"","title":"Motivation"},{"location":"proposals/resource-quota/#goals","text":"Supports the global quota management for multi-clusters on Karmada. Allows administrators to perform quota management on Karmada to balance multi-clusters resources allocation, monitor usage, and control billing for different businesses.","title":"Goals"},{"location":"proposals/resource-quota/#non-goals","text":"Provide complex quotas to meet various business scenarios.","title":"Non-Goals"},{"location":"proposals/resource-quota/#proposal","text":"","title":"Proposal"},{"location":"proposals/resource-quota/#function-overview","text":"Ability to enumerate resource usage limits per namespace. Ability to monitor resource usage for tracked resources. Ability to reject resource usage exceeding hard quotas.","title":"Function overview"},{"location":"proposals/resource-quota/#user-stories","text":"","title":"User Stories"},{"location":"proposals/resource-quota/#story-1","text":"As a Karmada administrator, I want to create a resource limit of 100 core CPUs for business A. The available resources on clusters: - cluster1: 20C - cluster2: 50C - cluster3: 100C - ... Generally, administrators have two methods. - In each cluster, manually divide a small quota for business A. - Or use Karmada PropagationPolicy to propagate the same quota to sub-clusters. Neither of these two methods can meet the needs of administrators. Now, the administrator can directly create a 100 core CPUs Quota in Karmada for business A by Karmada's KarmadaQuota . The Quota can control all the available resources of the sub-clusters, administrators no longer need to operate ResourceQouota in the sub-clusters.","title":"Story 1"},{"location":"proposals/resource-quota/#story-2","text":"As a business user, I want to know how many quotas I have in total and how many quotas I currently used. Business users usually pay attention to the total resource usage of Quota that they have applied for, rather than how many resources are used in a specific cluster. Now we can create a total quota for business A (business representative) on karmada, and Karmada itself can monitor the resource usage of the tracked resource, not the monitor of sub-clusters quota.","title":"Story 2"},{"location":"proposals/resource-quota/#notesconstraintscaveats-optional","text":"","title":"Notes/Constraints/Caveats (Optional)"},{"location":"proposals/resource-quota/#risks-and-mitigations","text":"If a customer create a pod in member cluster itself, karmada is not able to perceive them, and then KarmadaQuota will not limit them too. It reverts the usage of single cluster. If a customer use karmada to propagate a controller which can create pods in the member clusters, as the quota webhook can not check these pods's resources, these pod will be created in the member clusters lead to a customer may use resources more than the quota limit. There are currently no mitigation measures yet, please try to avoid this usage, and continue to pay attention to the karmada community. See discuss about this situation .","title":"Risks and Mitigations"},{"location":"proposals/resource-quota/#design-details","text":"","title":"Design Details"},{"location":"proposals/resource-quota/#general-idea","text":"The quota management of multi-cluster Karmada is expected to a centralized design, all quota management information and resource verification will be closed-loop within karmada. Do not have too much coupling with the outside world or sub-clusters. The sub-clusters do not need to create ResourceQuotas to manage its own resources. Quota information is stored in Karmada etcd, and cluster resources are managed by Karmada. The subordinate clusters of Karmada is equivalent to the node of Kubernetes.","title":"General Idea"},{"location":"proposals/resource-quota/#data-model-api","text":"In order to distinguish the functions and positioning of Karmada's multi-cluster management quota and the ResourceQuota of k8s, the quota API on Karmada is defined as KarmadaQuota . The KarmadaQuota object is scoped to a Namespace . // KarmadaQuota sets aggregate quota restrictions enforced per namespace type KarmadaQuota struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` // Spec defines the desired quota. // +optional Spec corev1.ResourceQuotaSpec `json:\"spec,omitempty\" protobuf:\"bytes,2,opt,name=spec\"` // Status defines the actual enforced quota and its current usage. // +optional Status corev1.ResourceQuotaStatus `json:\"status,omitempty\" protobuf:\"bytes,3,opt,name=status\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // KarmadaQuotaList is a list of KarmadaQuota resources. type KarmadaQuotaList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata,omitempty\" protobuf:\"bytes,1,opt,name=metadata\"` Items []KarmadaQuota `json:\"items\" protobuf:\"bytes,2,rep,name=items\"` }","title":"Data Model API"},{"location":"proposals/resource-quota/#quota-tracked-resources","text":"KarmadaQuota supports resources almost similar to k8s native ResourceQuota . But unlike k8s, KarmadaQuota only supports objects and resources that can be distributed to sub-clusters through PropagationPolicy . If an object or resource does not need to be distributed to sub-clusters, then the KarmadaQuota does not track it. The following example resources are supported by the quota system: Resource Description cpu Total requested cpu usage memory Total requested memory usage pods Total number of active pods where phase is pending or active. services Total number of services secrets Total number of secrets If a third-party wants to track additional resources, it must follow the resource naming conventions prescribed by Kubernetes. This means the resource must have a fully-qualified name (i.e. mycompany.org/shinynewresource)","title":"Quota Tracked Resources"},{"location":"proposals/resource-quota/#resource-requirements-requests-vs-limits","text":"The admission check and resource consumption deduction of KarmadaQuota are achieved by tracking Resourcebinding . Resourcebinding of karmada is designed to only support ResourceRequest, and the limit of pod Container does not be distributed to the sub-cluster, so KarmadaQuota only supports Request type resources The following is the API definition of ResourceBinding's replicaRequirements, the resourceRequest is the resource Request that karmada will distribute to the sub-cluster by PropagationPolicy. // ReplicaRequirements represents the requirements required by each replica. type ReplicaRequirements struct { // A node selector represents the union of the results of one or more label queries // over a set of nodes; that is, it represents the OR of the selectors represented // by the node selector terms. // +optional NodeAffinity *corev1.NodeSelector `json:\"nodeAffinity,omitempty\"` // NodeSelector is a selector which must be true for the pod to fit on a node. // Selector which must match a node's labels for the pod to be scheduled on that node. // +optional NodeSelector map[string]string `json:\"nodeSelector,omitempty\"` // If specified, the pod's tolerations. // +optional Tolerations []corev1.Toleration `json:\"tolerations,omitempty\"` // ResourceRequest represents the resources required by each replica. // +optional ResourceRequest corev1.ResourceList `json:\"resourceRequest,omitempty\"` } ```` ### Scopes in ResourceQuota Each quota can have an associated set of scopes. A quota will only measure usage for a resource if it matches the intersection of enumerated scopes. When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope. Resources specified on the quota outside of the allowed set results in a validation error. | Scope | Description | | --- | --------- | | PriorityClass | Match pods that references the specified priority class. | The scopeSelector supports the following values in the operator field: - In - NotIn - Exists - DoesNotExist If the operator is In or NotIn, the values field must have at least one value. For example: ```go scopeSelector: matchExpressions: - scopeName: PriorityClass operator: In values: - middle","title":"Resource Requirements: Requests vs. Limits"},{"location":"proposals/resource-quota/#karmada-quota-controller","text":"A resource quota controller monitors observed usage for tracked resources in the Namespace . If there is observed difference between the current usage stats versus the current KarmadaQuota.Status , the controller posts an update of the currently observed usage metrics to the KarmadaQuota via the /status endpoint. The resource quota controller is the only component capable of monitoring and recording usage updates after a DELETE operation since admission control is incapable of guaranteeing a DELETE request actually succeeded.","title":"karmada Quota Controller"},{"location":"proposals/resource-quota/#karmada-quota-webhook","text":"The KarmadaQuota plug-in introspects all incoming admission requests. It makes decisions by evaluating the incoming object against all defined KarmadaQuota.Status.Hard resource limits in the request namespace. If acceptance of the resource would cause the total usage of a named resource to exceed its hard limit, the request is denied. If the incoming request does not cause the total usage to exceed any of the enumerated hard resource limits, the plug-in will post a KarmadaQuota.Status document to the server to atomically update the observed usage based on the previously read KarmadaQuota.ResourceVersion . This keeps incremental usage atomically consistent, but does introduce a bottleneck (intentionally) into the system. To optimize system performance, it is encouraged that all resource quotas are tracked on the same KarmadaQuota document in a Namespace . As a result, it is encouraged to impose a cap on the total number of individual quotas that are tracked in the Namespace to 1 in the KarmadaQuota document.","title":"karmada Quota webhook"},{"location":"proposals/resource-quota/#test-plan","text":"Propose E2E test cases according to our use cases above: Test we can enumerate resource usage limits per namespace. Test we can monitor resource usage for tracked resources. Test we can reject resource usage exceeding hard quotas. Propose a tool that people can test the script.","title":"Test Plan"},{"location":"proposals/resource-quota/#more-information","text":"See k8s resource quota document and the example of Resource Quota for more information.","title":"More information"},{"location":"proposals/scheduling/521-scheduler-estimator/","text":"Cluster Accurate Scheduler Estimator Summary As workloads in multi-clusters become more heterogeneous, it is natural that they have different scheduling needs such as NodeSelector, NodeAffinity and Tolerations. Meanwhile, the scheduler could not perceive free node resource of every node in member cluster accurately. This KEP proposes a new component, Karmada Cluster Accurate Scheduler Estimator, to enhance the accurate scheduling for Karmada Scheduler. It aims for estimating available replicas and return the result back to Karmada Scheduler for decision reference\u3002 Motivation There is currently no right way for Karmada Scheduler to perceive the dynamic node resource and pod request resource of the member clusters. The scheduler could only know the total resource situation of a member cluster according to the NodeSummary and ResourceSummary in Cluster.Status . If a workload specifies the replica resource claim, we should know how many available replicas a member cluster could produce in case of propagating too many replicas, which may lead to pending pods that fail to be scheduled. The Cluster Accurate Scheduler Estimator aims to fix these problems. Goals Make the available replica estimation more acurate for scheduler decision reference. Allow user to specify node claim such as NodeAffinity , NodeSelector and Tolerations for multi-cluster scheduling. Non-Goals Estimating how many pods have failed to been scheduled for rescheduling decision reference. Estimating available replicas groups in group and gang scheduling. Proposal This proposal gives a new component to estimate the maximum available replica of a workload. When assigning replicas, the Karmada Scheduler parallelly requests the corresponding Cluster Accurate Scheduler Estimators for estimation by gRPC request. This proposal is divided into several steps, see below: [ ] ResourceBinding API changes to add NodeAffinity , NodeSelector and Tolerations . [ ] Definition of proto and gRPC struct file. [ ] Estimator client integration in Karmada Scheduler. [ ] Estimator server development in Karmada Cluster Accurate Scheduler Estimator. [ ] Deployment script of Karmada Cluster Accurate Scheduler Estimator. [ ] Associated docs and architecture diagram as a supplement. User Stories Story 1 Imagine that we have 1 workload and 2 member clusters, and the ReplicaDivisionPreference is Aggregated . - Condition: - Cluster A has 10 nodes and each has 8 cpu remaining. - Cluster B has 2 nodes and each has 16 cpu remaining. - Workload has 1 replica and requests for 12 cpu. - Result: - Workload will be scheduled to Cluster A because it has more cpu remaining in total. However, it won't work because there is no node that could match the 12-cpu request. - Only Cluster B can handle the workload's request but its total resource does not have a competitiveness. Story 2 Imagine that 1 workload has a NodeSelector of \"key=value\", and the ReplicaDivisionPreference is Aggregated . - Condition: - Cluster A has 10 nodes and each has 16 cpu remaining. - Cluster B has 2 nodes, which have a label of \"key=value\" and each node has 16 cpu remaining. - Workload has 1 replica with a NodeSelector of \"key=value\" and requests for 12 cpu. - Result: - Workload will be scheduled to Cluster A because it has more cpu remaining in total. However, it won't work because there is no node that could match the NodeSelector . - Only Cluster B can handle the workload's request but its total resource does not have a competitiveness. Design Details The cluster maximum available replicas result has a significant influence with the scheduling. To solve this problem, we could change the way that we estimate the cluster available replicas. Note that it is now estimated by resource summary of a cluster. This function could be converted into a new scheduler plugin type. Here's the architecture design diagram. API Changes First, the API of ResourceBinding must be changed. NodeAffinity , NodeSelector and Tolerations would be added as a representative for NodeClaim along with the existed ResourceRequest . It is noticed that ReplicaResourceRequirements and Replicas is now in ObjectReference . It would be better to move these two fields in ResourceBindingSpec . // ResourceBindingSpec represents the expectation of ResourceBinding. type ResourceBindingSpec struct { // Resource represents the Kubernetes resource to be propagated. Resource ObjectReference `json:\"resource\"` // ReplicaRequirements represents the requirements required by each replica. // +optional ReplicaRequirements *ReplicaRequirements `json:\"replicaRequirements,omitempty\"` // Replicas represents the replica number of the referencing resource. // +optional Replicas int32 `json:\"replicas,omitempty\"` // Clusters represents target member clusters where the resource to be deployed. // +optional Clusters []TargetCluster `json:\"clusters,omitempty\"` } // ReplicaRequirements represents the requirements required by each replica. type ReplicaRequirements struct { // NodeClaim represents the node claim HardNodeAffinity, NodeSelector and Tolerations required by each replica. // +optional NodeClaim *NodeClaim `json:\"nodeClaim,omitempty\"` // ResourceRequest represents the resources required by each replica. // +optional ResourceRequest corev1.ResourceList `json:\"resourceRequest,omitempty\"` } // NodeClaim represents the node claim HardNodeAffinity, NodeSelector and Tolerations required by each replica. type NodeClaim struct { // A node selector represents the union of the results of one or more label queries over a set of // nodes; that is, it represents the OR of the selectors represented by the node selector terms. // Note that only PodSpec.Affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution // is included here because it has a hard limit on pod scheduling. // +optional HardNodeAffinity *corev1.NodeSelector `json:\"hardNodeAffinity,omitempty\"` // NodeSelector is a selector which must be true for the pod to fit on a node. // Selector which must match a node's labels for the pod to be scheduled on that node. // +optional NodeSelector map[string]string `json:\"nodeSelector,omitempty\"` // If specified, the pod's tolerations. // +optional Tolerations []corev1.Toleration `json:\"tolerations,omitempty\"` } Karmada Scheduler First, the existing plugins in Karmada Scheduler such as ClusterAffinity, APIInstalled and TaintToleration will select the suitable clusters. Based on this prefilter result, when assigning replicas, the Karmada Scheduler could try to calculate cluster max available replicas by starting gRPC requests concurrently to the Cluster Accurate Scheduler Estimator. At last, the Cluster Accurate Scheduler Estimator will soon return how many available replicas that the cluster could produce. Then the Karmada Scheduler assgin replicas into different clusters in terms of the estimation result. We could implement this by modifying function calClusterAvailableReplicas to a interface. The previous estimation method, based on ResourceSummary in Cluster.Status , is able to be a default normal estimation approach. Now we could just add a switch to determine whether Cluster Accurate Scheduler Estimator is applied, while the estimator via ResourceSummary could be a default one that does not support disabled. In the future, after the scheduler profile is added, a user could customize the config by using a profile. Furthermore, replica estimation can be considered as a new scheduler plugin. Karmada Cluster Accurate Scheduler Estimator Cluster Accurate Scheduler Estimator is a independent component that works as a gRPC server. Before its server starts, a pod and node informer associated with a member cluster will be created as a cache. Once the cache has been synced, the gRPC server would start and serve the incoming scheduler request as a replica estimator. Each Cluster Accurate Scheduler Estimator serves for one cluster, as same as karmada-agent . There are five steps for a scheduler estimation: Verify whether the request meets the requirements. Find all nodes that matches the node claim. List nodes by label selector. Filter nodes by node affinity. Filter schedulable nodes by taints and tolerations. Estimate max available replicas in every filter node. Get pods that assigned to the node. Calculate node idle resource. Calculate how many replicas that the node can be divided into, marked as r1 . Calculate the maximum remaining pods that the node allows, marked as r2 . Return the max available replicas --> min(r1, r2) . Return the sum of all node max available replicas. Test Plan Unit Test covering: Core changes in Karmada Scheduler that consists of gRPC connection establishment, replica estimation request sending. Core changes in Karmada Cluster Accurate Scheduler Estimator that consists of node filtering and node idle resource calculation. E2E Test covering: Deploy Karmada Cluster Accurate Scheduler Estimator. Specify different node claim in a workload and test the scheduler result.","title":"Cluster Accurate Scheduler Estimator"},{"location":"proposals/scheduling/521-scheduler-estimator/#cluster-accurate-scheduler-estimator","text":"","title":"Cluster Accurate Scheduler Estimator"},{"location":"proposals/scheduling/521-scheduler-estimator/#summary","text":"As workloads in multi-clusters become more heterogeneous, it is natural that they have different scheduling needs such as NodeSelector, NodeAffinity and Tolerations. Meanwhile, the scheduler could not perceive free node resource of every node in member cluster accurately. This KEP proposes a new component, Karmada Cluster Accurate Scheduler Estimator, to enhance the accurate scheduling for Karmada Scheduler. It aims for estimating available replicas and return the result back to Karmada Scheduler for decision reference\u3002","title":"Summary"},{"location":"proposals/scheduling/521-scheduler-estimator/#motivation","text":"There is currently no right way for Karmada Scheduler to perceive the dynamic node resource and pod request resource of the member clusters. The scheduler could only know the total resource situation of a member cluster according to the NodeSummary and ResourceSummary in Cluster.Status . If a workload specifies the replica resource claim, we should know how many available replicas a member cluster could produce in case of propagating too many replicas, which may lead to pending pods that fail to be scheduled. The Cluster Accurate Scheduler Estimator aims to fix these problems.","title":"Motivation"},{"location":"proposals/scheduling/521-scheduler-estimator/#goals","text":"Make the available replica estimation more acurate for scheduler decision reference. Allow user to specify node claim such as NodeAffinity , NodeSelector and Tolerations for multi-cluster scheduling.","title":"Goals"},{"location":"proposals/scheduling/521-scheduler-estimator/#non-goals","text":"Estimating how many pods have failed to been scheduled for rescheduling decision reference. Estimating available replicas groups in group and gang scheduling.","title":"Non-Goals"},{"location":"proposals/scheduling/521-scheduler-estimator/#proposal","text":"This proposal gives a new component to estimate the maximum available replica of a workload. When assigning replicas, the Karmada Scheduler parallelly requests the corresponding Cluster Accurate Scheduler Estimators for estimation by gRPC request. This proposal is divided into several steps, see below: [ ] ResourceBinding API changes to add NodeAffinity , NodeSelector and Tolerations . [ ] Definition of proto and gRPC struct file. [ ] Estimator client integration in Karmada Scheduler. [ ] Estimator server development in Karmada Cluster Accurate Scheduler Estimator. [ ] Deployment script of Karmada Cluster Accurate Scheduler Estimator. [ ] Associated docs and architecture diagram as a supplement.","title":"Proposal"},{"location":"proposals/scheduling/521-scheduler-estimator/#user-stories","text":"","title":"User Stories"},{"location":"proposals/scheduling/521-scheduler-estimator/#story-1","text":"Imagine that we have 1 workload and 2 member clusters, and the ReplicaDivisionPreference is Aggregated . - Condition: - Cluster A has 10 nodes and each has 8 cpu remaining. - Cluster B has 2 nodes and each has 16 cpu remaining. - Workload has 1 replica and requests for 12 cpu. - Result: - Workload will be scheduled to Cluster A because it has more cpu remaining in total. However, it won't work because there is no node that could match the 12-cpu request. - Only Cluster B can handle the workload's request but its total resource does not have a competitiveness.","title":"Story 1"},{"location":"proposals/scheduling/521-scheduler-estimator/#story-2","text":"Imagine that 1 workload has a NodeSelector of \"key=value\", and the ReplicaDivisionPreference is Aggregated . - Condition: - Cluster A has 10 nodes and each has 16 cpu remaining. - Cluster B has 2 nodes, which have a label of \"key=value\" and each node has 16 cpu remaining. - Workload has 1 replica with a NodeSelector of \"key=value\" and requests for 12 cpu. - Result: - Workload will be scheduled to Cluster A because it has more cpu remaining in total. However, it won't work because there is no node that could match the NodeSelector . - Only Cluster B can handle the workload's request but its total resource does not have a competitiveness.","title":"Story 2"},{"location":"proposals/scheduling/521-scheduler-estimator/#design-details","text":"The cluster maximum available replicas result has a significant influence with the scheduling. To solve this problem, we could change the way that we estimate the cluster available replicas. Note that it is now estimated by resource summary of a cluster. This function could be converted into a new scheduler plugin type. Here's the architecture design diagram.","title":"Design Details"},{"location":"proposals/scheduling/521-scheduler-estimator/#api-changes","text":"First, the API of ResourceBinding must be changed. NodeAffinity , NodeSelector and Tolerations would be added as a representative for NodeClaim along with the existed ResourceRequest . It is noticed that ReplicaResourceRequirements and Replicas is now in ObjectReference . It would be better to move these two fields in ResourceBindingSpec . // ResourceBindingSpec represents the expectation of ResourceBinding. type ResourceBindingSpec struct { // Resource represents the Kubernetes resource to be propagated. Resource ObjectReference `json:\"resource\"` // ReplicaRequirements represents the requirements required by each replica. // +optional ReplicaRequirements *ReplicaRequirements `json:\"replicaRequirements,omitempty\"` // Replicas represents the replica number of the referencing resource. // +optional Replicas int32 `json:\"replicas,omitempty\"` // Clusters represents target member clusters where the resource to be deployed. // +optional Clusters []TargetCluster `json:\"clusters,omitempty\"` } // ReplicaRequirements represents the requirements required by each replica. type ReplicaRequirements struct { // NodeClaim represents the node claim HardNodeAffinity, NodeSelector and Tolerations required by each replica. // +optional NodeClaim *NodeClaim `json:\"nodeClaim,omitempty\"` // ResourceRequest represents the resources required by each replica. // +optional ResourceRequest corev1.ResourceList `json:\"resourceRequest,omitempty\"` } // NodeClaim represents the node claim HardNodeAffinity, NodeSelector and Tolerations required by each replica. type NodeClaim struct { // A node selector represents the union of the results of one or more label queries over a set of // nodes; that is, it represents the OR of the selectors represented by the node selector terms. // Note that only PodSpec.Affinity.NodeAffinity.RequiredDuringSchedulingIgnoredDuringExecution // is included here because it has a hard limit on pod scheduling. // +optional HardNodeAffinity *corev1.NodeSelector `json:\"hardNodeAffinity,omitempty\"` // NodeSelector is a selector which must be true for the pod to fit on a node. // Selector which must match a node's labels for the pod to be scheduled on that node. // +optional NodeSelector map[string]string `json:\"nodeSelector,omitempty\"` // If specified, the pod's tolerations. // +optional Tolerations []corev1.Toleration `json:\"tolerations,omitempty\"` }","title":"API Changes"},{"location":"proposals/scheduling/521-scheduler-estimator/#karmada-scheduler","text":"First, the existing plugins in Karmada Scheduler such as ClusterAffinity, APIInstalled and TaintToleration will select the suitable clusters. Based on this prefilter result, when assigning replicas, the Karmada Scheduler could try to calculate cluster max available replicas by starting gRPC requests concurrently to the Cluster Accurate Scheduler Estimator. At last, the Cluster Accurate Scheduler Estimator will soon return how many available replicas that the cluster could produce. Then the Karmada Scheduler assgin replicas into different clusters in terms of the estimation result. We could implement this by modifying function calClusterAvailableReplicas to a interface. The previous estimation method, based on ResourceSummary in Cluster.Status , is able to be a default normal estimation approach. Now we could just add a switch to determine whether Cluster Accurate Scheduler Estimator is applied, while the estimator via ResourceSummary could be a default one that does not support disabled. In the future, after the scheduler profile is added, a user could customize the config by using a profile. Furthermore, replica estimation can be considered as a new scheduler plugin.","title":"Karmada Scheduler"},{"location":"proposals/scheduling/521-scheduler-estimator/#karmada-cluster-accurate-scheduler-estimator","text":"Cluster Accurate Scheduler Estimator is a independent component that works as a gRPC server. Before its server starts, a pod and node informer associated with a member cluster will be created as a cache. Once the cache has been synced, the gRPC server would start and serve the incoming scheduler request as a replica estimator. Each Cluster Accurate Scheduler Estimator serves for one cluster, as same as karmada-agent . There are five steps for a scheduler estimation: Verify whether the request meets the requirements. Find all nodes that matches the node claim. List nodes by label selector. Filter nodes by node affinity. Filter schedulable nodes by taints and tolerations. Estimate max available replicas in every filter node. Get pods that assigned to the node. Calculate node idle resource. Calculate how many replicas that the node can be divided into, marked as r1 . Calculate the maximum remaining pods that the node allows, marked as r2 . Return the max available replicas --> min(r1, r2) . Return the sum of all node max available replicas.","title":"Karmada Cluster Accurate Scheduler Estimator"},{"location":"proposals/scheduling/521-scheduler-estimator/#test-plan","text":"Unit Test covering: Core changes in Karmada Scheduler that consists of gRPC connection establishment, replica estimation request sending. Core changes in Karmada Cluster Accurate Scheduler Estimator that consists of node filtering and node idle resource calculation. E2E Test covering: Deploy Karmada Cluster Accurate Scheduler Estimator. Specify different node claim in a workload and test the scheduler result.","title":"Test Plan"},{"location":"proposals/scheduling/697-descheduler/","text":"Descheduler for Karmada Summary Scheduling in Karmada is the process of binding ResourceBindings to target clusters and assigning replicas, and is performed by a component of Karmada called karmada-scheduler. However, the scheduler's decisions are influenced by its view of Karmada at that point of time when a new ResourceBinding appears for scheduling. As Karmada multi-clusters are very dynamic and their state changes over time, there may be desire to move already running replicas to some other clusters for various reasons: Some clusters are under or over utilized. New clusters are added to clusters. Some nodes of a cluster failed and the cluster do not have enough resource to accommodate their pods. This KEP proposes a new component, Descheduler, which has a similar behavior with Descheduler for Kubernetes , based on its policy, finds some replicas that can be moved and evicts them from a cluster. Please note, in current implementation, descheduler does not reschedule evicted replicas but relies on the default karmada-scheduler for that. After evicting part of replicas, karmada-scheduler will start a ScaleSchedule . Motivation Now scheduler only selects clusters and assigns replicas based on the current state of Karmada control-plane and member clusters. But as the state changes over time, the scheduling result may not be optimal. Descheduler helps evict some replicas and trigger a ScaleSchedule action of karmada-scheduler. Goals Provide some strategies to evict replicas of target scheduled clusters, i.e., reduce the number of replica in some target clusters. Non-Goals Make some adaptation of karmada-scheduler-estimator for estimating the evicting replicas. Proposal This proposal introduces a new component to evicting some replicas of a workload. After scheduling, the relevant ResourceBinding would have a scheduling result in resourcebindings.spec.clusters . This field is an array, consisting of the target scheduled cluster and its assigned replicas. The only thing the descheduler have to do is detect and evict some replicas that match some strategies, i.e., reduce the number of replica in some target clusters. User Stories (Optional) Story 1 Imagine that we have a deployment with spec.replicas=10 . It has been scheduled towards three clusters: member1: spec.replicas=5 member2: spec.replicas=3 member3: spec.replicas=2 Now some nodes of member1 failed, leading to 3 replicas eviction after timeout. However, the cluster does not have any more idle resource to accommodate the new pending pods. Descheduler helps detect these pods and evict them, as a result of member1 spec.replicas=2 . After that, karmada-scheduler will start a scale scheduling. Story 2 Imagine that we have a deployment with spec.replicas=12 . It has been scheduled towards three clusters: member1: spec.replicas=4 member2: spec.replicas=4 member3: spec.replicas=4 Now we add a new cluster member4. We may want to reschedule some replicas towards member4 to balance the load. Descheduler helps evict some replicas in member1, member2 and member3. Design Details Architecture It is noticed that this design only focus on User Story 1, which means that only unscheduable pods are included for descheduling, usually happening when cluster resources are insufficient. Other stragety is not considered in this proposal because it needs more discussion. Here is the descheduler workflow. Descheduler: Start and load the strategy file. Descheduler: Watch ResourceBinding and Cluster . Descheduler: Establish connections with all scheduler-estimators . Descheduler: Run regularly in a fixed interval. List all ResourceBindings . Find the non-desired ones, i.e., whose desired replicas are less than ready replicas, and then request scheduler-estimator to get the replicas that need to be evicted. Get the unscheduable pods number from every cluster scheduler-estimator. Modify scheduling result in resourcebindings.spec.clusters for eviction, i.e., substract unscheduable pods number in the relevant field resourcebindings.spec.clusters . Scheduler: Watch the ResourceBinding and begin to scale scheduling. API Change Add a new field in propagationpolicy.spec.placement.replicaScheduling to represent which strategy the workload wants to use. If not specified, the workload should never be rescheduled. type ReschedulingPolicy string const ( // ReschedulingPolicyUnscheduable means rescheduling replicas when unscheduable. ReschedulingPolicyUnscheduable ReschedulingPolicy = \"OnUnscheduable\" // ReschedulingPolicyNever means never rescheduling replicas between member clusters. ReschedulingPolicyNever ReschedulingPolicy = \"Never\" ) // ReplicaSchedulingStrategy represents the assignment strategy of replicas. type ReplicaSchedulingStrategy struct { // ReplicaSchedulingType determines how the replicas is scheduled when karmada propagating // a resource. Valid options are Duplicated and Divided. // \"Duplicated\" duplicates the same replicas to each candidate member cluster from resource. // \"Divided\" divides replicas into parts according to number of valid candidate member // clusters, and exact replicas for each cluster are determined by ReplicaDivisionPreference. // +kubebuilder:validation:Enum=Duplicated;Divided // +optional ReplicaSchedulingType ReplicaSchedulingType `json:\"replicaSchedulingType,omitempty\"` // ReplicaDivisionPreference determines how the replicas is divided // when ReplicaSchedulingType is \"Divided\". Valid options are Aggregated and Weighted. // \"Aggregated\" divides replicas into clusters as few as possible, // while respecting clusters' resource availabilities during the division. // \"Weighted\" divides replicas by weight according to WeightPreference. // +kubebuilder:validation:Enum=Aggregated;Weighted // +optional ReplicaDivisionPreference ReplicaDivisionPreference `json:\"replicaDivisionPreference,omitempty\"` // WeightPreference describes weight for each cluster or for each group of cluster // If ReplicaDivisionPreference is set to \"Weighted\", and WeightPreference is not set, scheduler will weight all clusters the same. // +optional WeightPreference *ClusterPreferences `json:\"weightPreference,omitempty\"` // ReschedulingPolicy describes which rescheduling strategies the workload wants to apply. // If not specified, the workload should never be rescheduled. // +optional ReschedulingPolicy ReschedulingPolicy `json:\"reschedulingPolicy,omitempty\"` } Scheduler Estimator The descheduler has a close relationship with scheduler-estimator. In this case, the scheduler estimator should add a new service interface to estimate the unscheduable replicas that belong to a workload. The gRPC interface shows below. // UnschedulableReplicasRequest represents the request that sent by gRPC client to calculate unschedulable replicas. type UnschedulableReplicasRequest struct { // Cluster represents the cluster name. // +required Cluster string `json:\"cluster\" protobuf:\"bytes,1,opt,name=cluster\"` // Resource represents the Kubernetes resource to be propagated. // +required Resource ObjectReference `json:\"resource\" protobuf:\"bytes,2,opt,name=resource\"` // UnscheduableThreshold represents the threshold period to distinguish whether the replica is unschedulable. // +optional UnscheduableThreshold time.Duration `json:\"unscheduableThreshold,omitempty\" protobuf:\"varint,3,opt,name=unscheduableThreshold,casttype=time.Duration\"` } // UnschedulableReplicasResponse represents the response that sent by gRPC server to calculate unscheduable replicas. type UnschedulableReplicasResponse struct { // UnschedulableReplicas represents the unscheduable replicas that the object manages. // +required UnschedulableReplicas int32 `json:\"unschedulableReplicas\" protobuf:\"varint,1,opt,name=maxReplicas\"` } And the RPC function will be like this: rpc UnschedulableReplicas(UnschedulableReplicasRequest) returns (UnschedulableReplicasResponse) {} Test Plan Unit Test covering: Core changes in karmada-scheduler-estimator that consists of estimating the unschedulable replicas of a workload correctly. Core changes in karmada-descheduler that consists of evicting unschedulable replicas when a workload needs reschedule. E2E Test covering: Deploy karmada-descheduler. Rescheduling replicas when resources are insufficient.","title":"Descheduler for Karmada"},{"location":"proposals/scheduling/697-descheduler/#descheduler-for-karmada","text":"","title":"Descheduler for Karmada"},{"location":"proposals/scheduling/697-descheduler/#summary","text":"Scheduling in Karmada is the process of binding ResourceBindings to target clusters and assigning replicas, and is performed by a component of Karmada called karmada-scheduler. However, the scheduler's decisions are influenced by its view of Karmada at that point of time when a new ResourceBinding appears for scheduling. As Karmada multi-clusters are very dynamic and their state changes over time, there may be desire to move already running replicas to some other clusters for various reasons: Some clusters are under or over utilized. New clusters are added to clusters. Some nodes of a cluster failed and the cluster do not have enough resource to accommodate their pods. This KEP proposes a new component, Descheduler, which has a similar behavior with Descheduler for Kubernetes , based on its policy, finds some replicas that can be moved and evicts them from a cluster. Please note, in current implementation, descheduler does not reschedule evicted replicas but relies on the default karmada-scheduler for that. After evicting part of replicas, karmada-scheduler will start a ScaleSchedule .","title":"Summary"},{"location":"proposals/scheduling/697-descheduler/#motivation","text":"Now scheduler only selects clusters and assigns replicas based on the current state of Karmada control-plane and member clusters. But as the state changes over time, the scheduling result may not be optimal. Descheduler helps evict some replicas and trigger a ScaleSchedule action of karmada-scheduler.","title":"Motivation"},{"location":"proposals/scheduling/697-descheduler/#goals","text":"Provide some strategies to evict replicas of target scheduled clusters, i.e., reduce the number of replica in some target clusters.","title":"Goals"},{"location":"proposals/scheduling/697-descheduler/#non-goals","text":"Make some adaptation of karmada-scheduler-estimator for estimating the evicting replicas.","title":"Non-Goals"},{"location":"proposals/scheduling/697-descheduler/#proposal","text":"This proposal introduces a new component to evicting some replicas of a workload. After scheduling, the relevant ResourceBinding would have a scheduling result in resourcebindings.spec.clusters . This field is an array, consisting of the target scheduled cluster and its assigned replicas. The only thing the descheduler have to do is detect and evict some replicas that match some strategies, i.e., reduce the number of replica in some target clusters.","title":"Proposal"},{"location":"proposals/scheduling/697-descheduler/#user-stories-optional","text":"","title":"User Stories (Optional)"},{"location":"proposals/scheduling/697-descheduler/#story-1","text":"Imagine that we have a deployment with spec.replicas=10 . It has been scheduled towards three clusters: member1: spec.replicas=5 member2: spec.replicas=3 member3: spec.replicas=2 Now some nodes of member1 failed, leading to 3 replicas eviction after timeout. However, the cluster does not have any more idle resource to accommodate the new pending pods. Descheduler helps detect these pods and evict them, as a result of member1 spec.replicas=2 . After that, karmada-scheduler will start a scale scheduling.","title":"Story 1"},{"location":"proposals/scheduling/697-descheduler/#story-2","text":"Imagine that we have a deployment with spec.replicas=12 . It has been scheduled towards three clusters: member1: spec.replicas=4 member2: spec.replicas=4 member3: spec.replicas=4 Now we add a new cluster member4. We may want to reschedule some replicas towards member4 to balance the load. Descheduler helps evict some replicas in member1, member2 and member3.","title":"Story 2"},{"location":"proposals/scheduling/697-descheduler/#design-details","text":"","title":"Design Details"},{"location":"proposals/scheduling/697-descheduler/#architecture","text":"It is noticed that this design only focus on User Story 1, which means that only unscheduable pods are included for descheduling, usually happening when cluster resources are insufficient. Other stragety is not considered in this proposal because it needs more discussion. Here is the descheduler workflow. Descheduler: Start and load the strategy file. Descheduler: Watch ResourceBinding and Cluster . Descheduler: Establish connections with all scheduler-estimators . Descheduler: Run regularly in a fixed interval. List all ResourceBindings . Find the non-desired ones, i.e., whose desired replicas are less than ready replicas, and then request scheduler-estimator to get the replicas that need to be evicted. Get the unscheduable pods number from every cluster scheduler-estimator. Modify scheduling result in resourcebindings.spec.clusters for eviction, i.e., substract unscheduable pods number in the relevant field resourcebindings.spec.clusters . Scheduler: Watch the ResourceBinding and begin to scale scheduling.","title":"Architecture"},{"location":"proposals/scheduling/697-descheduler/#api-change","text":"Add a new field in propagationpolicy.spec.placement.replicaScheduling to represent which strategy the workload wants to use. If not specified, the workload should never be rescheduled. type ReschedulingPolicy string const ( // ReschedulingPolicyUnscheduable means rescheduling replicas when unscheduable. ReschedulingPolicyUnscheduable ReschedulingPolicy = \"OnUnscheduable\" // ReschedulingPolicyNever means never rescheduling replicas between member clusters. ReschedulingPolicyNever ReschedulingPolicy = \"Never\" ) // ReplicaSchedulingStrategy represents the assignment strategy of replicas. type ReplicaSchedulingStrategy struct { // ReplicaSchedulingType determines how the replicas is scheduled when karmada propagating // a resource. Valid options are Duplicated and Divided. // \"Duplicated\" duplicates the same replicas to each candidate member cluster from resource. // \"Divided\" divides replicas into parts according to number of valid candidate member // clusters, and exact replicas for each cluster are determined by ReplicaDivisionPreference. // +kubebuilder:validation:Enum=Duplicated;Divided // +optional ReplicaSchedulingType ReplicaSchedulingType `json:\"replicaSchedulingType,omitempty\"` // ReplicaDivisionPreference determines how the replicas is divided // when ReplicaSchedulingType is \"Divided\". Valid options are Aggregated and Weighted. // \"Aggregated\" divides replicas into clusters as few as possible, // while respecting clusters' resource availabilities during the division. // \"Weighted\" divides replicas by weight according to WeightPreference. // +kubebuilder:validation:Enum=Aggregated;Weighted // +optional ReplicaDivisionPreference ReplicaDivisionPreference `json:\"replicaDivisionPreference,omitempty\"` // WeightPreference describes weight for each cluster or for each group of cluster // If ReplicaDivisionPreference is set to \"Weighted\", and WeightPreference is not set, scheduler will weight all clusters the same. // +optional WeightPreference *ClusterPreferences `json:\"weightPreference,omitempty\"` // ReschedulingPolicy describes which rescheduling strategies the workload wants to apply. // If not specified, the workload should never be rescheduled. // +optional ReschedulingPolicy ReschedulingPolicy `json:\"reschedulingPolicy,omitempty\"` }","title":"API Change"},{"location":"proposals/scheduling/697-descheduler/#scheduler-estimator","text":"The descheduler has a close relationship with scheduler-estimator. In this case, the scheduler estimator should add a new service interface to estimate the unscheduable replicas that belong to a workload. The gRPC interface shows below. // UnschedulableReplicasRequest represents the request that sent by gRPC client to calculate unschedulable replicas. type UnschedulableReplicasRequest struct { // Cluster represents the cluster name. // +required Cluster string `json:\"cluster\" protobuf:\"bytes,1,opt,name=cluster\"` // Resource represents the Kubernetes resource to be propagated. // +required Resource ObjectReference `json:\"resource\" protobuf:\"bytes,2,opt,name=resource\"` // UnscheduableThreshold represents the threshold period to distinguish whether the replica is unschedulable. // +optional UnscheduableThreshold time.Duration `json:\"unscheduableThreshold,omitempty\" protobuf:\"varint,3,opt,name=unscheduableThreshold,casttype=time.Duration\"` } // UnschedulableReplicasResponse represents the response that sent by gRPC server to calculate unscheduable replicas. type UnschedulableReplicasResponse struct { // UnschedulableReplicas represents the unscheduable replicas that the object manages. // +required UnschedulableReplicas int32 `json:\"unschedulableReplicas\" protobuf:\"varint,1,opt,name=maxReplicas\"` } And the RPC function will be like this: rpc UnschedulableReplicas(UnschedulableReplicasRequest) returns (UnschedulableReplicasResponse) {}","title":"Scheduler Estimator"},{"location":"proposals/scheduling/697-descheduler/#test-plan","text":"Unit Test covering: Core changes in karmada-scheduler-estimator that consists of estimating the unschedulable replicas of a workload correctly. Core changes in karmada-descheduler that consists of evicting unschedulable replicas when a workload needs reschedule. E2E Test covering: Deploy karmada-descheduler. Rescheduling replicas when resources are insufficient.","title":"Test Plan"},{"location":"upgrading/","text":"Table of Contents generated with DocToc Upgrading Instruction Overview Regular Upgrading Process Upgrading APIs Manual Upgrade API Upgrading Components Details Upgrading Instruction v0.8 to v0.9 v0.9 to v0.10 v0.10 to v1.0 v1.0 to v1.1 v1.1 to v1.2 Upgrading Instruction Overview Karmada uses the semver versioning and each version in the format of v MAJOR . MINOR . PATCH : - The PATCH release does not introduce breaking changes. - The MINOR release might introduce minor breaking changes with a workaround. - The Major release might introduce backward-incompatible behavior changes. Regular Upgrading Process Upgrading APIs For releases that introduce API changes, the Karmada API(CRD) that Karmada components rely on must upgrade to keep consistent. Karmada CRD is composed of two parts: - bases: The CRD definition generated via API structs. - patches: conversion settings for the CRD. In order to support multiple versions of custom resources, the patches should be injected into bases . To achieve this we introduced a kustomization.yaml configuration then use kubectl kustomize to build the final CRD. The bases , patches and kustomization.yaml now located at charts/_crds directory of the repo. Manual Upgrade API Step 1: Get the Webhook CA certificate The CA certificate will be injected into patches before building the final CRD. We can retrieve it from the MutatingWebhookConfiguration or ValidatingWebhookConfiguration configurations, e.g: kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io mutating-config Copy the ca_string from the yaml path webhooks.name[x].clientConfig.caBundle , then replace the {{caBundle}} from the yaml files in patches . e.g: sed -i'' -e \"s/{{caBundle}}/${ca_string}/g\" ./\"charts/_crds/patches/webhook_in_resourcebindings.yaml\" sed -i'' -e \"s/{{caBundle}}/${ca_string}/g\" ./\"charts/_crds/patches/webhook_in_clusterresourcebindings.yaml\" Step2: Build final CRD Generate the final CRD by kubectl kustomize command, e.g: kubectl kustomize ./charts/_crds Or, you can apply to karmada-apiserver by: kubectl kustomize ./charts/_crds | kubectl apply -f - Upgrading Components Components upgrading is composed of image version update and possible command args changes. For the argument changes please refer to Details Upgrading Instruction below. Details Upgrading Instruction The following instructions are for minor version upgrades. Cross-version upgrades are not recommended. And it is recommended to use the latest patch version when upgrading, for example, if you are upgrading from v1.1.x to v1.2.x and the available patch versions are v1.2.0, v1.2.1 and v1.2.2, then select v1.2.2. v0.8 to v0.9 v0.9 to v0.10 v0.10 to v1.0 v1.0 to v1.1 v1.1 to v1.2","title":"Index"},{"location":"upgrading/#upgrading-instruction","text":"","title":"Upgrading Instruction"},{"location":"upgrading/#overview","text":"Karmada uses the semver versioning and each version in the format of v MAJOR . MINOR . PATCH : - The PATCH release does not introduce breaking changes. - The MINOR release might introduce minor breaking changes with a workaround. - The Major release might introduce backward-incompatible behavior changes.","title":"Overview"},{"location":"upgrading/#regular-upgrading-process","text":"","title":"Regular Upgrading Process"},{"location":"upgrading/#upgrading-apis","text":"For releases that introduce API changes, the Karmada API(CRD) that Karmada components rely on must upgrade to keep consistent. Karmada CRD is composed of two parts: - bases: The CRD definition generated via API structs. - patches: conversion settings for the CRD. In order to support multiple versions of custom resources, the patches should be injected into bases . To achieve this we introduced a kustomization.yaml configuration then use kubectl kustomize to build the final CRD. The bases , patches and kustomization.yaml now located at charts/_crds directory of the repo.","title":"Upgrading APIs"},{"location":"upgrading/#manual-upgrade-api","text":"Step 1: Get the Webhook CA certificate The CA certificate will be injected into patches before building the final CRD. We can retrieve it from the MutatingWebhookConfiguration or ValidatingWebhookConfiguration configurations, e.g: kubectl get mutatingwebhookconfigurations.admissionregistration.k8s.io mutating-config Copy the ca_string from the yaml path webhooks.name[x].clientConfig.caBundle , then replace the {{caBundle}} from the yaml files in patches . e.g: sed -i'' -e \"s/{{caBundle}}/${ca_string}/g\" ./\"charts/_crds/patches/webhook_in_resourcebindings.yaml\" sed -i'' -e \"s/{{caBundle}}/${ca_string}/g\" ./\"charts/_crds/patches/webhook_in_clusterresourcebindings.yaml\" Step2: Build final CRD Generate the final CRD by kubectl kustomize command, e.g: kubectl kustomize ./charts/_crds Or, you can apply to karmada-apiserver by: kubectl kustomize ./charts/_crds | kubectl apply -f -","title":"Manual Upgrade API"},{"location":"upgrading/#upgrading-components","text":"Components upgrading is composed of image version update and possible command args changes. For the argument changes please refer to Details Upgrading Instruction below.","title":"Upgrading Components"},{"location":"upgrading/#details-upgrading-instruction","text":"The following instructions are for minor version upgrades. Cross-version upgrades are not recommended. And it is recommended to use the latest patch version when upgrading, for example, if you are upgrading from v1.1.x to v1.2.x and the available patch versions are v1.2.0, v1.2.1 and v1.2.2, then select v1.2.2.","title":"Details Upgrading Instruction"},{"location":"upgrading/#v08-to-v09","text":"","title":"v0.8 to v0.9"},{"location":"upgrading/#v09-to-v010","text":"","title":"v0.9 to v0.10"},{"location":"upgrading/#v010-to-v10","text":"","title":"v0.10 to v1.0"},{"location":"upgrading/#v10-to-v11","text":"","title":"v1.0 to v1.1"},{"location":"upgrading/#v11-to-v12","text":"","title":"v1.1 to v1.2"},{"location":"upgrading/v0.10-v1.0/","text":"v0.10 to v1.0 Follow the Regular Upgrading Process . Upgrading Notable Changes Introduced karmada-aggregated-apiserver component In the releases before v1.0.0, we are using CRD to extend the Cluster API , and starts v1.0.0 we use API Aggregation (AA) to extend it. Based on the above change, perform the following operations during the upgrade: Step 1: Stop karmada-apiserver You can stop karmada-apiserver by updating its replica to 0 . Step 2: Remove Cluster CRD from ETCD Remove the Cluster CRD from ETCD directly by running the following command. etcdctl --cert=\"/etc/kubernetes/pki/etcd/karmada.crt\" \\ --key=\"/etc/kubernetes/pki/etcd/karmada.key\" \\ --cacert=\"/etc/kubernetes/pki/etcd/server-ca.crt\" \\ del /registry/apiextensions.k8s.io/customresourcedefinitions/clusters.cluster.karmada.io Note: This command only removed the CRD resource, all the CR (Cluster objects) not changed. That's the reason why we don't remove CRD by karmada-apiserver . Step 3: Prepare the certificate for the karmada-aggregated-apiserver To avoid CA Reusage and Conflicts , create CA signer and sign a certificate to enable the aggregation layer. Update karmada-cert-secret secret in karmada-system namespace: apiVersion: v1 kind: Secret metadata: name: karmada-cert-secret namespace: karmada-system type: Opaque data: ... + front-proxy-ca.crt: | + {{front_proxy_ca_crt}} + front-proxy-client.crt: | + {{front_proxy_client_crt}} + front-proxy-client.key: | + {{front_proxy_client_key}} Then update karmada-apiserver deployment's container command: - - --proxy-client-cert-file=/etc/kubernetes/pki/karmada.crt - - --proxy-client-key-file=/etc/kubernetes/pki/karmada.key + - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt + - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key - - --requestheader-client-ca-file=/etc/kubernetes/pki/server-ca.crt + - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt After the update, restore the replicas of karmada-apiserver instances. Step 4: Deploy karmada-aggregated-apiserver : Deploy karmada-aggregated-apiserver instance to your host cluster by following manifests: unfold me to see the yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: karmada-aggregated-apiserver namespace: karmada-system labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: selector: matchLabels: app: karmada-aggregated-apiserver apiserver: \"true\" replicas: 1 template: metadata: labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: automountServiceAccountToken: false containers: - name: karmada-aggregated-apiserver image: swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-aggregated-apiserver:v1.0.0 imagePullPolicy: IfNotPresent volumeMounts: - name: k8s-certs mountPath: /etc/kubernetes/pki readOnly: true - name: kubeconfig subPath: kubeconfig mountPath: /etc/kubeconfig command: - /bin/karmada-aggregated-apiserver - --kubeconfig=/etc/kubeconfig - --authentication-kubeconfig=/etc/kubeconfig - --authorization-kubeconfig=/etc/kubeconfig - --karmada-config=/etc/kubeconfig - --etcd-servers=https://etcd-client.karmada-system.svc.cluster.local:2379 - --etcd-cafile=/etc/kubernetes/pki/server-ca.crt - --etcd-certfile=/etc/kubernetes/pki/karmada.crt - --etcd-keyfile=/etc/kubernetes/pki/karmada.key - --tls-cert-file=/etc/kubernetes/pki/karmada.crt - --tls-private-key-file=/etc/kubernetes/pki/karmada.key - --audit-log-path=- - --feature-gates=APIPriorityAndFairness=false - --audit-log-maxage=0 - --audit-log-maxbackup=0 resources: requests: cpu: 100m volumes: - name: k8s-certs secret: secretName: karmada-cert-secret - name: kubeconfig secret: secretName: kubeconfig --- apiVersion: v1 kind: Service metadata: name: karmada-aggregated-apiserver namespace: karmada-system labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: ports: - port: 443 protocol: TCP targetPort: 443 selector: app: karmada-aggregated-apiserver Then, deploy APIService to karmada-apiserver by following manifests. unfold me to see the yaml apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name: v1alpha1.cluster.karmada.io labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: insecureSkipTLSVerify: true group: cluster.karmada.io groupPriorityMinimum: 2000 service: name: karmada-aggregated-apiserver namespace: karmada-system version: v1alpha1 versionPriority: 10 --- apiVersion: v1 kind: Service metadata: name: karmada-aggregated-apiserver namespace: karmada-system spec: type: ExternalName externalName: karmada-aggregated-apiserver.karmada-system.svc.cluster.local Step 5: check clusters status If everything goes well, you can see all your clusters just as before the upgrading. kubectl get clusters karmada-agent requires an extra impersonate verb In order to proxy user's request, the karmada-agent now request an extra impersonate verb. Please check the ClusterRole configuration or apply the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: karmada-agent rules: - apiGroups: ['*'] resources: ['*'] verbs: ['*'] - nonResourceURLs: ['*'] verbs: [\"get\"] MCS feature now supports Kubernetes v1.21+ Since the discovery.k8s.io/v1beta1 of EndpointSlices has been deprecated in favor of discovery.k8s.io/v1 , in Kubernetes v1.21 , Karmada adopt this change at release v1.0.0. Now the MCS feature requires member cluster version no less than v1.21.","title":"v0.10 to v1.0"},{"location":"upgrading/v0.10-v1.0/#v010-to-v10","text":"Follow the Regular Upgrading Process .","title":"v0.10 to v1.0"},{"location":"upgrading/v0.10-v1.0/#upgrading-notable-changes","text":"","title":"Upgrading Notable Changes"},{"location":"upgrading/v0.10-v1.0/#introduced-karmada-aggregated-apiserver-component","text":"In the releases before v1.0.0, we are using CRD to extend the Cluster API , and starts v1.0.0 we use API Aggregation (AA) to extend it. Based on the above change, perform the following operations during the upgrade:","title":"Introduced karmada-aggregated-apiserver component"},{"location":"upgrading/v0.10-v1.0/#step-1-stop-karmada-apiserver","text":"You can stop karmada-apiserver by updating its replica to 0 .","title":"Step 1: Stop karmada-apiserver"},{"location":"upgrading/v0.10-v1.0/#step-2-remove-cluster-crd-from-etcd","text":"Remove the Cluster CRD from ETCD directly by running the following command. etcdctl --cert=\"/etc/kubernetes/pki/etcd/karmada.crt\" \\ --key=\"/etc/kubernetes/pki/etcd/karmada.key\" \\ --cacert=\"/etc/kubernetes/pki/etcd/server-ca.crt\" \\ del /registry/apiextensions.k8s.io/customresourcedefinitions/clusters.cluster.karmada.io Note: This command only removed the CRD resource, all the CR (Cluster objects) not changed. That's the reason why we don't remove CRD by karmada-apiserver .","title":"Step 2: Remove Cluster CRD from ETCD"},{"location":"upgrading/v0.10-v1.0/#step-3-prepare-the-certificate-for-the-karmada-aggregated-apiserver","text":"To avoid CA Reusage and Conflicts , create CA signer and sign a certificate to enable the aggregation layer. Update karmada-cert-secret secret in karmada-system namespace: apiVersion: v1 kind: Secret metadata: name: karmada-cert-secret namespace: karmada-system type: Opaque data: ... + front-proxy-ca.crt: | + {{front_proxy_ca_crt}} + front-proxy-client.crt: | + {{front_proxy_client_crt}} + front-proxy-client.key: | + {{front_proxy_client_key}} Then update karmada-apiserver deployment's container command: - - --proxy-client-cert-file=/etc/kubernetes/pki/karmada.crt - - --proxy-client-key-file=/etc/kubernetes/pki/karmada.key + - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt + - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key - - --requestheader-client-ca-file=/etc/kubernetes/pki/server-ca.crt + - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt After the update, restore the replicas of karmada-apiserver instances.","title":"Step 3: Prepare the certificate for the karmada-aggregated-apiserver"},{"location":"upgrading/v0.10-v1.0/#step-4-deploy-karmada-aggregated-apiserver","text":"Deploy karmada-aggregated-apiserver instance to your host cluster by following manifests: unfold me to see the yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: karmada-aggregated-apiserver namespace: karmada-system labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: selector: matchLabels: app: karmada-aggregated-apiserver apiserver: \"true\" replicas: 1 template: metadata: labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: automountServiceAccountToken: false containers: - name: karmada-aggregated-apiserver image: swr.ap-southeast-1.myhuaweicloud.com/karmada/karmada-aggregated-apiserver:v1.0.0 imagePullPolicy: IfNotPresent volumeMounts: - name: k8s-certs mountPath: /etc/kubernetes/pki readOnly: true - name: kubeconfig subPath: kubeconfig mountPath: /etc/kubeconfig command: - /bin/karmada-aggregated-apiserver - --kubeconfig=/etc/kubeconfig - --authentication-kubeconfig=/etc/kubeconfig - --authorization-kubeconfig=/etc/kubeconfig - --karmada-config=/etc/kubeconfig - --etcd-servers=https://etcd-client.karmada-system.svc.cluster.local:2379 - --etcd-cafile=/etc/kubernetes/pki/server-ca.crt - --etcd-certfile=/etc/kubernetes/pki/karmada.crt - --etcd-keyfile=/etc/kubernetes/pki/karmada.key - --tls-cert-file=/etc/kubernetes/pki/karmada.crt - --tls-private-key-file=/etc/kubernetes/pki/karmada.key - --audit-log-path=- - --feature-gates=APIPriorityAndFairness=false - --audit-log-maxage=0 - --audit-log-maxbackup=0 resources: requests: cpu: 100m volumes: - name: k8s-certs secret: secretName: karmada-cert-secret - name: kubeconfig secret: secretName: kubeconfig --- apiVersion: v1 kind: Service metadata: name: karmada-aggregated-apiserver namespace: karmada-system labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: ports: - port: 443 protocol: TCP targetPort: 443 selector: app: karmada-aggregated-apiserver Then, deploy APIService to karmada-apiserver by following manifests. unfold me to see the yaml apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: name: v1alpha1.cluster.karmada.io labels: app: karmada-aggregated-apiserver apiserver: \"true\" spec: insecureSkipTLSVerify: true group: cluster.karmada.io groupPriorityMinimum: 2000 service: name: karmada-aggregated-apiserver namespace: karmada-system version: v1alpha1 versionPriority: 10 --- apiVersion: v1 kind: Service metadata: name: karmada-aggregated-apiserver namespace: karmada-system spec: type: ExternalName externalName: karmada-aggregated-apiserver.karmada-system.svc.cluster.local","title":"Step 4: Deploy karmada-aggregated-apiserver:"},{"location":"upgrading/v0.10-v1.0/#step-5-check-clusters-status","text":"If everything goes well, you can see all your clusters just as before the upgrading. kubectl get clusters","title":"Step 5: check clusters status"},{"location":"upgrading/v0.10-v1.0/#karmada-agent-requires-an-extra-impersonate-verb","text":"In order to proxy user's request, the karmada-agent now request an extra impersonate verb. Please check the ClusterRole configuration or apply the following manifest. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: karmada-agent rules: - apiGroups: ['*'] resources: ['*'] verbs: ['*'] - nonResourceURLs: ['*'] verbs: [\"get\"]","title":"karmada-agent requires an extra impersonate verb"},{"location":"upgrading/v0.10-v1.0/#mcs-feature-now-supports-kubernetes-v121","text":"Since the discovery.k8s.io/v1beta1 of EndpointSlices has been deprecated in favor of discovery.k8s.io/v1 , in Kubernetes v1.21 , Karmada adopt this change at release v1.0.0. Now the MCS feature requires member cluster version no less than v1.21.","title":"MCS feature now supports Kubernetes v1.21+"},{"location":"upgrading/v0.8-v0.9/","text":"v0.8 to v0.9 Nothing special other than the Regular Upgrading Process . Upgrading Notable Changes Please refer to v0.9.0 Release Notes for more details.","title":"v0.8 to v0.9"},{"location":"upgrading/v0.8-v0.9/#v08-to-v09","text":"Nothing special other than the Regular Upgrading Process .","title":"v0.8 to v0.9"},{"location":"upgrading/v0.8-v0.9/#upgrading-notable-changes","text":"Please refer to v0.9.0 Release Notes for more details.","title":"Upgrading Notable Changes"},{"location":"upgrading/v0.9-v0.10/","text":"v0.9 to v0.10 Follow the Regular Upgrading Process . Upgrading Notable Changes karmada-scheduler The --failover flag has been removed and replaced by --feature-gates . If you enable fail over feature by --failover , now should be change to --feature-gates=Failover=true . Please refer to v0.10.0 Release Notes for more details.","title":"v0.9 to v0.10"},{"location":"upgrading/v0.9-v0.10/#v09-to-v010","text":"Follow the Regular Upgrading Process .","title":"v0.9 to v0.10"},{"location":"upgrading/v0.9-v0.10/#upgrading-notable-changes","text":"","title":"Upgrading Notable Changes"},{"location":"upgrading/v0.9-v0.10/#karmada-scheduler","text":"The --failover flag has been removed and replaced by --feature-gates . If you enable fail over feature by --failover , now should be change to --feature-gates=Failover=true . Please refer to v0.10.0 Release Notes for more details.","title":"karmada-scheduler"},{"location":"upgrading/v1.0-v1.1/","text":"v1.0 to v1.1 Follow the Regular Upgrading Process . Upgrading Notable Changes The validation process for Cluster objects now has been moved from karmada-webhook to karmada-aggregated-apiserver by PR 1152 , you have to remove the Cluster webhook configuration from ValidatingWebhookConfiguration , such as: diff --git a/artifacts/deploy/webhook-configuration.yaml b/artifacts/deploy/webhook-configuration.yaml index 0a89ad36..f7a9f512 100644 --- a/artifacts/deploy/webhook-configuration.yaml +++ b/artifacts/deploy/webhook-configuration.yaml @@ -69,20 +69,6 @@ metadata: labels: app: validating-config webhooks: - - name: cluster.karmada.io - rules: - - operations: [\"CREATE\", \"UPDATE\"] - apiGroups: [\"cluster.karmada.io\"] - apiVersions: [\"*\"] - resources: [\"clusters\"] - scope: \"Cluster\" - clientConfig: - url: https://karmada-webhook.karmada-system.svc:443/validate-cluster - caBundle: {{caBundle}} - failurePolicy: Fail - sideEffects: None - admissionReviewVersions: [\"v1\"] - timeoutSeconds: 3 - name: propagationpolicy.karmada.io rules: - operations: [\"CREATE\", \"UPDATE\"] Otherwise, when joining clusters(or updating Cluster objects) the request will be rejected with following errors: Error: failed to create cluster(host) object. error: Internal error occurred: failed calling webhook \"cluster.karmada.io\": the server could not find the requested resource Please refer to v1.1.0 Release Notes for more details.","title":"v1.0 to v1.1"},{"location":"upgrading/v1.0-v1.1/#v10-to-v11","text":"Follow the Regular Upgrading Process .","title":"v1.0 to v1.1"},{"location":"upgrading/v1.0-v1.1/#upgrading-notable-changes","text":"The validation process for Cluster objects now has been moved from karmada-webhook to karmada-aggregated-apiserver by PR 1152 , you have to remove the Cluster webhook configuration from ValidatingWebhookConfiguration , such as: diff --git a/artifacts/deploy/webhook-configuration.yaml b/artifacts/deploy/webhook-configuration.yaml index 0a89ad36..f7a9f512 100644 --- a/artifacts/deploy/webhook-configuration.yaml +++ b/artifacts/deploy/webhook-configuration.yaml @@ -69,20 +69,6 @@ metadata: labels: app: validating-config webhooks: - - name: cluster.karmada.io - rules: - - operations: [\"CREATE\", \"UPDATE\"] - apiGroups: [\"cluster.karmada.io\"] - apiVersions: [\"*\"] - resources: [\"clusters\"] - scope: \"Cluster\" - clientConfig: - url: https://karmada-webhook.karmada-system.svc:443/validate-cluster - caBundle: {{caBundle}} - failurePolicy: Fail - sideEffects: None - admissionReviewVersions: [\"v1\"] - timeoutSeconds: 3 - name: propagationpolicy.karmada.io rules: - operations: [\"CREATE\", \"UPDATE\"] Otherwise, when joining clusters(or updating Cluster objects) the request will be rejected with following errors: Error: failed to create cluster(host) object. error: Internal error occurred: failed calling webhook \"cluster.karmada.io\": the server could not find the requested resource Please refer to v1.1.0 Release Notes for more details.","title":"Upgrading Notable Changes"},{"location":"upgrading/v1.1-v1.2/","text":"v1.1 to v1.2 Follow the Regular Upgrading Process . Upgrading Notable Changes karmada-controller-manager The hpa controller has been disabled by default now, if you are using this controller, please enable it as per Configure Karmada controllers . karmada-aggregated-apiserver The deprecated flags --karmada-config and --master in v1.1 have been removed from the codebase. Please remember to remove the flags --karmada-config and --master in the karmada-aggregated-apiserver deployment yaml. karmadactl We enable karmadactl promote command to support AA. For details info, please refer to 1795 . In order to use AA by default, need to deploy some RBAC by following manifests. unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-admin rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-admin subjects: - kind: User name: \"system:admin\" Please refer to v1.2.0 Release Notes for more details.","title":"v1.1 to v1.2"},{"location":"upgrading/v1.1-v1.2/#v11-to-v12","text":"Follow the Regular Upgrading Process .","title":"v1.1 to v1.2"},{"location":"upgrading/v1.1-v1.2/#upgrading-notable-changes","text":"","title":"Upgrading Notable Changes"},{"location":"upgrading/v1.1-v1.2/#karmada-controller-manager","text":"The hpa controller has been disabled by default now, if you are using this controller, please enable it as per Configure Karmada controllers .","title":"karmada-controller-manager"},{"location":"upgrading/v1.1-v1.2/#karmada-aggregated-apiserver","text":"The deprecated flags --karmada-config and --master in v1.1 have been removed from the codebase. Please remember to remove the flags --karmada-config and --master in the karmada-aggregated-apiserver deployment yaml.","title":"karmada-aggregated-apiserver"},{"location":"upgrading/v1.1-v1.2/#karmadactl","text":"We enable karmadactl promote command to support AA. For details info, please refer to 1795 . In order to use AA by default, need to deploy some RBAC by following manifests. unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-admin rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-admin subjects: - kind: User name: \"system:admin\" Please refer to v1.2.0 Release Notes for more details.","title":"karmadactl"},{"location":"userguide/aggregated-api-endpoint/","text":"Aggregated Kubernetes API Endpoint The newly introduced karmada-aggregated-apiserver component aggregates all registered clusters and allows users to access member clusters through Karmada by the proxy endpoint, For detailed discussion topic, see here . Here's a quick start. Quick start To quickly experience this feature, we experimented with karmada-apiserver certificate. Step1: Obtain the karmada-apiserver Certificate For karmada deployed using hack/local-up-karmada.sh , you can directly copy it from the /root/.kube/ directory. cp /root/.kube/karmada.config karmada-apiserver.config Step2: Grant permission to user system:admin system:admin is the user for karmada-apiserver certificate. We need to grant the clusters/proxy permission to it explicitly. Apply the following yaml file: cluster-proxy-rbac.yaml: unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy resourceNames: - member1 - member2 - member3 verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: User name: \"system:admin\" kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml Step3: Access member clusters Run the below command (replace {clustername} with your actual cluster name): kubectl --kubeconfig karmada-apiserver.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy/api/v1/nodes Or append /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy to the server address of karmada-apiserver.config, and then you can directly use: kubectl --kubeconfig karmada-apiserver.config get node Note: For a member cluster that joins karmada in pull mode and allows only cluster-to-karmada access, we can deploy apiserver-network-proxy (ANP) to access it. Unified authentication For one or a group of user subjects (users, groups, or service accounts) in a member cluster, we can import them into karmada control plane and grant them the clusters/proxy permission, so that we can access the member cluster with permission of the user subject through karmada. In this section, we use a serviceaccount named tom for the test. Step1: Create ServiceAccount in member1 cluster (optional) If the serviceaccount has been created in your environment, you can skip this step. Create a serviceaccount that does not have any permission: kubectl --kubeconfig /root/.kube/members.config --context member1 create serviceaccount tom Step2: Create ServiceAccount in karmada control plane kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver create serviceaccount tom In order to grant serviceaccount the clusters/proxy permission, apply the following rbac yaml file: cluster-proxy-rbac.yaml: unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy resourceNames: - member1 verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: ServiceAccount name: tom namespace: default # The token generated by the serviceaccount can parse the group information. Therefore, you need to specify the group information below. - kind: Group name: \"system:serviceaccounts\" - kind: Group name: \"system:serviceaccounts:default\" kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml Step3: Access member1 cluster Obtain token of serviceaccount tom : kubectl get secret `kubectl get sa tom -oyaml | grep token | awk '{print $3}'` -oyaml | grep token: | awk '{print $2}' | base64 -d Then construct a kubeconfig file tom.config for tom serviceaccount: apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: {karmada-apiserver-address} # Replace {karmada-apiserver-address} with karmada-apiserver-address. You can find it in /root/.kube/karmada.config file. name: tom contexts: - context: cluster: tom user: tom name: tom current-context: tom kind: Config users: - name: tom user: token: {token} # Replace {token} with the token obtain above. Run the command below to access member1 cluster: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/apis We can found that we were able to access, but run the command below: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes It will fail because serviceaccount tom does not have any permissions in the member1 cluster. Step4: Grant permission to Serviceaccount in member1 cluster Apply the following YAML file: member1-rbac.yaml unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: tom rules: - apiGroups: - '*' resources: - '*' verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tom roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: tom subjects: - kind: ServiceAccount name: tom namespace: default kubectl --kubeconfig /root/.kube/members.config --context member1 apply -f member1-rbac.yaml Run the command that failed in the previous step again: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes The access will be successful. Or we can append /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy to the server address of tom.config , and then you can directly use: kubectl --kubeconfig tom.config get node Note: For a member cluster that joins karmada in pull mode and allows only cluster-to-karmada access, we can deploy apiserver-network-proxy (ANP) to access it.","title":"aggregated-api-endpoint"},{"location":"userguide/aggregated-api-endpoint/#aggregated-kubernetes-api-endpoint","text":"The newly introduced karmada-aggregated-apiserver component aggregates all registered clusters and allows users to access member clusters through Karmada by the proxy endpoint, For detailed discussion topic, see here . Here's a quick start.","title":"Aggregated Kubernetes API Endpoint"},{"location":"userguide/aggregated-api-endpoint/#quick-start","text":"To quickly experience this feature, we experimented with karmada-apiserver certificate.","title":"Quick start"},{"location":"userguide/aggregated-api-endpoint/#step1-obtain-the-karmada-apiserver-certificate","text":"For karmada deployed using hack/local-up-karmada.sh , you can directly copy it from the /root/.kube/ directory. cp /root/.kube/karmada.config karmada-apiserver.config","title":"Step1: Obtain the karmada-apiserver Certificate"},{"location":"userguide/aggregated-api-endpoint/#step2-grant-permission-to-user-systemadmin","text":"system:admin is the user for karmada-apiserver certificate. We need to grant the clusters/proxy permission to it explicitly. Apply the following yaml file: cluster-proxy-rbac.yaml: unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy resourceNames: - member1 - member2 - member3 verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: User name: \"system:admin\" kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml","title":"Step2: Grant permission to user system:admin"},{"location":"userguide/aggregated-api-endpoint/#step3-access-member-clusters","text":"Run the below command (replace {clustername} with your actual cluster name): kubectl --kubeconfig karmada-apiserver.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy/api/v1/nodes Or append /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy to the server address of karmada-apiserver.config, and then you can directly use: kubectl --kubeconfig karmada-apiserver.config get node Note: For a member cluster that joins karmada in pull mode and allows only cluster-to-karmada access, we can deploy apiserver-network-proxy (ANP) to access it.","title":"Step3: Access member clusters"},{"location":"userguide/aggregated-api-endpoint/#unified-authentication","text":"For one or a group of user subjects (users, groups, or service accounts) in a member cluster, we can import them into karmada control plane and grant them the clusters/proxy permission, so that we can access the member cluster with permission of the user subject through karmada. In this section, we use a serviceaccount named tom for the test.","title":"Unified authentication"},{"location":"userguide/aggregated-api-endpoint/#step1-create-serviceaccount-in-member1-cluster-optional","text":"If the serviceaccount has been created in your environment, you can skip this step. Create a serviceaccount that does not have any permission: kubectl --kubeconfig /root/.kube/members.config --context member1 create serviceaccount tom","title":"Step1: Create ServiceAccount in member1 cluster (optional)"},{"location":"userguide/aggregated-api-endpoint/#step2-create-serviceaccount-in-karmada-control-plane","text":"kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver create serviceaccount tom In order to grant serviceaccount the clusters/proxy permission, apply the following rbac yaml file: cluster-proxy-rbac.yaml: unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - 'cluster.karmada.io' resources: - clusters/proxy resourceNames: - member1 verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: ServiceAccount name: tom namespace: default # The token generated by the serviceaccount can parse the group information. Therefore, you need to specify the group information below. - kind: Group name: \"system:serviceaccounts\" - kind: Group name: \"system:serviceaccounts:default\" kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml","title":"Step2: Create ServiceAccount in karmada control plane"},{"location":"userguide/aggregated-api-endpoint/#step3-access-member1-cluster","text":"Obtain token of serviceaccount tom : kubectl get secret `kubectl get sa tom -oyaml | grep token | awk '{print $3}'` -oyaml | grep token: | awk '{print $2}' | base64 -d Then construct a kubeconfig file tom.config for tom serviceaccount: apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: {karmada-apiserver-address} # Replace {karmada-apiserver-address} with karmada-apiserver-address. You can find it in /root/.kube/karmada.config file. name: tom contexts: - context: cluster: tom user: tom name: tom current-context: tom kind: Config users: - name: tom user: token: {token} # Replace {token} with the token obtain above. Run the command below to access member1 cluster: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/apis We can found that we were able to access, but run the command below: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes It will fail because serviceaccount tom does not have any permissions in the member1 cluster.","title":"Step3: Access member1 cluster"},{"location":"userguide/aggregated-api-endpoint/#step4-grant-permission-to-serviceaccount-in-member1-cluster","text":"Apply the following YAML file: member1-rbac.yaml unfold me to see the yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: tom rules: - apiGroups: - '*' resources: - '*' verbs: - '*' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tom roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: tom subjects: - kind: ServiceAccount name: tom namespace: default kubectl --kubeconfig /root/.kube/members.config --context member1 apply -f member1-rbac.yaml Run the command that failed in the previous step again: kubectl --kubeconfig tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes The access will be successful. Or we can append /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy to the server address of tom.config , and then you can directly use: kubectl --kubeconfig tom.config get node Note: For a member cluster that joins karmada in pull mode and allows only cluster-to-karmada access, we can deploy apiserver-network-proxy (ANP) to access it.","title":"Step4: Grant permission to Serviceaccount in member1 cluster"},{"location":"userguide/cluster-registration/","text":"Table of Contents generated with DocToc Cluster Registration Overview of cluster mode Push mode Pull mode Register cluster with 'Push' mode Register cluster by CLI Check cluster status Unregister cluster by CLI Register cluster with 'Pull' mode Register cluster Check cluster status Unregister cluster Cluster Registration Overview of cluster mode Karmada supports both Push and Pull modes to manage the member clusters. The main difference between Push and Pull modes is the way access to member clusters when deploying manifests. Push mode Karmada control plane will access member cluster's kube-apiserver directly to get cluster status and deploy manifests. Pull mode Karmada control plane will not access member cluster but delegate it to an extra component named karmada-agent . Each karmada-agent serves for a cluster and take responsibility for: - Register cluster to Karmada(creates the Cluster object) - Maintains cluster status and reports to Karmada(updates the status of Cluster object) - Watch manifests from Karmada execution space(namespace, karmada-es-<cluster name> ) and deploy to the cluster which serves for. Register cluster with 'Push' mode You can use the kubectl-karmada CLI to join (register) and unjoin (unregister) clusters. Register cluster by CLI Join cluster with name member1 to Karmada by using the following command. kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --cluster-kubeconfig=<member1 kubeconfig> Repeat this step to join any additional clusters. The --kubeconfig specifies the Karmada's kubeconfig file and the CLI infers karmada-apiserver context from the current-context field of the kubeconfig . If there are more than one context is configured in the kubeconfig file, it is recommended to specify the context by the --karmada-context flag. For example: kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --karmada-context=karmada --cluster-kubeconfig=<member1 kubeconfig> The --cluster-kubeconfig specifies the member cluster's kubeconfig and the CLI infers the member cluster's context by the cluster name. If there is more than one context is configured in the kubeconfig file, or you don't want to use the context name to register, it is recommended to specify the context by the --cluster-context flag. For example: kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --karmada-context=karmada \\ --cluster-kubeconfig=<member1 kubeconfig> --cluster-context=member1 Note: The registering cluster name can be different from the context with --cluster-context specified. Check cluster status Check the status of the joined clusters by using the following command. kubectl get clusters NAME VERSION MODE READY AGE member1 v1.20.7 Push True 66s Unregister cluster by CLI You can unjoin clusters by using the following command. kubectl karmada unjoin member1 --kubeconfig=<karmada kubeconfig> --cluster-kubeconfig=<member1 kubeconfig> During unjoin process, the resources propagated to member1 by Karmada will be cleaned up. And the --cluster-kubeconfig is used to clean up the secret created at the join phase. Repeat this step to unjoin any additional clusters. Register cluster with 'Pull' mode Register cluster After karmada-agent be deployed, it will register cluster automatically at the start-up phase. Check cluster status Check the status of the registered clusters by using the same command above. kubectl get clusters NAME VERSION MODE READY AGE member3 v1.20.7 Pull True 66s Unregister cluster Undeploy the karmada-agent and then remove the cluster manually from Karmada. kubectl delete cluster member3","title":"cluster-registration"},{"location":"userguide/cluster-registration/#cluster-registration","text":"","title":"Cluster Registration"},{"location":"userguide/cluster-registration/#overview-of-cluster-mode","text":"Karmada supports both Push and Pull modes to manage the member clusters. The main difference between Push and Pull modes is the way access to member clusters when deploying manifests.","title":"Overview of cluster mode"},{"location":"userguide/cluster-registration/#push-mode","text":"Karmada control plane will access member cluster's kube-apiserver directly to get cluster status and deploy manifests.","title":"Push mode"},{"location":"userguide/cluster-registration/#pull-mode","text":"Karmada control plane will not access member cluster but delegate it to an extra component named karmada-agent . Each karmada-agent serves for a cluster and take responsibility for: - Register cluster to Karmada(creates the Cluster object) - Maintains cluster status and reports to Karmada(updates the status of Cluster object) - Watch manifests from Karmada execution space(namespace, karmada-es-<cluster name> ) and deploy to the cluster which serves for.","title":"Pull mode"},{"location":"userguide/cluster-registration/#register-cluster-with-push-mode","text":"You can use the kubectl-karmada CLI to join (register) and unjoin (unregister) clusters.","title":"Register cluster with 'Push' mode"},{"location":"userguide/cluster-registration/#register-cluster-by-cli","text":"Join cluster with name member1 to Karmada by using the following command. kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --cluster-kubeconfig=<member1 kubeconfig> Repeat this step to join any additional clusters. The --kubeconfig specifies the Karmada's kubeconfig file and the CLI infers karmada-apiserver context from the current-context field of the kubeconfig . If there are more than one context is configured in the kubeconfig file, it is recommended to specify the context by the --karmada-context flag. For example: kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --karmada-context=karmada --cluster-kubeconfig=<member1 kubeconfig> The --cluster-kubeconfig specifies the member cluster's kubeconfig and the CLI infers the member cluster's context by the cluster name. If there is more than one context is configured in the kubeconfig file, or you don't want to use the context name to register, it is recommended to specify the context by the --cluster-context flag. For example: kubectl karmada join member1 --kubeconfig=<karmada kubeconfig> --karmada-context=karmada \\ --cluster-kubeconfig=<member1 kubeconfig> --cluster-context=member1 Note: The registering cluster name can be different from the context with --cluster-context specified.","title":"Register cluster by CLI"},{"location":"userguide/cluster-registration/#check-cluster-status","text":"Check the status of the joined clusters by using the following command. kubectl get clusters NAME VERSION MODE READY AGE member1 v1.20.7 Push True 66s","title":"Check cluster status"},{"location":"userguide/cluster-registration/#unregister-cluster-by-cli","text":"You can unjoin clusters by using the following command. kubectl karmada unjoin member1 --kubeconfig=<karmada kubeconfig> --cluster-kubeconfig=<member1 kubeconfig> During unjoin process, the resources propagated to member1 by Karmada will be cleaned up. And the --cluster-kubeconfig is used to clean up the secret created at the join phase. Repeat this step to unjoin any additional clusters.","title":"Unregister cluster by CLI"},{"location":"userguide/cluster-registration/#register-cluster-with-pull-mode","text":"","title":"Register cluster with 'Pull' mode"},{"location":"userguide/cluster-registration/#register-cluster","text":"After karmada-agent be deployed, it will register cluster automatically at the start-up phase.","title":"Register cluster"},{"location":"userguide/cluster-registration/#check-cluster-status_1","text":"Check the status of the registered clusters by using the same command above. kubectl get clusters NAME VERSION MODE READY AGE member3 v1.20.7 Pull True 66s","title":"Check cluster status"},{"location":"userguide/cluster-registration/#unregister-cluster","text":"Undeploy the karmada-agent and then remove the cluster manually from Karmada. kubectl delete cluster member3","title":"Unregister cluster"},{"location":"userguide/configure-controllers/","text":"Table of Contents generated with DocToc Configure Controllers Karmada Controllers Configure Karmada Controllers Kubernetes Controllers Recommended Controllers namespace garbagecollector serviceaccount-token Configure Controllers Karmada maintains a bunch of controllers which are control loops that watch the state of your system, then make or request changes where needed. Each controller tries to move the current state closer to the desired state. See Kubernetes Controller Concepts for more details. Karmada Controllers The controllers are embedded into components of karmada-controller-manager or karmada-agent and will be launched along with components startup. Some controllers may be shared by karmada-controller-manager and karmada-agent . Controller In karmada-controller-manager In karmada-agent cluster Y N clusterStatus Y Y binding Y N execution Y Y workStatus Y Y namespace Y N serviceExport Y Y endpointSlice Y N serviceImport Y N unifiedAuth Y N hpa Y N Configure Karmada Controllers You can use --controllers flag to specify the enabled controller list for karmada-controller-manager and karmada-agent , or disable some of them in addition to the default list. E.g. Specify a controller list: --controllers=cluster,clusterStatus,binding,xxx E.g. Disable some controllers(remember to keep * if you want to keep the rest controllers in the default list): --controllers=-hpa,-unifiedAuth,* Use -foo to disable the controller named foo . Note: The default controller list might be changed in the future releases. The controllers enabled in the last release might be disabled or deprecated and new controllers might be introduced too. Users who are using this flag should check the release notes before system upgrade. Kubernetes Controllers In addition to the controllers that are maintained by the Karmada community, Karmada also requires some controllers from Kubernetes. These controllers run as part of kube-controller-manager and are maintained by the Kubernetes community. Users are recommended to deploy the kube-controller-manager along with Karmada components. And the installation methods list in installation guide would help you deploy it as well as Karmada components. Recommended Controllers Not all controllers in kube-controller-manager are necessary for Karmada, if you are deploying Karmada using other tools, you might have to configure the controllers by --controllers flag just like what we did in example of kube-controller-manager deployment . The following controllers are tested and recommended by Karmada. namespace The namespace controller runs as part of kube-controller-manager . It watches Namespace deletion and deletes all resources in the given namespace. For the Karmada control plane, we inherit this behavior to keep a consistent user experience. More than that, we also rely on this feature in the implementation of Karmada controllers, for example, when un-registering a cluster, Karmada would delete the execution namespace (named karmada-es-<cluster name> ) that stores all the resources propagated to that cluster, to ensure all the resources could be cleaned up from both the Karmada control plane and the given cluster. More details about the namespace controller, please refer to namespace controller sync logic . garbagecollector The garbagecollector controller runs as part of kube-controller-manager . It is used to clean up garbage resources. It manages owner reference and deletes the resources once all owners are absent. For the Karmada control plane, we also use owner reference to link objects to each other. For example, each ResourceBinding has an owner reference that link to the resource template . Once the resource template is removed, the ResourceBinding will be removed by garbagecollector controller automatically. For more details about garbage collection mechanisms, please refer to Garbage Collection . serviceaccount-token The serviceaccount-token controller runs as part of kube-controller-manager . It watches ServiceAccount creation and creates a corresponding ServiceAccount token Secret to allow API access. For the Karmada control plane, after a ServiceAccount object is created by the administrator, we also need serviceaccount-token controller to generate the ServiceAccount token Secret , which will be a relief for administrator as he/she doesn't need to manually prepare the token. More details please refer to: - service account token controller - service account tokens","title":"configure-controllers"},{"location":"userguide/configure-controllers/#configure-controllers","text":"Karmada maintains a bunch of controllers which are control loops that watch the state of your system, then make or request changes where needed. Each controller tries to move the current state closer to the desired state. See Kubernetes Controller Concepts for more details.","title":"Configure Controllers"},{"location":"userguide/configure-controllers/#karmada-controllers","text":"The controllers are embedded into components of karmada-controller-manager or karmada-agent and will be launched along with components startup. Some controllers may be shared by karmada-controller-manager and karmada-agent . Controller In karmada-controller-manager In karmada-agent cluster Y N clusterStatus Y Y binding Y N execution Y Y workStatus Y Y namespace Y N serviceExport Y Y endpointSlice Y N serviceImport Y N unifiedAuth Y N hpa Y N","title":"Karmada Controllers"},{"location":"userguide/configure-controllers/#configure-karmada-controllers","text":"You can use --controllers flag to specify the enabled controller list for karmada-controller-manager and karmada-agent , or disable some of them in addition to the default list. E.g. Specify a controller list: --controllers=cluster,clusterStatus,binding,xxx E.g. Disable some controllers(remember to keep * if you want to keep the rest controllers in the default list): --controllers=-hpa,-unifiedAuth,* Use -foo to disable the controller named foo . Note: The default controller list might be changed in the future releases. The controllers enabled in the last release might be disabled or deprecated and new controllers might be introduced too. Users who are using this flag should check the release notes before system upgrade.","title":"Configure Karmada Controllers"},{"location":"userguide/configure-controllers/#kubernetes-controllers","text":"In addition to the controllers that are maintained by the Karmada community, Karmada also requires some controllers from Kubernetes. These controllers run as part of kube-controller-manager and are maintained by the Kubernetes community. Users are recommended to deploy the kube-controller-manager along with Karmada components. And the installation methods list in installation guide would help you deploy it as well as Karmada components.","title":"Kubernetes Controllers"},{"location":"userguide/configure-controllers/#recommended-controllers","text":"Not all controllers in kube-controller-manager are necessary for Karmada, if you are deploying Karmada using other tools, you might have to configure the controllers by --controllers flag just like what we did in example of kube-controller-manager deployment . The following controllers are tested and recommended by Karmada.","title":"Recommended Controllers"},{"location":"userguide/configure-controllers/#namespace","text":"The namespace controller runs as part of kube-controller-manager . It watches Namespace deletion and deletes all resources in the given namespace. For the Karmada control plane, we inherit this behavior to keep a consistent user experience. More than that, we also rely on this feature in the implementation of Karmada controllers, for example, when un-registering a cluster, Karmada would delete the execution namespace (named karmada-es-<cluster name> ) that stores all the resources propagated to that cluster, to ensure all the resources could be cleaned up from both the Karmada control plane and the given cluster. More details about the namespace controller, please refer to namespace controller sync logic .","title":"namespace"},{"location":"userguide/configure-controllers/#garbagecollector","text":"The garbagecollector controller runs as part of kube-controller-manager . It is used to clean up garbage resources. It manages owner reference and deletes the resources once all owners are absent. For the Karmada control plane, we also use owner reference to link objects to each other. For example, each ResourceBinding has an owner reference that link to the resource template . Once the resource template is removed, the ResourceBinding will be removed by garbagecollector controller automatically. For more details about garbage collection mechanisms, please refer to Garbage Collection .","title":"garbagecollector"},{"location":"userguide/configure-controllers/#serviceaccount-token","text":"The serviceaccount-token controller runs as part of kube-controller-manager . It watches ServiceAccount creation and creates a corresponding ServiceAccount token Secret to allow API access. For the Karmada control plane, after a ServiceAccount object is created by the administrator, we also need serviceaccount-token controller to generate the ServiceAccount token Secret , which will be a relief for administrator as he/she doesn't need to manually prepare the token. More details please refer to: - service account token controller - service account tokens","title":"serviceaccount-token"},{"location":"userguide/customizing-resource-interpreter/","text":"Table of Contents generated with DocToc Customizing Resource Interpreter Resource Interpreter Framework Interpreter Operations Built-in Interpreter InterpretReplica ReviseReplica Retain AggregateStatus InterpretStatus InterpretDependency Customized Interpreter What are interpreter webhooks? Write an interpreter webhook server Deploy the admission webhook service Configure webhook on the fly Customizing Resource Interpreter Resource Interpreter Framework In the progress of propagating a resource from karmada-apiserver to member clusters, Karmada needs to know the resource definition. Take Propagating Deployment as an example, at the phase of building ResourceBinding , the karmada-controller-manager will parse the replicas from the deployment object. For Kubernetes native resources, Karmada knows how to parse them, but for custom resources defined by CRD (or extended by something like aggregated-apiserver ), as lack of the knowledge of the resource structure, they can only be treated as normal resources. Therefore, the advanced scheduling algorithms cannot be used for them. The Resource Interpreter Framework is designed for interpreting resource structure. It consists of built-in and customized interpreters: - built-in interpreter: used for common Kubernetes native or well-known extended resources. - customized interpreter: interprets custom resources or overrides the built-in interpreters. Note: The major difference between built-in and customized interpreters is that the built-in interpreter is implemented and maintained by Karmada community and will be built into Karmada components, such as karmada-controller-manager . On the contrary, the customized interpreter is implemented and maintained by users. It should be registered to Karmada as an Interpreter Webhook (see below for more details). Interpreter Operations When interpreting resources, we often get multiple pieces of information extracted. The Interpreter Operations defines the interpreter request type, and the Resource Interpreter Framework provides services for each operation type. For all operations designed by Resource Interpreter Framework , please refer to Interpreter Operations . Note: Not all the designed operations are supported (see below for supported operations). Note: At most one interpreter will be consulted to when interpreting a resource with specific interpreter operation and the customized interpreter has higher priority than built-in interpreter if they are both interpreting the same resource. For example, the built-in interpreter serves InterpretReplica for Deployment with version apps/v1 . If there is a customized interpreter registered to Karmada for interpreting the same resource, the customized interpreter wins and the built-in interpreter will be ignored. Built-in Interpreter For the common Kubernetes native or well-known extended resources, the interpreter operations are built-in, which means the users usually don't need to implement customized interpreters. If you want more resources to be built-in, please feel free to file an issue to let us know your user case. The built-in interpreter now supports following interpreter operations: InterpretReplica Supported resources: - Deployment(apps/v1) - Job(batch/v1) ReviseReplica Supported resources: - Deployment(apps/v1) - Job(batch/v1) Retain Supported resources: - Pod(v1) - Service(v1) - ServiceAccount(v1) - PersistentVolumeClaim(v1) - Job(batch/v1) AggregateStatus Supported resources: - Deployment(apps/v1) - Service(v1) - Ingress(extensions/v1beta1) - Job(batch/v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1) InterpretStatus Supported resources: - Deployment(apps/v1) - Service(v1) - Ingress(extensions/v1beta1) - Job(batch/v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1) InterpretDependency Supported resources: - Deployment(apps/v1) - Job(batch/v1) - Pod(v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1) Customized Interpreter The customized interpreter is implemented and maintained by users, it developed as extensions and run as webhooks at runtime. What are interpreter webhooks? Interpreter webhooks are HTTP callbacks that receive interpret requests and do something with them. Write an interpreter webhook server Please refer to the implementation of the Example of Customize Interpreter that is validated in Karmada E2E test. The webhook handles the ResourceInterpreterRequest request sent by the Karmada components (such as karmada-controller-manager ), and sends back its decision as an ResourceInterpreterResponse . Deploy the admission webhook service The Example of Customize Interpreter is deployed in the host cluster for E2E and exposed by a service as the front-end of the webhook server. You may also deploy your webhooks outside the cluster. You will need to update your webhook configurations accordingly. Configure webhook on the fly You can configure what resources and supported operations are subject to what interpreter webhook via ResourceInterpreterWebhookConfiguration . The following is an example ResourceInterpreterWebhookConfiguration : apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterWebhookConfiguration metadata: name: examples webhooks: - name: workloads.example.com rules: - operations: [ \"InterpretReplica\",\"ReviseReplica\",\"Retain\",\"AggregateStatus\" ] apiGroups: [ \"workload.example.io\" ] apiVersions: [ \"v1alpha1\" ] kinds: [ \"Workload\" ] clientConfig: url: https://karmada-interpreter-webhook-example.karmada-system.svc:443/interpreter-workload caBundle: {{caBundle}} interpreterContextVersions: [ \"v1alpha1\" ] timeoutSeconds: 3 You can config more than one webhook in a ResourceInterpreterWebhookConfiguration , each webhook serves at least one operation.","title":"customizing-resource-interpreter"},{"location":"userguide/customizing-resource-interpreter/#customizing-resource-interpreter","text":"","title":"Customizing Resource Interpreter"},{"location":"userguide/customizing-resource-interpreter/#resource-interpreter-framework","text":"In the progress of propagating a resource from karmada-apiserver to member clusters, Karmada needs to know the resource definition. Take Propagating Deployment as an example, at the phase of building ResourceBinding , the karmada-controller-manager will parse the replicas from the deployment object. For Kubernetes native resources, Karmada knows how to parse them, but for custom resources defined by CRD (or extended by something like aggregated-apiserver ), as lack of the knowledge of the resource structure, they can only be treated as normal resources. Therefore, the advanced scheduling algorithms cannot be used for them. The Resource Interpreter Framework is designed for interpreting resource structure. It consists of built-in and customized interpreters: - built-in interpreter: used for common Kubernetes native or well-known extended resources. - customized interpreter: interprets custom resources or overrides the built-in interpreters. Note: The major difference between built-in and customized interpreters is that the built-in interpreter is implemented and maintained by Karmada community and will be built into Karmada components, such as karmada-controller-manager . On the contrary, the customized interpreter is implemented and maintained by users. It should be registered to Karmada as an Interpreter Webhook (see below for more details).","title":"Resource Interpreter Framework"},{"location":"userguide/customizing-resource-interpreter/#interpreter-operations","text":"When interpreting resources, we often get multiple pieces of information extracted. The Interpreter Operations defines the interpreter request type, and the Resource Interpreter Framework provides services for each operation type. For all operations designed by Resource Interpreter Framework , please refer to Interpreter Operations . Note: Not all the designed operations are supported (see below for supported operations). Note: At most one interpreter will be consulted to when interpreting a resource with specific interpreter operation and the customized interpreter has higher priority than built-in interpreter if they are both interpreting the same resource. For example, the built-in interpreter serves InterpretReplica for Deployment with version apps/v1 . If there is a customized interpreter registered to Karmada for interpreting the same resource, the customized interpreter wins and the built-in interpreter will be ignored.","title":"Interpreter Operations"},{"location":"userguide/customizing-resource-interpreter/#built-in-interpreter","text":"For the common Kubernetes native or well-known extended resources, the interpreter operations are built-in, which means the users usually don't need to implement customized interpreters. If you want more resources to be built-in, please feel free to file an issue to let us know your user case. The built-in interpreter now supports following interpreter operations:","title":"Built-in Interpreter"},{"location":"userguide/customizing-resource-interpreter/#interpretreplica","text":"Supported resources: - Deployment(apps/v1) - Job(batch/v1)","title":"InterpretReplica"},{"location":"userguide/customizing-resource-interpreter/#revisereplica","text":"Supported resources: - Deployment(apps/v1) - Job(batch/v1)","title":"ReviseReplica"},{"location":"userguide/customizing-resource-interpreter/#retain","text":"Supported resources: - Pod(v1) - Service(v1) - ServiceAccount(v1) - PersistentVolumeClaim(v1) - Job(batch/v1)","title":"Retain"},{"location":"userguide/customizing-resource-interpreter/#aggregatestatus","text":"Supported resources: - Deployment(apps/v1) - Service(v1) - Ingress(extensions/v1beta1) - Job(batch/v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1)","title":"AggregateStatus"},{"location":"userguide/customizing-resource-interpreter/#interpretstatus","text":"Supported resources: - Deployment(apps/v1) - Service(v1) - Ingress(extensions/v1beta1) - Job(batch/v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1)","title":"InterpretStatus"},{"location":"userguide/customizing-resource-interpreter/#interpretdependency","text":"Supported resources: - Deployment(apps/v1) - Job(batch/v1) - Pod(v1) - DaemonSet(apps/v1) - StatefulSet(apps/v1)","title":"InterpretDependency"},{"location":"userguide/customizing-resource-interpreter/#customized-interpreter","text":"The customized interpreter is implemented and maintained by users, it developed as extensions and run as webhooks at runtime.","title":"Customized Interpreter"},{"location":"userguide/customizing-resource-interpreter/#what-are-interpreter-webhooks","text":"Interpreter webhooks are HTTP callbacks that receive interpret requests and do something with them.","title":"What are interpreter webhooks?"},{"location":"userguide/customizing-resource-interpreter/#write-an-interpreter-webhook-server","text":"Please refer to the implementation of the Example of Customize Interpreter that is validated in Karmada E2E test. The webhook handles the ResourceInterpreterRequest request sent by the Karmada components (such as karmada-controller-manager ), and sends back its decision as an ResourceInterpreterResponse .","title":"Write an interpreter webhook server"},{"location":"userguide/customizing-resource-interpreter/#deploy-the-admission-webhook-service","text":"The Example of Customize Interpreter is deployed in the host cluster for E2E and exposed by a service as the front-end of the webhook server. You may also deploy your webhooks outside the cluster. You will need to update your webhook configurations accordingly.","title":"Deploy the admission webhook service"},{"location":"userguide/customizing-resource-interpreter/#configure-webhook-on-the-fly","text":"You can configure what resources and supported operations are subject to what interpreter webhook via ResourceInterpreterWebhookConfiguration . The following is an example ResourceInterpreterWebhookConfiguration : apiVersion: config.karmada.io/v1alpha1 kind: ResourceInterpreterWebhookConfiguration metadata: name: examples webhooks: - name: workloads.example.com rules: - operations: [ \"InterpretReplica\",\"ReviseReplica\",\"Retain\",\"AggregateStatus\" ] apiGroups: [ \"workload.example.io\" ] apiVersions: [ \"v1alpha1\" ] kinds: [ \"Workload\" ] clientConfig: url: https://karmada-interpreter-webhook-example.karmada-system.svc:443/interpreter-workload caBundle: {{caBundle}} interpreterContextVersions: [ \"v1alpha1\" ] timeoutSeconds: 3 You can config more than one webhook in a ResourceInterpreterWebhookConfiguration , each webhook serves at least one operation.","title":"Configure webhook on the fly"},{"location":"userguide/failover/","text":"Failover Overview Monitor the cluster health status Karmada supports both Push and Pull modes to manage member clusters. More details about cluster registration please refer to Cluster Registration . Determining failures For clusters there are two forms of heartbeats: - updates to the .status of a Cluster. - Lease objects within the karmada-cluster namespace in karmada control plane. Each cluster has an associated Lease object. Cluster status collection For Push mode clusters, the cluster status controller in karmada control plane will continually collect cluster's status for a configured interval. For Pull mode clusters, the karmada-agent is responsible for creating and updating the .status of clusters with configured interval. The interval for .status updates to Cluster can be configured via --cluster-status-update-frequency flag(default is 10 seconds). Cluster might be set to the NotReady state with following conditions: - cluster is unreachable(retry 4 times within 2 seconds). - cluster's health endpoint responded without ok. - failed to collect cluster status including the kubernetes\u2019 version, installed APIs, resources usages, etc. Lease updates Karmada will create a Lease object and a lease controller for each cluster when clusters are joined. Each lease controller is responsible for updating the related Leases. The lease renewing time can be configured via --cluster-lease-duration and --cluster-lease-renew-interval-fraction flags(default is 10 seconds). Lease\u2019s updating process is independent with cluster\u2019s status updating process, since cluster\u2019s .status field is maintained by cluster status controller. The cluster controller in Karmada control plane would check the state of each cluster every --cluster-monitor-period period(default is 5 seconds). The cluster's Ready condition would be changed to Unknown when cluster controller has not heard from the cluster in the last --cluster-monitor-grace-period (default is 40 seconds). Check cluster status You can use kubectl to check a Cluster's status and other details: kubectl describe cluster <cluster-name> The Ready condition in Status field indicates the cluster is healthy and ready to accept workloads. It will be set to False if the cluster is not healthy and is not accepting workloads, and Unknown if the cluster controller has not heard from the cluster in the last cluster-monitor-grace-period . The following example describes an unhealthy cluster: kubectl describe cluster member1 Name: member1 Namespace: Labels: <none> Annotations: <none> API Version: cluster.karmada.io/v1alpha1 Kind: Cluster Metadata: Creation Timestamp: 2021-12-29T08:49:35Z Finalizers: karmada.io/cluster-controller Resource Version: 152047 UID: 53c133ab-264e-4e8e-ab63-a21611f7fae8 Spec: API Endpoint: https://172.23.0.7:6443 Impersonator Secret Ref: Name: member1-impersonator Namespace: karmada-cluster Secret Ref: Name: member1 Namespace: karmada-cluster Sync Mode: Push Status: Conditions: Last Transition Time: 2021-12-31T03:36:08Z Message: cluster is not reachable Reason: ClusterNotReachable Status: False Type: Ready Events: <none> Failover feature of Karmada The failover feature is controlled by the Failover feature gate, users need to enable the Failover feature gate of karmada scheduler: --feature-gates=Failover=true Concept When it is determined that member clusters becoming unhealthy, the karmada scheduler will reschedule the reference application. There are several constraints: - For each rescheduled application, it still needs to meet the restrictions of PropagationPolicy, such as ClusterAffinity or SpreadConstraints. - The application distributed on the ready clusters after the initial scheduling will remain when failover schedule. Duplicated schedule type For Duplicated schedule policy, when the number of candidate clusters that meet the PropagationPolicy restriction is not less than the number of failed clusters, it will be rescheduled to candidate clusters according to the number of failed clusters. Otherwise, no rescheduling. Take Deployment as example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 - member5 spreadConstraints: - maxGroups: 2 minGroups: 2 replicaScheduling: replicaSchedulingType: Duplicated Suppose there are 5 member clusters, and the initial scheduling result is in member1 and member2. When member2 fails, it triggers rescheduling. It should be noted that rescheduling will not delete the application on the ready cluster member1. In the remaining 3 clusters, only member3 and member5 match the clusterAffinity policy. Due to the limitations of spreadConstraints, the final result can be [member1, member3] or [member1, member5]. Divided schedule type For Divided schedule policy, karmada scheduler will try to migrate replicas to the other health clusters. Take Deployment as example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 2 Karmada scheduler will divide the replicas according the weightPreference . The initial schedule result is member1 with 1 replica and member2 with 2 replicas. When member1 fails, it triggers rescheduling. Karmada scheduler will try to migrate replicas to the other health clusters. The final result will be member2 with 3 replicas.","title":"Failover Overview"},{"location":"userguide/failover/#failover-overview","text":"","title":"Failover Overview"},{"location":"userguide/failover/#monitor-the-cluster-health-status","text":"Karmada supports both Push and Pull modes to manage member clusters. More details about cluster registration please refer to Cluster Registration .","title":"Monitor the cluster health status"},{"location":"userguide/failover/#determining-failures","text":"For clusters there are two forms of heartbeats: - updates to the .status of a Cluster. - Lease objects within the karmada-cluster namespace in karmada control plane. Each cluster has an associated Lease object.","title":"Determining failures"},{"location":"userguide/failover/#cluster-status-collection","text":"For Push mode clusters, the cluster status controller in karmada control plane will continually collect cluster's status for a configured interval. For Pull mode clusters, the karmada-agent is responsible for creating and updating the .status of clusters with configured interval. The interval for .status updates to Cluster can be configured via --cluster-status-update-frequency flag(default is 10 seconds). Cluster might be set to the NotReady state with following conditions: - cluster is unreachable(retry 4 times within 2 seconds). - cluster's health endpoint responded without ok. - failed to collect cluster status including the kubernetes\u2019 version, installed APIs, resources usages, etc.","title":"Cluster status collection"},{"location":"userguide/failover/#lease-updates","text":"Karmada will create a Lease object and a lease controller for each cluster when clusters are joined. Each lease controller is responsible for updating the related Leases. The lease renewing time can be configured via --cluster-lease-duration and --cluster-lease-renew-interval-fraction flags(default is 10 seconds). Lease\u2019s updating process is independent with cluster\u2019s status updating process, since cluster\u2019s .status field is maintained by cluster status controller. The cluster controller in Karmada control plane would check the state of each cluster every --cluster-monitor-period period(default is 5 seconds). The cluster's Ready condition would be changed to Unknown when cluster controller has not heard from the cluster in the last --cluster-monitor-grace-period (default is 40 seconds).","title":"Lease updates"},{"location":"userguide/failover/#check-cluster-status","text":"You can use kubectl to check a Cluster's status and other details: kubectl describe cluster <cluster-name> The Ready condition in Status field indicates the cluster is healthy and ready to accept workloads. It will be set to False if the cluster is not healthy and is not accepting workloads, and Unknown if the cluster controller has not heard from the cluster in the last cluster-monitor-grace-period . The following example describes an unhealthy cluster: kubectl describe cluster member1 Name: member1 Namespace: Labels: <none> Annotations: <none> API Version: cluster.karmada.io/v1alpha1 Kind: Cluster Metadata: Creation Timestamp: 2021-12-29T08:49:35Z Finalizers: karmada.io/cluster-controller Resource Version: 152047 UID: 53c133ab-264e-4e8e-ab63-a21611f7fae8 Spec: API Endpoint: https://172.23.0.7:6443 Impersonator Secret Ref: Name: member1-impersonator Namespace: karmada-cluster Secret Ref: Name: member1 Namespace: karmada-cluster Sync Mode: Push Status: Conditions: Last Transition Time: 2021-12-31T03:36:08Z Message: cluster is not reachable Reason: ClusterNotReachable Status: False Type: Ready Events: <none>","title":"Check cluster status"},{"location":"userguide/failover/#failover-feature-of-karmada","text":"The failover feature is controlled by the Failover feature gate, users need to enable the Failover feature gate of karmada scheduler: --feature-gates=Failover=true","title":"Failover feature of Karmada"},{"location":"userguide/failover/#concept","text":"When it is determined that member clusters becoming unhealthy, the karmada scheduler will reschedule the reference application. There are several constraints: - For each rescheduled application, it still needs to meet the restrictions of PropagationPolicy, such as ClusterAffinity or SpreadConstraints. - The application distributed on the ready clusters after the initial scheduling will remain when failover schedule.","title":"Concept"},{"location":"userguide/failover/#duplicated-schedule-type","text":"For Duplicated schedule policy, when the number of candidate clusters that meet the PropagationPolicy restriction is not less than the number of failed clusters, it will be rescheduled to candidate clusters according to the number of failed clusters. Otherwise, no rescheduling. Take Deployment as example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 - member3 - member5 spreadConstraints: - maxGroups: 2 minGroups: 2 replicaScheduling: replicaSchedulingType: Duplicated Suppose there are 5 member clusters, and the initial scheduling result is in member1 and member2. When member2 fails, it triggers rescheduling. It should be noted that rescheduling will not delete the application on the ready cluster member1. In the remaining 3 clusters, only member3 and member5 match the clusterAffinity policy. Due to the limitations of spreadConstraints, the final result can be [member1, member3] or [member1, member5].","title":"Duplicated schedule type"},{"location":"userguide/failover/#divided-schedule-type","text":"For Divided schedule policy, karmada scheduler will try to migrate replicas to the other health clusters. Take Deployment as example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx --- apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 2 Karmada scheduler will divide the replicas according the weightPreference . The initial schedule result is member1 with 1 replica and member2 with 2 replicas. When member1 fails, it triggers rescheduling. Karmada scheduler will try to migrate replicas to the other health clusters. The final result will be member2 with 3 replicas.","title":"Divided schedule type"},{"location":"userguide/override-policy/","text":"Override Policy The OverridePolicy and ClusterOverridePolicy are used to declare override rules for resources when they are propagating to different clusters. Difference between OverridePolicy and ClusterOverridePolicy ClusterOverridePolicy represents the cluster-wide policy that overrides a group of resources to one or more clusters while OverridePolicy will apply to resources in the same namespace as the namespace-wide policy. For cluster scoped resources, apply ClusterOverridePolicy by policies name in ascending. For namespaced scoped resources, first apply ClusterOverridePolicy, then apply OverridePolicy. Resource Selector ResourceSelectors restricts resource types that this override policy applies to. If you ignore this field it means matching all resources. Resource Selector required apiVersion field which represents the API version of the target resources and kind which represents the Kind of the target resources. The allowed selectors are as follows: - namespace : namespace of the target resource. - name : name of the target resource - labelSelector : A label query over a set of resources. Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx namespace: test labelSelector: matchLabels: app: nginx overrideRules: ... It means override rules above will only be applied to Deployment which is named nginx in test namespace and has labels with app: nginx . Target Cluster Target Cluster defines restrictions on the override policy that only applies to resources propagated to the matching clusters. If you ignore this field it means matching all clusters. The allowed selectors are as follows: - labelSelector : a filter to select member clusters by labels. - fieldSelector : a filter to select member clusters by fields. Currently only three fields of provider(cluster.spec.provider), zone(cluster.spec.zone), and region(cluster.spec.region) are supported. - clusterNames : the list of clusters to be selected. - exclude : the list of clusters to be ignored. labelSelector Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: labelSelector: matchLabels: cluster: member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has cluster: member1 label. fieldSelector Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: fieldSelector: matchExpressions: - key: region operator: In values: - cn-north-1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has the spec.region field with values in [cn-north-1]. fieldSelector Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: fieldSelector: matchExpressions: - key: region operator: In values: - cn-north-1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has the spec.region field with values in [cn-north-1]. clusterNames Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: clusterNames: - member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters whose clusterNames are member1. exclude Examples apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: exclude: - member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters whose clusterNames are not member1. Overriders Karmada offers various alternatives to declare the override rules: - ImageOverrider : dedicated to override images for workloads. - CommandOverrider : dedicated to override commands for workloads. - ArgsOverrider : dedicated to override args for workloads. - PlaintextOverrider : a general-purpose tool to override any kind of resources. ImageOverrider The ImageOverrider is a refined tool to override images with format [registry/]repository[:tag|@digest] (e.g. /spec/template/spec/containers/0/image ) for workloads such as Deployment . The allowed operations are as follows: - add : appends the registry, repository or tag/digest to the image from containers. - remove : removes the registry, repository or tag/digest from the image from containers. - replace : replaces the registry, repository or tag/digest of the image from containers. Examples Suppose we create a deployment named myapp . apiVersion: apps/v1 kind: Deployment metadata: name: myapp ... spec: template: spec: containers: - image: myapp:1.0.0 name: myapp Example 1: Add the registry when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Registry operator: add value: test-repo It means add a registry test-repo to the image of myapp . After the policy is applied for myapp , the image will be: containers: - image: test-repo/myapp:1.0.0 name: myapp Example 2: replace the repository when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Repository operator: replace value: myapp2 It means replace the repository from myapp to myapp2 . After the policy is applied for myapp , the image will be: containers: - image: myapp2:1.0.0 name: myapp Example 3: remove the tag when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Tag operator: remove It means remove the tag of the image myapp . After the policy is applied for myapp , the image will be: containers: - image: myapp name: myapp CommandOverrider The CommandOverrider is a refined tool to override commands(e.g. /spec/template/spec/containers/0/command ) for workloads, such as Deployment . The allowed operations are as follows: - add : appends one or more flags to the command list. - remove : removes one or more flags from the command list. Examples Suppose we create a deployment named myapp . apiVersion: apps/v1 kind: Deployment metadata: name: myapp ... spec: template: spec: containers: - image: myapp name: myapp command: - ./myapp - --parameter1=foo - --parameter2=bar Example 1: Add flags when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: commandOverrider: - containerName: myapp operator: add value: - --cluster=member1 It means add (appending) a new flag --cluster=member1 to the myapp . After the policy is applied for myapp , the command list will be: containers: - image: myapp name: myapp command: - ./myapp - --parameter1=foo - --parameter2=bar - --cluster=member1 Example 2: Remove flags when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: commandOverrider: - containerName: myapp operator: remove value: - --parameter1=foo It means remove the flag --parameter1=foo from the command list. After the policy is applied for myapp , the command will be: containers: - image: myapp name: myapp command: - ./myapp - --parameter2=bar ArgsOverrider The ArgsOverrider is a refined tool to override args(such as /spec/template/spec/containers/0/args ) for workloads, such as Deployments . The allowed operations are as follows: - add : appends one or more args to the command list. - remove : removes one or more args from the command list. Note: the usage of ArgsOverrider is similar to CommandOverrider , You can refer to the CommandOverrider examples. PlaintextOverrider The PlaintextOverrider is a simple overrider that overrides target fields according to path, operator and value, just like kubectl patch . The allowed operations are as follows: - add : appends one or more elements to the resources. - remove : removes one or more elements from the resources. - replace : replaces one or more elements from the resources. Suppose we create a configmap named myconfigmap . apiVersion: v1 kind: ConfigMap metadata: name: myconfigmap ... data: example: 1 Example 1: replace data of the configmap when resources are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: plaintext: - path: /data/example operator: replace value: 2 It means replace data of the configmap from example: 1 to the example: 2 . After the policy is applied for myconfigmap , the configmap will be: apiVersion: v1 kind: ConfigMap metadata: name: myconfigmap ... data: example: 2","title":"override-policy"},{"location":"userguide/override-policy/#override-policy","text":"The OverridePolicy and ClusterOverridePolicy are used to declare override rules for resources when they are propagating to different clusters.","title":"Override Policy"},{"location":"userguide/override-policy/#difference-between-overridepolicy-and-clusteroverridepolicy","text":"ClusterOverridePolicy represents the cluster-wide policy that overrides a group of resources to one or more clusters while OverridePolicy will apply to resources in the same namespace as the namespace-wide policy. For cluster scoped resources, apply ClusterOverridePolicy by policies name in ascending. For namespaced scoped resources, first apply ClusterOverridePolicy, then apply OverridePolicy.","title":"Difference between OverridePolicy and ClusterOverridePolicy"},{"location":"userguide/override-policy/#resource-selector","text":"ResourceSelectors restricts resource types that this override policy applies to. If you ignore this field it means matching all resources. Resource Selector required apiVersion field which represents the API version of the target resources and kind which represents the Kind of the target resources. The allowed selectors are as follows: - namespace : namespace of the target resource. - name : name of the target resource - labelSelector : A label query over a set of resources.","title":"Resource Selector"},{"location":"userguide/override-policy/#examples","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx namespace: test labelSelector: matchLabels: app: nginx overrideRules: ... It means override rules above will only be applied to Deployment which is named nginx in test namespace and has labels with app: nginx .","title":"Examples"},{"location":"userguide/override-policy/#target-cluster","text":"Target Cluster defines restrictions on the override policy that only applies to resources propagated to the matching clusters. If you ignore this field it means matching all clusters. The allowed selectors are as follows: - labelSelector : a filter to select member clusters by labels. - fieldSelector : a filter to select member clusters by fields. Currently only three fields of provider(cluster.spec.provider), zone(cluster.spec.zone), and region(cluster.spec.region) are supported. - clusterNames : the list of clusters to be selected. - exclude : the list of clusters to be ignored.","title":"Target Cluster"},{"location":"userguide/override-policy/#labelselector","text":"","title":"labelSelector"},{"location":"userguide/override-policy/#examples_1","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: labelSelector: matchLabels: cluster: member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has cluster: member1 label.","title":"Examples"},{"location":"userguide/override-policy/#fieldselector","text":"","title":"fieldSelector"},{"location":"userguide/override-policy/#examples_2","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: fieldSelector: matchExpressions: - key: region operator: In values: - cn-north-1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has the spec.region field with values in [cn-north-1].","title":"Examples"},{"location":"userguide/override-policy/#fieldselector_1","text":"","title":"fieldSelector"},{"location":"userguide/override-policy/#examples_3","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: fieldSelector: matchExpressions: - key: region operator: In values: - cn-north-1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters which has the spec.region field with values in [cn-north-1].","title":"Examples"},{"location":"userguide/override-policy/#clusternames","text":"","title":"clusterNames"},{"location":"userguide/override-policy/#examples_4","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: clusterNames: - member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters whose clusterNames are member1.","title":"Examples"},{"location":"userguide/override-policy/#exclude","text":"","title":"exclude"},{"location":"userguide/override-policy/#examples_5","text":"apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - targetCluster: exclude: - member1 overriders: ... It means override rules above will only be applied to those resources propagated to clusters whose clusterNames are not member1.","title":"Examples"},{"location":"userguide/override-policy/#overriders","text":"Karmada offers various alternatives to declare the override rules: - ImageOverrider : dedicated to override images for workloads. - CommandOverrider : dedicated to override commands for workloads. - ArgsOverrider : dedicated to override args for workloads. - PlaintextOverrider : a general-purpose tool to override any kind of resources.","title":"Overriders"},{"location":"userguide/override-policy/#imageoverrider","text":"The ImageOverrider is a refined tool to override images with format [registry/]repository[:tag|@digest] (e.g. /spec/template/spec/containers/0/image ) for workloads such as Deployment . The allowed operations are as follows: - add : appends the registry, repository or tag/digest to the image from containers. - remove : removes the registry, repository or tag/digest from the image from containers. - replace : replaces the registry, repository or tag/digest of the image from containers.","title":"ImageOverrider"},{"location":"userguide/override-policy/#examples_6","text":"Suppose we create a deployment named myapp . apiVersion: apps/v1 kind: Deployment metadata: name: myapp ... spec: template: spec: containers: - image: myapp:1.0.0 name: myapp Example 1: Add the registry when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Registry operator: add value: test-repo It means add a registry test-repo to the image of myapp . After the policy is applied for myapp , the image will be: containers: - image: test-repo/myapp:1.0.0 name: myapp Example 2: replace the repository when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Repository operator: replace value: myapp2 It means replace the repository from myapp to myapp2 . After the policy is applied for myapp , the image will be: containers: - image: myapp2:1.0.0 name: myapp Example 3: remove the tag when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: imageOverrider: - component: Tag operator: remove It means remove the tag of the image myapp . After the policy is applied for myapp , the image will be: containers: - image: myapp name: myapp","title":"Examples"},{"location":"userguide/override-policy/#commandoverrider","text":"The CommandOverrider is a refined tool to override commands(e.g. /spec/template/spec/containers/0/command ) for workloads, such as Deployment . The allowed operations are as follows: - add : appends one or more flags to the command list. - remove : removes one or more flags from the command list.","title":"CommandOverrider"},{"location":"userguide/override-policy/#examples_7","text":"Suppose we create a deployment named myapp . apiVersion: apps/v1 kind: Deployment metadata: name: myapp ... spec: template: spec: containers: - image: myapp name: myapp command: - ./myapp - --parameter1=foo - --parameter2=bar Example 1: Add flags when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: commandOverrider: - containerName: myapp operator: add value: - --cluster=member1 It means add (appending) a new flag --cluster=member1 to the myapp . After the policy is applied for myapp , the command list will be: containers: - image: myapp name: myapp command: - ./myapp - --parameter1=foo - --parameter2=bar - --cluster=member1 Example 2: Remove flags when workloads are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: commandOverrider: - containerName: myapp operator: remove value: - --parameter1=foo It means remove the flag --parameter1=foo from the command list. After the policy is applied for myapp , the command will be: containers: - image: myapp name: myapp command: - ./myapp - --parameter2=bar","title":"Examples"},{"location":"userguide/override-policy/#argsoverrider","text":"The ArgsOverrider is a refined tool to override args(such as /spec/template/spec/containers/0/args ) for workloads, such as Deployments . The allowed operations are as follows: - add : appends one or more args to the command list. - remove : removes one or more args from the command list. Note: the usage of ArgsOverrider is similar to CommandOverrider , You can refer to the CommandOverrider examples.","title":"ArgsOverrider"},{"location":"userguide/override-policy/#plaintextoverrider","text":"The PlaintextOverrider is a simple overrider that overrides target fields according to path, operator and value, just like kubectl patch . The allowed operations are as follows: - add : appends one or more elements to the resources. - remove : removes one or more elements from the resources. - replace : replaces one or more elements from the resources. Suppose we create a configmap named myconfigmap . apiVersion: v1 kind: ConfigMap metadata: name: myconfigmap ... data: example: 1 Example 1: replace data of the configmap when resources are propagating to specific clusters. apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overrideRules: - overriders: plaintext: - path: /data/example operator: replace value: 2 It means replace data of the configmap from example: 1 to the example: 2 . After the policy is applied for myconfigmap , the configmap will be: apiVersion: v1 kind: ConfigMap metadata: name: myconfigmap ... data: example: 2","title":"PlaintextOverrider"},{"location":"userguide/promote-legacy-workload/","text":"Promote legacy workload Assume that there is a member cluster where a workload (like Deployment) is deployed but not managed by Karmada, we can use the karmadactl promote command to let Karmada take over this workload directly and not to cause its pods to restart. Example For member cluster in Push mode There is an nginx Deployment that belongs to namespace default in member cluster cluster1 . [root@master1]# kubectl get cluster NAME VERSION MODE READY AGE cluster1 v1.22.3 Push True 24d [root@cluster1]# kubectl get deploy nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 66s [root@cluster1]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-sqjj4 1/1 Running 0 2m12s We can promote it to Karmada by executing the command below on the Karmada control plane. [root@master1]# karmadactl promote deployment nginx -n default -c member1 Resource \"apps/v1, Resource=deployments\"(default/nginx) is promoted successfully The nginx deployment has been adopted by Karmada. [root@master1]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 7m25s And the pod created by the nginx deployment in the member cluster wasn't restarted. [root@cluster1]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-sqjj4 1/1 Running 0 15m For member cluster in Pull mode Most steps are same as those for clusters in Push mode. Only the flags of the karmadactl promote command are different. karmadactl promote deployment nginx -n default -c cluster1 --cluster-kubeconfig=<CLUSTER_KUBECONFIG_PATH> For more flags and example about the command, you can use karmadactl promote --help . Note: As the version upgrade of resources in Kubernetes is in progress, the apiserver of Karmada control plane cloud be different from member clusters. To avoid compatibility issues, you can specify the GVK of a resource, such as replacing deployment with deployment.v1.apps .","title":"promote-legacy-workload"},{"location":"userguide/promote-legacy-workload/#promote-legacy-workload","text":"Assume that there is a member cluster where a workload (like Deployment) is deployed but not managed by Karmada, we can use the karmadactl promote command to let Karmada take over this workload directly and not to cause its pods to restart.","title":"Promote legacy workload"},{"location":"userguide/promote-legacy-workload/#example","text":"","title":"Example"},{"location":"userguide/promote-legacy-workload/#for-member-cluster-in-push-mode","text":"There is an nginx Deployment that belongs to namespace default in member cluster cluster1 . [root@master1]# kubectl get cluster NAME VERSION MODE READY AGE cluster1 v1.22.3 Push True 24d [root@cluster1]# kubectl get deploy nginx NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 66s [root@cluster1]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-sqjj4 1/1 Running 0 2m12s We can promote it to Karmada by executing the command below on the Karmada control plane. [root@master1]# karmadactl promote deployment nginx -n default -c member1 Resource \"apps/v1, Resource=deployments\"(default/nginx) is promoted successfully The nginx deployment has been adopted by Karmada. [root@master1]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 7m25s And the pod created by the nginx deployment in the member cluster wasn't restarted. [root@cluster1]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-sqjj4 1/1 Running 0 15m","title":"For member cluster in Push mode"},{"location":"userguide/promote-legacy-workload/#for-member-cluster-in-pull-mode","text":"Most steps are same as those for clusters in Push mode. Only the flags of the karmadactl promote command are different. karmadactl promote deployment nginx -n default -c cluster1 --cluster-kubeconfig=<CLUSTER_KUBECONFIG_PATH> For more flags and example about the command, you can use karmadactl promote --help . Note: As the version upgrade of resources in Kubernetes is in progress, the apiserver of Karmada control plane cloud be different from member clusters. To avoid compatibility issues, you can specify the GVK of a resource, such as replacing deployment with deployment.v1.apps .","title":"For member cluster in Pull mode"},{"location":"userguide/propagate-dependencies/","text":"Propagate dependencies Deployment, Job, Pod, DaemonSet and StatefulSet dependencies (ConfigMaps and Secrets) can be propagated to member clusters automatically. This document demonstrates how to use this feature. For more design details, please refer to dependencies-automatically-propagation Prerequisites Karmada has been installed We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases. Enable PropagateDeps feature kubectl edit deployment karmada-controller-manager -n karmada-system Add --feature-gates=PropagateDeps=true option. Example Create a Deployment mounted with a ConfigMap apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx labels: app: my-nginx spec: replicas: 2 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - image: nginx name: my-nginx ports: - containerPort: 80 volumeMounts: - name: configmap mountPath: \"/configmap\" volumes: - name: configmap configMap: name: my-nginx-config --- apiVersion: v1 kind: ConfigMap metadata: name: my-nginx-config data: nginx.properties: | proxy-connect-timeout: \"10s\" proxy-read-timeout: \"10s\" client-max-body-size: \"2m\" Create a propagation policy with this Deployment and set propagateDeps: true . apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: my-nginx-propagation spec: propagateDeps: true resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: my-nginx placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 1 Upon successful policy execution, the Deployment and ConfigMap are properly propagated to the member cluster. $ kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get propagationpolicy NAME AGE my-nginx-propagation 16s $ kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 2/2 2 2 22m # member cluster1 $ kubectl config use-context member1 Switched to context \"member1\". $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 1/1 1 1 25m $ kubectl get configmap NAME DATA AGE my-nginx-config 1 26m # member cluster2 $ kubectl config use-context member2 Switched to context \"member2\". $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 1/1 1 1 27m $ kubectl get configmap NAME DATA AGE my-nginx-config 1 27m","title":"propagate-dependencies"},{"location":"userguide/propagate-dependencies/#propagate-dependencies","text":"Deployment, Job, Pod, DaemonSet and StatefulSet dependencies (ConfigMaps and Secrets) can be propagated to member clusters automatically. This document demonstrates how to use this feature. For more design details, please refer to dependencies-automatically-propagation","title":"Propagate dependencies"},{"location":"userguide/propagate-dependencies/#prerequisites","text":"","title":"Prerequisites"},{"location":"userguide/propagate-dependencies/#karmada-has-been-installed","text":"We can install Karmada by referring to quick-start , or directly run hack/local-up-karmada.sh script which is also used to run our E2E cases.","title":"Karmada has been installed"},{"location":"userguide/propagate-dependencies/#enable-propagatedeps-feature","text":"kubectl edit deployment karmada-controller-manager -n karmada-system Add --feature-gates=PropagateDeps=true option.","title":"Enable PropagateDeps feature"},{"location":"userguide/propagate-dependencies/#example","text":"Create a Deployment mounted with a ConfigMap apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx labels: app: my-nginx spec: replicas: 2 selector: matchLabels: app: my-nginx template: metadata: labels: app: my-nginx spec: containers: - image: nginx name: my-nginx ports: - containerPort: 80 volumeMounts: - name: configmap mountPath: \"/configmap\" volumes: - name: configmap configMap: name: my-nginx-config --- apiVersion: v1 kind: ConfigMap metadata: name: my-nginx-config data: nginx.properties: | proxy-connect-timeout: \"10s\" proxy-read-timeout: \"10s\" client-max-body-size: \"2m\" Create a propagation policy with this Deployment and set propagateDeps: true . apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: my-nginx-propagation spec: propagateDeps: true resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: my-nginx placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 1 Upon successful policy execution, the Deployment and ConfigMap are properly propagated to the member cluster. $ kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get propagationpolicy NAME AGE my-nginx-propagation 16s $ kubectl --kubeconfig /etc/karmada/karmada-apiserver.config get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 2/2 2 2 22m # member cluster1 $ kubectl config use-context member1 Switched to context \"member1\". $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 1/1 1 1 25m $ kubectl get configmap NAME DATA AGE my-nginx-config 1 26m # member cluster2 $ kubectl config use-context member2 Switched to context \"member2\". $ kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE my-nginx 1/1 1 1 27m $ kubectl get configmap NAME DATA AGE my-nginx-config 1 27m","title":"Example"},{"location":"userguide/resource-propagating/","text":"Resource Propagating The PropagationPolicy and ClusterPropagationPolicy APIs are provided to propagate resources. For the differences between the two APIs, please see here . Here, we use PropagationPolicy as an example to describe how to propagate resources. Before you start Install Karmada and prepare the karmadactl command-line tool. Deploy a simplest multi-cluster Deployment Create a PropagationPolicy object You can propagate a Deployment by creating a PropagationPolicy object defined in a YAML file. For example, this YAML file describes a Deployment object named nginx under default namespace need to be propagated to member1 cluster: # propagationpolicy.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: example-policy # The default namespace is `default`. spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx # If no namespace is specified, the namespace is inherited from the parent object scope. placement: clusterAffinity: clusterNames: - member1 Create a propagationPolicy base on the YAML file: kubectl apply -f propagationpolicy.yaml Create a Deployment nginx resource: kubectl create deployment nginx --image nginx Note: The resource exists only as a template in karmada. After being propagated to a member cluster, the behavior of the resource is the same as that of a single kubernetes cluster. Note: Resources and PropagationPolicy are created in no sequence. 3. Display information of the deployment: karmadactl get deployment The output is similar to this: The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member1 1/1 1 1 52s Y List the pods created by the deployment: karmadactl get pod -l app=nginx The output is similar to this: The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-s7vv9 member1 1/1 Running 0 52s Update PropagationPolicy You can update the propagationPolicy by applying a new YAML file. This YAML file propagates the Deployment to the member2 cluster. # propagationpolicy-update.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: example-policy spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: # Modify the selected cluster to propagate the Deployment. - member2 Apply the new YAML file: kubectl apply -f propagationpolicy-update.yaml Display information of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 1/1 1 1 5s Y List the pods of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-8t8cc member2 1/1 Running 0 17s Note: Updating the .spec.resourceSelectors field to change hit resources is currently not supported. Update Deployment You can update the deployment template. The changes will be automatically synchronized to the member clusters. Update deployment replicas to 2 Display information of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 2/2 2 2 7m59s Y List the pods of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-8t8cc member2 1/1 Running 0 8m12s nginx-6799fc88d8-zpl4j member2 1/1 Running 0 17s Delete a propagationPolicy Delete the propagationPolicy by name: kubectl delete propagationpolicy example-policy Deleting a propagationPolicy does not delete deployments propagated to member clusters. You need to delete deployments in the karmada control-plane: kubectl delete deployment nginx Deploy deployment into a specified set of target clusters .spec.placement.clusterAffinity field of PropagationPolicy represents scheduling restrictions on a certain set of clusters, without which any cluster can be scheduling candidates. It has four fields to set: - LabelSelector - FieldSelector - ClusterNames - ExcludeClusters LabelSelector LabelSelector is a filter to select member clusters by labels. It uses *metav1.LabelSelector type. If it is non-nil and non-empty, only the clusters match this filter will be selected. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: test-propagation spec: ... placement: clusterAffinity: labelSelector: matchLabels: location: us ... PropagationPolicy can also be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: test-propagation spec: ... placement: clusterAffinity: labelSelector: matchExpressions: - key: location operator: In values: - us ... For a description of matchLabels and matchExpressions , you can refer to Resources that support set-based requirements . FieldSelector FieldSelector is a filter to select member clusters by fields. If it is non-nil and non-empty, only the clusters match this filter will be selected. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: fieldSelector: matchExpressions: - key: provider operator: In values: - huaweicloud - key: region operator: NotIn values: - cn-south-1 ... If multiple matchExpressions are specified in the fieldSelector , the cluster must match all matchExpressions . The key in matchExpressions now supports three values: provider , region , and zone , which correspond to the .spec.provider , .spec.region , and .spec.zone fields of the Cluster object, respectively. The operator in matchExpressions now supports In and NotIn . ClusterNames Users can set the ClusterNames field to specify the selected clusters. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: clusterNames: - member1 - member2 ... ExcludeClusters Users can set the ExcludeClusters fields to specify the clusters to be ignored. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: exclude: - member1 - member3 ... Configuring Multi-Cluster HA for Deployment Multi-Cluster Failover Please refer to Failover feature of Karmada . Propagate specified resources to clusters Adjusting the instance propagation policy of Deployment in clusters","title":"resource-propagating"},{"location":"userguide/resource-propagating/#resource-propagating","text":"The PropagationPolicy and ClusterPropagationPolicy APIs are provided to propagate resources. For the differences between the two APIs, please see here . Here, we use PropagationPolicy as an example to describe how to propagate resources.","title":"Resource Propagating"},{"location":"userguide/resource-propagating/#before-you-start","text":"Install Karmada and prepare the karmadactl command-line tool.","title":"Before you start"},{"location":"userguide/resource-propagating/#deploy-a-simplest-multi-cluster-deployment","text":"","title":"Deploy a simplest multi-cluster Deployment"},{"location":"userguide/resource-propagating/#create-a-propagationpolicy-object","text":"You can propagate a Deployment by creating a PropagationPolicy object defined in a YAML file. For example, this YAML file describes a Deployment object named nginx under default namespace need to be propagated to member1 cluster: # propagationpolicy.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: example-policy # The default namespace is `default`. spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx # If no namespace is specified, the namespace is inherited from the parent object scope. placement: clusterAffinity: clusterNames: - member1 Create a propagationPolicy base on the YAML file: kubectl apply -f propagationpolicy.yaml Create a Deployment nginx resource: kubectl create deployment nginx --image nginx Note: The resource exists only as a template in karmada. After being propagated to a member cluster, the behavior of the resource is the same as that of a single kubernetes cluster. Note: Resources and PropagationPolicy are created in no sequence. 3. Display information of the deployment: karmadactl get deployment The output is similar to this: The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member1 1/1 1 1 52s Y List the pods created by the deployment: karmadactl get pod -l app=nginx The output is similar to this: The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-s7vv9 member1 1/1 Running 0 52s","title":"Create a PropagationPolicy object"},{"location":"userguide/resource-propagating/#update-propagationpolicy","text":"You can update the propagationPolicy by applying a new YAML file. This YAML file propagates the Deployment to the member2 cluster. # propagationpolicy-update.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: example-policy spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: clusterAffinity: clusterNames: # Modify the selected cluster to propagate the Deployment. - member2 Apply the new YAML file: kubectl apply -f propagationpolicy-update.yaml Display information of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 1/1 1 1 5s Y List the pods of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-8t8cc member2 1/1 Running 0 17s Note: Updating the .spec.resourceSelectors field to change hit resources is currently not supported.","title":"Update PropagationPolicy"},{"location":"userguide/resource-propagating/#update-deployment","text":"You can update the deployment template. The changes will be automatically synchronized to the member clusters. Update deployment replicas to 2 Display information of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY UP-TO-DATE AVAILABLE AGE ADOPTION nginx member2 2/2 2 2 7m59s Y List the pods of the deployment (the output is similar to this): The karmadactl get command now only supports the push mode. [ member3 ] is not running in push mode. NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-8t8cc member2 1/1 Running 0 8m12s nginx-6799fc88d8-zpl4j member2 1/1 Running 0 17s","title":"Update Deployment"},{"location":"userguide/resource-propagating/#delete-a-propagationpolicy","text":"Delete the propagationPolicy by name: kubectl delete propagationpolicy example-policy Deleting a propagationPolicy does not delete deployments propagated to member clusters. You need to delete deployments in the karmada control-plane: kubectl delete deployment nginx","title":"Delete a propagationPolicy"},{"location":"userguide/resource-propagating/#deploy-deployment-into-a-specified-set-of-target-clusters","text":".spec.placement.clusterAffinity field of PropagationPolicy represents scheduling restrictions on a certain set of clusters, without which any cluster can be scheduling candidates. It has four fields to set: - LabelSelector - FieldSelector - ClusterNames - ExcludeClusters","title":"Deploy deployment into a specified set of target clusters"},{"location":"userguide/resource-propagating/#labelselector","text":"LabelSelector is a filter to select member clusters by labels. It uses *metav1.LabelSelector type. If it is non-nil and non-empty, only the clusters match this filter will be selected. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: test-propagation spec: ... placement: clusterAffinity: labelSelector: matchLabels: location: us ... PropagationPolicy can also be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: test-propagation spec: ... placement: clusterAffinity: labelSelector: matchExpressions: - key: location operator: In values: - us ... For a description of matchLabels and matchExpressions , you can refer to Resources that support set-based requirements .","title":"LabelSelector"},{"location":"userguide/resource-propagating/#fieldselector","text":"FieldSelector is a filter to select member clusters by fields. If it is non-nil and non-empty, only the clusters match this filter will be selected. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: fieldSelector: matchExpressions: - key: provider operator: In values: - huaweicloud - key: region operator: NotIn values: - cn-south-1 ... If multiple matchExpressions are specified in the fieldSelector , the cluster must match all matchExpressions . The key in matchExpressions now supports three values: provider , region , and zone , which correspond to the .spec.provider , .spec.region , and .spec.zone fields of the Cluster object, respectively. The operator in matchExpressions now supports In and NotIn .","title":"FieldSelector"},{"location":"userguide/resource-propagating/#clusternames","text":"Users can set the ClusterNames field to specify the selected clusters. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: clusterNames: - member1 - member2 ...","title":"ClusterNames"},{"location":"userguide/resource-propagating/#excludeclusters","text":"Users can set the ExcludeClusters fields to specify the clusters to be ignored. PropagationPolicy can be configured as follows: apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: ... placement: clusterAffinity: exclude: - member1 - member3 ...","title":"ExcludeClusters"},{"location":"userguide/resource-propagating/#configuring-multi-cluster-ha-for-deployment","text":"","title":"Configuring Multi-Cluster HA for Deployment"},{"location":"userguide/resource-propagating/#multi-cluster-failover","text":"Please refer to Failover feature of Karmada .","title":"Multi-Cluster Failover"},{"location":"userguide/resource-propagating/#propagate-specified-resources-to-clusters","text":"","title":"Propagate specified resources to clusters"},{"location":"userguide/resource-propagating/#adjusting-the-instance-propagation-policy-of-deployment-in-clusters","text":"","title":"Adjusting the instance propagation policy of Deployment in clusters"}]}